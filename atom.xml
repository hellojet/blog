<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>hellojet</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2020-05-15T12:10:05.510Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>李洁厅</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Nested Named Entity Recognition调研</title>
    <link href="http://yoursite.com/2020/05/15/2020-05/Nested%20Named%20Entity%20Recognition%E8%B0%83%E7%A0%94/"/>
    <id>http://yoursite.com/2020/05/15/2020-05/Nested Named Entity Recognition调研/</id>
    <published>2020-05-15T01:13:41.000Z</published>
    <updated>2020-05-15T12:10:05.510Z</updated>
    
    <content type="html"><![CDATA[<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p>nested ner第一次被Kim et al. (2003)提出</p><ul><li><p>Alex et al. (2007)：双层crf模型，第一层crf用来挖掘细粒度的实体，并对结果进行embedding和词向量一起放入第二层crf，挖掘粗粒度实体</p></li><li><p>Finkel and Manning (2009)：基于语法树，适合重叠非交叉实体</p></li><li><p>Lu and Roth (2015)：超图是复杂网络里的概念，一条边连接多个节点；用基于超图的标签，从词级标签中学习嵌套信息（需要进一步理解）；Katiyar and Cardie (2018)也用到了超图，做的是超图表示学习</p></li><li><p>Xu et al. (2017)：借鉴变长编码方法来做，将上下文利用变长编码编码成定长向量，然后识别中心词是否是命名实体。一句话的变长编码应该是可以保证唯一且会随着句子中的词而改变的。</p></li><li><p>Lin et al. (2019a)：Sequence-to-Nuggets: Nested Entity Mention Detection via Anchor-Region Networks</p><p><a href="https://zhuanlan.zhihu.com/p/69356826" target="_blank" rel="noopener">笔记</a></p><p>基于一个基本假设：每个实体有一个不能被共享的head word，比如教育部门部长，部长是教育部门部长（PER）的head word，部门是教育部门（ORG）的head word。</p><p>所以本文的方法是先找head word/anchor word并判断类型，再找这个word的region，即基于该词找实体头和实体尾的位置。</p><p>所以对于慢性肾病和肾病的head word是慢性和肾病还是都是肾病？以及这种标注信息怎么获得？</p></li><li><p>Luan et al. (2019)：A General Framework for Information Extraction using Dynamic Span Graphs</p><p><a href="https://blog.csdn.net/byn12345/article/details/105670780" target="_blank" rel="noopener">笔记</a></p><p>通过枚举所有的span解决重叠实体问题，span之间通过关系或者共指消解来共享信息，不仅限于bilstm的权重共享，这种共享被称作动态的span graph</p></li><li><p>Fisher and Vlachos (2019)：Merge and label: A novel neural network architecture for nested NER.</p><p>先找最细粒度的entity，再看这些entity能不能和其他的或entity组成更长的entity，这种merge被建模成连续值决策，帮助之后的label任务。</p></li><li><p>Shibuya and Hovy (2019)：Nested named entity recognition via second-best sequence learning and decoding</p><p>设计了一种新的解码方式（解码之路大有可为！）</p><p>先解码第一层，然后在第一层解码出的实体上通过找2nd best path解码第二层，依次迭代。直到实体长度为等于1.</p><p>这篇文章中的方法使用bert与否对GENIA数据集没有提升帮助。</p></li><li><p>Strakov´a et al. (2019)：Neural Architectures for Nested NER through Linearization</p><p>用seq2seq的方法，逐个生成每个token的一个或者多个标签。</p></li><li><p>A Unified MRC Framework for Named Entity Recognition</p><p>用mrc（阅读理解）来做nested ner；文中着重强调了query的建立需要引入关于实体的解释，保证query的多样性；对于nested实体，需要先找到头尾（指针标注），然后对头尾进行匹配（二分类）。</p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;方法&quot;&gt;&lt;a href=&quot;#方法&quot; class=&quot;headerlink&quot; title=&quot;方法&quot;&gt;&lt;/a&gt;方法&lt;/h2&gt;&lt;p&gt;nested ner第一次被Kim et al. (2003)提出&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Alex et al. (2007)：双
      
    
    </summary>
    
      <category term="自然语言处理" scheme="http://yoursite.com/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="命名实体识别" scheme="http://yoursite.com/tags/%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/"/>
    
  </entry>
  
  <entry>
    <title>通过cnSchema设计自己的知识图谱</title>
    <link href="http://yoursite.com/2020/05/09/2020-05/%E9%80%9A%E8%BF%87cnSchema%E8%AE%BE%E8%AE%A1%E8%87%AA%E5%B7%B1%E7%9A%84%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    <id>http://yoursite.com/2020/05/09/2020-05/通过cnSchema设计自己的知识图谱/</id>
    <published>2020-05-09T06:16:53.000Z</published>
    <updated>2020-05-09T12:14:59.305Z</updated>
    
    <content type="html"><![CDATA[<h1 id="通过cnSchema设计自己的知识图谱"><a href="#通过cnSchema设计自己的知识图谱" class="headerlink" title="通过cnSchema设计自己的知识图谱"></a>通过cnSchema设计自己的知识图谱</h1><h2 id="cnSchema"><a href="#cnSchema" class="headerlink" title="cnSchema"></a>cnSchema</h2><p>知识表示与算法模型是人工智能的两大基石。cnSchema面向中文信息处理,为开放中文知识图谱的建模、生产、推理、学习、交互以及应用落地提供可解释、可迭代、可复用的数据接口标5,支持中文领域知识图谱的构建以及相应AI技术的应用。</p><p>简单来说，cnSchema参照schema.org搞了套中文的web schema标准，大家都按这套schema标准来做的话，中文问答bot就可以方便的获取大量知识图谱数据。</p><p>除了知识图谱schema，cnSchema还支持本体schema的设计。支持实体（class)、属性(attribute)、关系(link)，以及本体论中的subclass关系。</p><p>cnSchema还可以做到1. 通过kgapi统一知识图谱数据服务接口；2. kgtool数据转化、校验、可视化；3. 领域扩展schema等等。</p><p>其中领域扩展schema是本文的重点内容，由于cnSchema和kgtool的使用文档太少，故在此记录。</p><p>如果对于schema是如何设计的或者想要进一步了解cnSchema，可以移步：</p><p><a href="https://github.com/cnschema" target="_blank" rel="noopener">https://github.com/cnschema</a></p><p>cnSchema核心概念：</p><p><a href="https://docs.google.com/document/d/19mi3rY1Haf2KY_0kehFAOejszHzyNG5uZTiyQiRclus/edit#" target="_blank" rel="noopener">https://docs.google.com/document/d/19mi3rY1Haf2KY_0kehFAOejszHzyNG5uZTiyQiRclus/edit#</a></p><h2 id="领域扩展schema"><a href="#领域扩展schema" class="headerlink" title="领域扩展schema"></a>领域扩展schema</h2><h3 id="继承"><a href="#继承" class="headerlink" title="继承"></a>继承</h3><p>cnSchema是通过继承来完成schema的扩展的，从kgtool这个项目来看，他已经定义了一系列的标准：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clipboard_20200509023400.png" alt=""></p><p>其中cns_meta是系统级元数据标准，通常用户不可见。通俗的来说，cns_meta定义了cnSchema作为一个系统时定义的一些系统模块，比如property、template等等，以及数据类型的元数据，比如Text、URL等。如果要用cnSchema是一定要导入这个标准的。</p><p>cns_top继承自cns_meta，定义了顶级数据标准，比如Thing，代表一切事物的总和，所有要增加的实体都是Thing的子类。</p><p>除此之外的一些标准cns_place、cns_person是cnSchema已经定义好的，如果你的知识图谱中存在这些内容（人以及人的各种属性、地点以及地点的各种属性），可以直接继承自他们，不用再重新自己造轮子。</p><p>这些内容你都可以在kgtool的schema目录下看到：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clipboard_20200509024320.png" alt=""></p><h3 id="扩展schema"><a href="#扩展schema" class="headerlink" title="扩展schema"></a>扩展schema</h3><p>了解了已有的一些schema标准，并已经设计好自己的知识图谱之后，就需要将知识图谱的schema定义下来。schema没有数据是没有意义的，而在得到结构化数据之前必须得先确定好schema，所以设计schema是一个迭代的过程。这就要求我们有一定的方式定义schema的版本迭代。</p><p>幸好的是，cnSchema通过excel文件来人工构造schema，并允许我们做好schema的迭代。</p><p>cnSchema excel格式例子：</p><p><a href="https://docs.google.com/spreadsheets/d/1iZYXZRQAuSAlUpdQdYRffrtJicurm6yUnsJDsEa2CAk/edit#gid=1781706819" target="_blank" rel="noopener">https://docs.google.com/spreadsheets/d/1iZYXZRQAuSAlUpdQdYRffrtJicurm6yUnsJDsEa2CAk/edit#gid=1781706819</a></p><p>通过version、changelog和metadata可以方便控制版本：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clipboard_20200509025132.png" alt=""></p><p>现在我们只需要把我们设计好的schema填入excel中，就可以完成知识图谱的规范化定义了，就是这么简单。</p><p>让我们着手来看其中一些字段代表的意义：</p><ul><li>表格definition：在此定义实体名（class）、关系名（link）以及属性名（attribute）<ul><li>super：本体论中的上下位关系通过该字段体现</li><li>range：默认是String，对于关系名，可能需要填目标实体的name</li></ul></li><li>表格template：在这里定义二元关系<ul><li>refClass：主语（这里我就用主谓宾来表示二元关系）</li><li>refProperty：谓语</li><li>propertyRan：宾语</li></ul></li><li>表格changelog：记录版本迭代信息</li><li>表格metadata：在此定义版本信息，知识图谱介绍以及需要继承的标准</li></ul><p>如果你已经完成了这份excel的编写，恭喜你已经完成了知识图谱的规范化定义和版本定义。</p><h2 id="schema可视化"><a href="#schema可视化" class="headerlink" title="schema可视化"></a>schema可视化</h2><p>excel并不是cnSchema的标准格式，cnSchema是以json-ld为他schema的发布格式，从kgtool的schema目录下的文件格式就可以看出。</p><p>我们可以通过kgtool来完成转化，具体方式是按照kgtool/cns/cns_io.py中的方式执行即可：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python cns/cns_io.py task_excel2jsonld --input_file=local/debug/cns_medicine.xlsx --schema_dir=schema/ --output_dir=schema/ --debug_dir=local/debug/</span><br></pre></td></tr></table></figure><p>执行完成之后，就可以看到在schema下生成了你自己的jsonld文件，这是你获得的第一份schema标准。同时在local/debug下生成了.dot、.json和.xls文件，你可以通过graphviz可视化.dot文件，下面是自定义医学schema部分可视化效果：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200509142815.png" alt=""></p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h3 id="todo"><a href="#todo" class="headerlink" title="todo:"></a>todo:</h3><ol><li>尝试jsondl2website</li><li><p>同一对主谓语有多个宾语在excel中怎么表示？已提起issue</p></li><li><p>schema验证</p></li><li>数据可视化</li><li>数据验证</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;通过cnSchema设计自己的知识图谱&quot;&gt;&lt;a href=&quot;#通过cnSchema设计自己的知识图谱&quot; class=&quot;headerlink&quot; title=&quot;通过cnSchema设计自己的知识图谱&quot;&gt;&lt;/a&gt;通过cnSchema设计自己的知识图谱&lt;/h1&gt;&lt;h2 i
      
    
    </summary>
    
      <category term="知识图谱" scheme="http://yoursite.com/categories/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
    
      <category term="cnSchema" scheme="http://yoursite.com/tags/cnSchema/"/>
    
      <category term="知识图谱" scheme="http://yoursite.com/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
  </entry>
  
  <entry>
    <title>xgboost笔记</title>
    <link href="http://yoursite.com/2020/03/25/2020-03/Xgboost%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2020/03/25/2020-03/Xgboost笔记/</id>
    <published>2020-03-25T03:49:08.000Z</published>
    <updated>2020-03-25T06:35:30.578Z</updated>
    
    <content type="html"><![CDATA[<h1 id="xgboost笔记"><a href="#xgboost笔记" class="headerlink" title="xgboost笔记"></a>xgboost笔记</h1><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/201904/gbdt/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20200325142857.jpg" alt=""></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/201904/gbdt/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20200325142901.jpg" alt=""></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/201904/gbdt/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20200325142905.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;xgboost笔记&quot;&gt;&lt;a href=&quot;#xgboost笔记&quot; class=&quot;headerlink&quot; title=&quot;xgboost笔记&quot;&gt;&lt;/a&gt;xgboost笔记&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://hellojet-blog-1251889946
      
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>GBDT笔记</title>
    <link href="http://yoursite.com/2020/03/25/2020-03/GBDT%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2020/03/25/2020-03/GBDT笔记/</id>
    <published>2020-03-25T02:49:08.000Z</published>
    <updated>2020-03-25T06:33:45.355Z</updated>
    
    <content type="html"><![CDATA[<h1 id="GBDT笔记"><a href="#GBDT笔记" class="headerlink" title="GBDT笔记"></a>GBDT笔记</h1><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/201904/gbdt/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20200325142812.jpg" alt=""></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/201904/gbdt/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20200325142822.jpg" alt=""></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/201904/gbdt/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20200325142836.jpg" alt=""></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/201904/gbdt/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20200325142845.jpg" alt=""></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/201904/gbdt/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20200325142852.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;GBDT笔记&quot;&gt;&lt;a href=&quot;#GBDT笔记&quot; class=&quot;headerlink&quot; title=&quot;GBDT笔记&quot;&gt;&lt;/a&gt;GBDT笔记&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://hellojet-blog-1251889946.cos.ap-shan
      
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>SVM硬间隔笔记</title>
    <link href="http://yoursite.com/2020/03/25/2020-03/SVM%E7%A1%AC%E9%97%B4%E9%9A%94%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2020/03/25/2020-03/SVM硬间隔笔记/</id>
    <published>2020-03-25T01:49:08.000Z</published>
    <updated>2020-03-25T06:45:27.667Z</updated>
    
    <content type="html"><![CDATA[<h1 id="SVM硬间隔笔记"><a href="#SVM硬间隔笔记" class="headerlink" title="SVM硬间隔笔记"></a>SVM硬间隔笔记</h1><h2 id="svm硬间隔"><a href="#svm硬间隔" class="headerlink" title="svm硬间隔"></a>svm硬间隔</h2><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/201904/svm/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20200325144018.jpg" alt=""></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/201904/svm/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20200325144022.jpg" alt=""></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/201904/svm/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20200325144033.jpg" alt=""></p><h2 id="什么是对偶问题？"><a href="#什么是对偶问题？" class="headerlink" title="什么是对偶问题？"></a>什么是对偶问题？</h2><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/201904/svm/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20200325144026.jpg" alt=""></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/201904/svm/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20200325144030.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;SVM硬间隔笔记&quot;&gt;&lt;a href=&quot;#SVM硬间隔笔记&quot; class=&quot;headerlink&quot; title=&quot;SVM硬间隔笔记&quot;&gt;&lt;/a&gt;SVM硬间隔笔记&lt;/h1&gt;&lt;h2 id=&quot;svm硬间隔&quot;&gt;&lt;a href=&quot;#svm硬间隔&quot; class=&quot;headerli
      
    
    </summary>
    
      <category term="machine learning" scheme="http://yoursite.com/categories/machine-learning/"/>
    
    
      <category term="machine learning" scheme="http://yoursite.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title>elasticsearch使用过程中的一些整理</title>
    <link href="http://yoursite.com/2019/11/28/2019-11/elasticsearch%E4%BD%BF%E7%94%A8%E8%BF%87%E7%A8%8B%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E6%95%B4%E7%90%86/"/>
    <id>http://yoursite.com/2019/11/28/2019-11/elasticsearch使用过程中的一些整理/</id>
    <published>2019-11-28T06:11:47.000Z</published>
    <updated>2019-11-28T09:04:30.535Z</updated>
    
    <content type="html"><![CDATA[<p>使用的es版本是<strong>6.4.0</strong>，其他版本不保证同款问题同款解决方案</p><p><strong>查看es版本</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -XGET &apos;localhost:9200&apos;</span><br></pre></td></tr></table></figure><p><strong>重启es</strong></p><p>粗暴的方式：</p><p>直接杀掉进程，然后进入bin目录执行以下命令进行启动。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.\elasticsearch -d</span><br></pre></td></tr></table></figure><p>如果出现报错信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.lang.RuntimeException: can not run elasticsearch as root</span><br></pre></td></tr></table></figure><p> 表示无法使用root账户启动es，加启动参数的方法也不行，需要切换到其他账户：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">su elsearch # elsearch是账户名</span><br></pre></td></tr></table></figure><p>优雅的方式（未成功）：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl restart elasticsearch.service</span><br></pre></td></tr></table></figure><p>显示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Failed to restart elasticsearch.service: Unit not found.</span><br></pre></td></tr></table></figure><h2 id="ik插件"><a href="#ik插件" class="headerlink" title="ik插件"></a><strong>ik插件</strong></h2><p><strong>安装ik时出现</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.net.SocketPermission * connect,resolve</span><br></pre></td></tr></table></figure><p>解决方案，在install后加上-b</p><p><strong>运行ik的quick example出现</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;analyzer [ik_max_word] not found for field [content]&quot;&#125;,&quot;status&quot;:400&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;使用的es版本是&lt;strong&gt;6.4.0&lt;/strong&gt;，其他版本不保证同款问题同款解决方案&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;查看es版本&lt;/strong&gt;&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=
      
    
    </summary>
    
      <category term="elasticsearch" scheme="http://yoursite.com/categories/elasticsearch/"/>
    
    
      <category term="elasticsearch" scheme="http://yoursite.com/tags/elasticsearch/"/>
    
  </entry>
  
  <entry>
    <title>NAACL2019医疗有关文献阅读</title>
    <link href="http://yoursite.com/2019/11/27/2019-11/NAACL2019%E5%8C%BB%E7%96%97%E6%9C%89%E5%85%B3%E6%96%87%E7%8C%AE%E9%98%85%E8%AF%BB/"/>
    <id>http://yoursite.com/2019/11/27/2019-11/NAACL2019医疗有关文献阅读/</id>
    <published>2019-11-26T16:00:00.000Z</published>
    <updated>2019-11-28T06:13:19.839Z</updated>
    
    <content type="html"><![CDATA[<h3 id="Publicly-Available-Clinical-BERT-Embeddings"><a href="#Publicly-Available-Clinical-BERT-Embeddings" class="headerlink" title="Publicly Available Clinical BERT Embeddings"></a>Publicly Available Clinical BERT Embeddings</h3><p>之前有一篇文章是讲生物医疗方面的bert的叫BioBERT，这篇文章的作者训练了新的医疗bert叫做<a href="github.com/EmilyAlsentzer/clinicalBERT">clinicalBERT</a>。数据集是MIMIC-III v1.4两百万的医疗note。作者训练了两个BERT的变体，一个是基于所有note的clinical BERT，一个叫做Discharge Summary BERT，使用出院总结，应对使用出院总结的下游任务（可能在英文中这类任务比较多？）。然后下游任务用了i2b2NER任务（比较经典的一个任务），和MedNLI（18年提出的一个任务）。效果和BioBERT不相上下。</p><h3 id="Medical-Entity-Linking-using-Triplet-Network"><a href="#Medical-Entity-Linking-using-Triplet-Network" class="headerlink" title="Medical Entity Linking using Triplet Network"></a>Medical Entity Linking using Triplet Network</h3><p>正好要做EL的内容。EL一般分成候选实体生成和排序两个阶段。</p><p>文章中生成部分来自C1和C2两个集合，C1是通过计算KB中的实体向量与Mention向量的相似度，根据一个阈值取top k1得到，实体向量是他的同义词向量相加。Mention向量是mention中所有词的向量和。C2是通过mention和候选实体之间的jaccard重合度（换了个相似度度量？），根据阈值取top k2得到。</p><p>排序部分用了triplet网络，编码部分是共享权重的，过embedding之后经conv。然后用距离函数来计算mention和正负实体之间距离，损失函数来最大化两个距离之差：<br>$$<br>L = max(d_p - d_{n_i} + \alpha, 0)<br>$$<br>实验部分用了从wikipedia和pubmed PMC-Corpus上训练的200维词向量。用fastText解决OOV问题。最好的模型效果是subword（字词模型）+abb（缩略词扩展）+Triplet CNN。</p><p>用CNN和fastText模型速度应该会挺快的，针对EL这种需要大量计算的问题来说，效率也很重要。这里实体向量的构建用的是同名，应该可以加上更多的，比如关系之类的来丰富实体语义。</p><h3 id="Attention-Neural-Model-for-Temporal-Relation-Extraction"><a href="#Attention-Neural-Model-for-Temporal-Relation-Extraction" class="headerlink" title="Attention Neural Model for Temporal Relation Extraction"></a>Attention Neural Model for Temporal Relation Extraction</h3><p>对我而言，了解到一个新的概念Temporal Relation Extraction，这个概念来自17年的一篇论文<a href="https://www.aclweb.org/anthology/E17-2118.pdf" target="_blank" rel="noopener">Neural temporal relation extraction</a>.在研究药物不良反应，疾病进展和临床结果等中，时间关系抽取式建立时间线最可行的方式，将每个医疗事件event与发生的时间time联系起来。比如</p><blockquote><p>Patient was diagnosed with a rectal cancer in May of 2010</p></blockquote><p>病人时在May of 2010这个时间包含被诊断为直肠癌这个事件。</p><p>除了time和event之间存在可能的关系，event和event之间也会存在关系，比如在手术期出现心跳加速。所以现在是一个多分类任务，有三种关系1. time contain event或者event contain event、2. event contain-1 time或者event contain-1 event、3.none表示不存在关系。</p><p>论文用到的数据集是THYME（医疗事件的时间）语料库，包含200名患者的临床记录。包含两类实体：time和event，time包括事件日期和事件戳。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;Publicly-Available-Clinical-BERT-Embeddings&quot;&gt;&lt;a href=&quot;#Publicly-Available-Clinical-BERT-Embeddings&quot; class=&quot;headerlink&quot; title=&quot;Public
      
    
    </summary>
    
      <category term="nlp" scheme="http://yoursite.com/categories/nlp/"/>
    
    
      <category term="nlp" scheme="http://yoursite.com/tags/nlp/"/>
    
      <category term="医疗" scheme="http://yoursite.com/tags/%E5%8C%BB%E7%96%97/"/>
    
  </entry>
  
  <entry>
    <title>Attention笔记</title>
    <link href="http://yoursite.com/2019/03/05/2019-03/Attention%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2019/03/05/2019-03/Attention笔记/</id>
    <published>2019-03-05T02:49:08.000Z</published>
    <updated>2020-03-10T07:23:01.189Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Attention笔记（从机翻角度）"><a href="#Attention笔记（从机翻角度）" class="headerlink" title="Attention笔记（从机翻角度）"></a>Attention笔记（从机翻角度）</h1><p>[TOC]</p><h2 id="神经翻译系统"><a href="#神经翻译系统" class="headerlink" title="神经翻译系统"></a>神经翻译系统</h2><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/blog/attention/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200310151838.png" alt=""></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/blog/attention/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200310151846.png" alt=""></p><h2 id="在seq2seq模型中加入attention"><a href="#在seq2seq模型中加入attention" class="headerlink" title="在seq2seq模型中加入attention"></a>在seq2seq模型中加入attention</h2><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/blog/attention/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200310151855.png" alt=""></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/blog/attention/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200310151904.png" alt=""></p><h2 id="全局attention-vs-局部attention"><a href="#全局attention-vs-局部attention" class="headerlink" title="全局attention vs 局部attention"></a>全局attention vs 局部attention</h2><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/blog/attention/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200310151918.png" alt=""></p><h2 id="更多思路"><a href="#更多思路" class="headerlink" title="更多思路"></a>更多思路</h2><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/blog/attention/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200310151928.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Attention笔记（从机翻角度）&quot;&gt;&lt;a href=&quot;#Attention笔记（从机翻角度）&quot; class=&quot;headerlink&quot; title=&quot;Attention笔记（从机翻角度）&quot;&gt;&lt;/a&gt;Attention笔记（从机翻角度）&lt;/h1&gt;&lt;p&gt;[TOC]&lt;
      
    
    </summary>
    
      <category term="nlp" scheme="http://yoursite.com/categories/nlp/"/>
    
    
      <category term="CS224n" scheme="http://yoursite.com/tags/CS224n/"/>
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>GRU和LSTM笔记</title>
    <link href="http://yoursite.com/2019/02/09/2019-02/GRU%E5%92%8CLSTM%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2019/02/09/2019-02/GRU和LSTM笔记/</id>
    <published>2019-02-09T02:49:08.000Z</published>
    <updated>2020-03-10T07:16:04.838Z</updated>
    
    <content type="html"><![CDATA[<h1 id="GRU和LSTM笔记"><a href="#GRU和LSTM笔记" class="headerlink" title="GRU和LSTM笔记"></a>GRU和LSTM笔记</h1><p>[TOC]</p><h2 id="RNN族机器翻译"><a href="#RNN族机器翻译" class="headerlink" title="RNN族机器翻译"></a>RNN族机器翻译</h2><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/blog/lstm/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200310151041.png" alt=""></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/blog/lstm/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200310151107.png" alt=""></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/blog/lstm/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200310151124.png" alt=""></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/blog/lstm/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200310151132.png" alt=""></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/blog/lstm/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200310151146.png" alt=""></p><h2 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h2><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/blog/lstm/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200310151200.png" alt=""></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/blog/lstm/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200310151210.png" alt=""></p><h2 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h2><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/blog/lstm/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200310151220.png" alt=""></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/blog/lstm/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200310151228.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;GRU和LSTM笔记&quot;&gt;&lt;a href=&quot;#GRU和LSTM笔记&quot; class=&quot;headerlink&quot; title=&quot;GRU和LSTM笔记&quot;&gt;&lt;/a&gt;GRU和LSTM笔记&lt;/h1&gt;&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id=&quot;RNN族机器翻译&quot;&gt;&lt;a href=&quot;#
      
    
    </summary>
    
      <category term="nlp" scheme="http://yoursite.com/categories/nlp/"/>
    
    
      <category term="CS224n" scheme="http://yoursite.com/tags/CS224n/"/>
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>RNN笔记</title>
    <link href="http://yoursite.com/2019/02/02/2019-02/RNN%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2019/02/02/2019-02/RNN笔记/</id>
    <published>2019-02-02T02:49:08.000Z</published>
    <updated>2020-03-10T07:09:19.392Z</updated>
    
    <content type="html"><![CDATA[<h1 id="RNN笔记"><a href="#RNN笔记" class="headerlink" title="RNN笔记"></a>RNN笔记</h1><p>[TOC]</p><h2 id="传统语言模型的弊端"><a href="#传统语言模型的弊端" class="headerlink" title="传统语言模型的弊端"></a>传统语言模型的弊端</h2><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/blog/rnn/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200310150401.png" alt=""></p><h2 id="RNN出现啦！"><a href="#RNN出现啦！" class="headerlink" title="RNN出现啦！"></a>RNN出现啦！</h2><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/blog/rnn/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200310150426.png" alt=""></p><h2 id="RNN公式"><a href="#RNN公式" class="headerlink" title="RNN公式"></a>RNN公式</h2><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/blog/rnn/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200310150434.png" alt=""></p><h2 id="如何训练RNN"><a href="#如何训练RNN" class="headerlink" title="如何训练RNN"></a>如何训练RNN</h2><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/blog/rnn/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200310150442.png" alt=""></p><h2 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h2><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/blog/rnn/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200310150451.png" alt=""></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/blog/rnn/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200310150458.png" alt=""></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/blog/rnn/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200310150506.png" alt=""></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/blog/rnn/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200310150514.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;RNN笔记&quot;&gt;&lt;a href=&quot;#RNN笔记&quot; class=&quot;headerlink&quot; title=&quot;RNN笔记&quot;&gt;&lt;/a&gt;RNN笔记&lt;/h1&gt;&lt;p&gt;[TOC]&lt;/p&gt;
&lt;h2 id=&quot;传统语言模型的弊端&quot;&gt;&lt;a href=&quot;#传统语言模型的弊端&quot; class=&quot;h
      
    
    </summary>
    
      <category term="nlp" scheme="http://yoursite.com/categories/nlp/"/>
    
    
      <category term="CS224n" scheme="http://yoursite.com/tags/CS224n/"/>
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>依存句法分析笔记</title>
    <link href="http://yoursite.com/2019/01/11/2019-01/%E4%BE%9D%E5%AD%98%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2019/01/11/2019-01/依存句法分析笔记/</id>
    <published>2019-01-11T02:49:08.000Z</published>
    <updated>2020-03-10T07:02:48.463Z</updated>
    
    <content type="html"><![CDATA[<h1 id="依存句法分析笔记"><a href="#依存句法分析笔记" class="headerlink" title="依存句法分析笔记"></a>依存句法分析笔记</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/blog/dependency/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200310145912.png" alt=""></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/blog/dependency/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200310145921.png" alt=""></p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/blog/dependency/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200310145929.png" alt=""></p><h2 id="transition-based-dependency-parsing"><a href="#transition-based-dependency-parsing" class="headerlink" title="transition-based dependency parsing"></a>transition-based dependency parsing</h2><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/blog/dependency/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200310145940.png" alt=""></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/blog/dependency/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200310145953.png" alt=""></p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/blog/dependency/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200310150001.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;依存句法分析笔记&quot;&gt;&lt;a href=&quot;#依存句法分析笔记&quot; class=&quot;headerlink&quot; title=&quot;依存句法分析笔记&quot;&gt;&lt;/a&gt;依存句法分析笔记&lt;/h1&gt;&lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; titl
      
    
    </summary>
    
      <category term="nlp" scheme="http://yoursite.com/categories/nlp/"/>
    
    
      <category term="CS224n" scheme="http://yoursite.com/tags/CS224n/"/>
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读：Enriching Taxonomies With Functional Domain Knowledge</title>
    <link href="http://yoursite.com/2019/01/07/2019-01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AEnriching%20Taxonomies%20With%20Functional%20Domain%20Knowledge/"/>
    <id>http://yoursite.com/2019/01/07/2019-01/论文阅读：Enriching Taxonomies With Functional Domain Knowledge/</id>
    <published>2019-01-07T12:03:28.000Z</published>
    <updated>2019-01-07T12:04:48.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="阅读文献来源"><a href="#阅读文献来源" class="headerlink" title="阅读文献来源"></a>阅读文献来源</h1><p><strong>Vedula N, Nicholson P K, Ajwani D, et al. Enriching taxonomies with functional domain knowledge[C]//The 41st International ACM SIGIR Conference on Research &amp; Development in Information Retrieval. ACM, 2018: 745-754.</strong></p><p>第一作者Nikhita Vedula：PhD Student, 俄亥俄州立大学(Ohio State University)。个人主页：<a href="http://web.cse.ohio-state.edu/~vedula.5/。发表的paper：" target="_blank" rel="noopener">http://web.cse.ohio-state.edu/~vedula.5/。发表的paper：</a></p><ul><li><strong>Nikhita Vedula</strong>, Patrick K. Nicholson, Deepak Ajwani, Sourav Dutta, Alessandra Sala and Srinivasan Parthasarathy. “Enriching Taxonomies with Functional Domain Knowledge.” <em>In Proceedings of the 41st International ACM SIG Conference on Research and Development in Information Retrieval (SIGIR)</em> , Ann Arbor, MI, July 2018.</li><li><strong>Nikhita Vedula</strong>, Wei Sun, Hyunhwan Lee, Harsh Gupta, Mitsunori Ogihara, Joseph Johnson, Gang Ren and Srinivasan Parthasarathy. “Multimodal Content Analysis for Effective Advertisements on YouTube.” <em>In Proceedings of the IEEE International Conference on Data Mining (ICDM)</em>, New Orleans, LA, November 2017.</li><li><strong>Nikhita Vedula</strong> and Srinivasan Parthasarathy. “Emotional and Linguistic Cues of Depression from Social Media.” <em>In Proceedings of the 7th International ACM Digital Health Conference (DH)</em> , London, UK, July 2017.</li><li><p><strong>Nikhita Vedula</strong>, Srinivasan Parthasarathy and Valerie Shalin. “Predicting Trust Relations in Social Networks: A Case Study on Emergency Response.” <em>In Proceedings of the 9th International ACM Web Science Conference (WebSci)</em>, Troy, NY, June 2017.</p></li><li><p>Hemant Purohit, <strong>Nikhita Vedula</strong>, Krishnaprasad Thirunarayan and Srinivasan Parthasarathy. “Modeling Transportation Uncertainty in Matching Help Seekers and Suppliers during Disasters.” <em>InTI Workshop at the ACM SIG Conference on Research and Development in Information Retrieval (SIGIR)</em>, Ann Arbor, MI, July 2018.</p></li><li><strong>Nikhita Vedula</strong>, Srinivasan Parthasarathy and Valerie Shalin. “Predicting Trust Relations Among Users in a Social Network: The Role of Influence, Cohesion and Valence.” <em>WISDOM workshop at the ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD)</em>, San Francisco, CA, August 2016.</li></ul><h1 id="本文解决的科学问题及研究动机"><a href="#本文解决的科学问题及研究动机" class="headerlink" title="本文解决的科学问题及研究动机"></a>本文解决的科学问题及研究动机</h1><p>用来自新闻或者研究刊物的概念来丰富大规模通用分类系统</p><h2 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h2><p>知识以分层的方式组织存在，比如Wikipedia Categories，Freebase和WordNet。但是，这些分层次的知识难以增加和维持，当新概念和新关系的快速出现。需要<strong>自动化的，可扩展的</strong>技术来解决分类系统扩张问题。</p><p>很多技术依赖于WordNet独特的同义词结构【14，31，37】，不能泛化到其他分类系统上，只能识别新概念到一个单独的分类。</p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>以前的工作不是：</p><ul><li>语言高度相关【47，48】</li><li>领域高度相关【28】<br> 就是：</li><li>不能扩展到大规模分类系统【3，21，43】</li></ul><p>机器学习方法：给予共现分析【42】，聚类【20】，图构造和遍历【25】，分布式相似度【47，48】。<br>基于语言模式匹配方法【9，25，31，49】<br>基于词嵌入的方法【8，36，38，51】开始流行<br>【8，36】精确率较差，【51】不能泛化到没有见过的关系实例。【38】解决了这些问题，但是对训练数据有和Wikipedia风格的概念和分类名称不一样的要求。<br>本文直接基于加强WordNet分类法来设计模型。<br>【37】用Wikipedia来扩展WordNet。<br>【45】设计了一种将不知道的单词放在其邻居大多集中的地方的方法，这个idea在本文中也被采用了。<br>【47，48】用新的日语词汇来增加日语Wikipedia和WordNet，从维基中找到相似词汇，对上位词打分，选择得分最高上位词作为输出。但是严重依赖日语中的动名词依赖关系，这在英语中不常见。<br>相关问题知识图谱补全：knowledge graph completion，预测实体间关系【3，21，43】。本文将ETF与链接预测方法TransR【21】进行了对比。</p><h1 id="算法框架"><a href="#算法框架" class="headerlink" title="算法框架"></a>算法框架</h1><p>ETF的框架如图：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1546826541580.png" alt="1546826541580"></p><ol><li>先学习分类系统中新概念和已存在概念的表征。</li><li>对每个新概念，利用表征来识别这个新概念最相似的一些实体</li><li>将对新概念父类的搜索缩小到对相似实体祖先集的搜索</li><li>使用图论和语义特征的结合作为语义类别的替代品，基于这个来对祖先进行筛选和排序，最后把适当的链接放入分类系统</li></ol><h2 id="1-发现概念和分类关系"><a href="#1-发现概念和分类关系" class="headerlink" title="1.发现概念和分类关系"></a>1.发现概念和分类关系</h2><p>从分类系统结构中获得实体（叶节点）和目录（非叶节点），并利用祖先-子孙关系来构建有向无环图T：分类系统。</p><p>因为我们一般接触的是文本，而非单个新概念实体，所以使用Off-the-shelf命名实体识别算法【11,24】从文本中定位并抽取命名实体。</p><p>从文本中找到区别于已存在实体的新实体是一个问题，但不是本文的重点。</p><h2 id="2-学习概念表征"><a href="#2-学习概念表征" class="headerlink" title="2.学习概念表征"></a>2.学习概念表征</h2><p>基于高效Skip-gram变体【23】。</p><p>最小化损失函数：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1546828593852.png" alt="1546828593852"></p><p>先通过消歧名词【22】将预料库D中的出现的实体进行替换（Wikipedia的锚链接）。</p><p>然后在D上训练doc2vec（分布式内存或者段落向量的DM版本）【19】，使用负采样，得到文档的向量表示。DM模型可以同时训练word2vec和doc2vec。</p><p>然后将word2vec的tf加权和（倾向高频）与doc2vec表征（倾向低频）进行聚合。聚合好处：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1546829428978.png" alt="1546829428978"></p><p>在得到已存在的概念之后，接下来学习新概念的表征，这是至关重要的一步【44】，用已存在的学习模型产生未见过的实体的表征。之前的工作有【27】重新训练模型，【44,50】基于邻居的半监督学习。本文对每个新概念产生一个上下文c，由频繁项基于tf分数组成。使用c通过梯度下降推出doc2vec嵌入【19】。然后通过加入word2vec的tf加权和得到新概念嵌入。</p><h2 id="3-筛序并排序父类"><a href="#3-筛序并排序父类" class="headerlink" title="3.筛序并排序父类"></a>3.筛序并排序父类</h2><p>有了概念的嵌入，接下来决定新概念最好的父类。对每个新概念使用kNN。假设最好父类与这些近邻的祖先相关。接下来就是排序这些候选父类。</p><p>学习排序模型：使用分类系统图结构中的拓扑特征，从文本中得到的语义特征。尝试了七种模型【13】，最终使用表现最好的LambdaMART【5】。</p><p>基于图的特征：</p><ul><li>Katz Similarity</li><li>Random Walk Betweenness Centrality</li><li>Information Propagation Score</li></ul><p>语义特征：</p><ul><li>Ancestor-Neighbor Similarity</li><li>New concept-Ancestor Similarity</li><li>Pointwise Mutual Information (PMI)：点间互信息</li></ul><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>在Wikipedia Categories和WordNet上评价ETF。</p><h2 id="评价概念表征"><a href="#评价概念表征" class="headerlink" title="评价概念表征"></a>评价概念表征</h2><p>测试新概念的聚合向量嵌入，通过检查和已存在实体的距离。</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1546831075503.png" alt="1546831075503"></p><p>28%的新概念与已存在概念有大于0.4的相似度。换句话说，大于70%的新概念和已存在概念分离开来，当相似度阈值设置为0.4的时候。</p><h2 id="评价排序模型"><a href="#评价排序模型" class="headerlink" title="评价排序模型"></a>评价排序模型</h2><p>本文算法和一些baseline（Random，Text similarity， Graph features，Semantic features，TransR）对比。</p><p>下面是不同的模型在几种评价指标下的结果：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1546841231166.png" alt="1546841231166"></p><p>和只用图特征（ranker-gr）与只用语义特征（ranker-sem-noPMI）相比，两种特征集合可以有更好的效果。</p><p>下面是在k跳之内能够召回的父类比例：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1546841600059.png" alt="1546841600059"></p><h2 id="计算消耗和特征重要性"><a href="#计算消耗和特征重要性" class="headerlink" title="计算消耗和特征重要性"></a>计算消耗和特征重要性</h2><p>本文框架大概平均一分钟插入一个新概念。瓶颈在于随机游走和PMI，其他的特征能够在几秒间完成。PMI可以用【18】快速批量PMI执行来解决。所以瓶颈在于随机游走。</p><p>最重要的三个特征是：information propagation，random walk betweenness centrality和Katz similarity。</p><h2 id="在SemEval-Task-Benchmark-15-上的结果"><a href="#在SemEval-Task-Benchmark-15-上的结果" class="headerlink" title="在SemEval Task Benchmark[15]上的结果"></a>在SemEval Task Benchmark[15]上的结果</h2><p>对比模型：Random synset，First-Word-First-Sense（FWFS），MSejrKU System 2（Task 14的冠军模型）</p><p>评价指标：Accuracy (Wu&amp;Palmer Similarity)【46】，Lemma Match（大概是算法找出的同义词集合中至少包含正确同义词的比例）</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1546842089195.png" alt="1546842089195"></p><p>ranker-ETF-FWFS：使用FWFS中用到的二进制特征的排序模型。</p><p>有了FWFS特征的模型表现的最好，FWFS是一个很强的特征，用来排序父类，但是这个特征不容易泛化。</p><h2 id="新兴领域概念"><a href="#新兴领域概念" class="headerlink" title="新兴领域概念"></a>新兴领域概念</h2><p>ETF如何将新兴领域概念加到Wikipedia分类层次结构中。危机应对和医学领域。</p><p>结果表明：对于这两个领域，大多数预测的父类都很好，并且与许多手动分配的父类有重叠。</p><p>对于危机事件，ETF正确地标识了它们的发生年份和许多受影响的区域，只需要初始的文本输入量。</p><p>对医学领域，本文的方法可以区分禽流感和猪流感。还能够为新概念，在没有手动分配的情况下提出准确的分类。</p><h2 id="Quora-问答分类"><a href="#Quora-问答分类" class="headerlink" title="Quora 问答分类"></a>Quora 问答分类</h2><p>将Quora上的问题和答案放到Wikipedia分类中。结果：每个问答有平均8个父类，NDCG=0.445，F1=0.523.比其他的结果都好。</p><h1 id="工作评价"><a href="#工作评价" class="headerlink" title="工作评价"></a>工作评价</h1><p>本文对自动分类系统扩充问题提出了一个解决方案ETF：先通过术语生成的上下文为每个概念学习高维向量嵌入；然后从近邻概念祖先基于图特征和语义特征预测新概念的潜在父类。实验表明ETF在大规模知识图谱Wikipedia和WordNet上比其他算法表现要好。</p><p>ETF有应用于其他文本源（短的社交媒体文本）和分类系统类型（特殊领域）的潜质。允许简单的并行化，并且可以通过分布式实现可伸缩性。ETF使用的所有特性都可以在几秒钟内计算出来，除了random walk betweenness centrality特征。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;阅读文献来源&quot;&gt;&lt;a href=&quot;#阅读文献来源&quot; class=&quot;headerlink&quot; title=&quot;阅读文献来源&quot;&gt;&lt;/a&gt;阅读文献来源&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;Vedula N, Nicholson P K, Ajwani D, et al. Enri
      
    
    </summary>
    
      <category term="KG" scheme="http://yoursite.com/categories/KG/"/>
    
      <category term="nlp" scheme="http://yoursite.com/categories/KG/nlp/"/>
    
      <category term="分类系统填充" scheme="http://yoursite.com/categories/KG/nlp/%E5%88%86%E7%B1%BB%E7%B3%BB%E7%BB%9F%E5%A1%AB%E5%85%85/"/>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="论文阅读" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
      <category term="知识图谱" scheme="http://yoursite.com/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
      <category term="分类系统填充" scheme="http://yoursite.com/tags/%E5%88%86%E7%B1%BB%E7%B3%BB%E7%BB%9F%E5%A1%AB%E5%85%85/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读：Weakly-supervised Contextualization of Knowledge Graph Facts</title>
    <link href="http://yoursite.com/2019/01/07/2019-01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AWeakly-supervised%20Contextualization%20of%20Knowledge%20Graph%20Facts/"/>
    <id>http://yoursite.com/2019/01/07/2019-01/论文阅读：Weakly-supervised Contextualization of Knowledge Graph Facts/</id>
    <published>2019-01-07T11:58:28.000Z</published>
    <updated>2019-01-07T11:59:52.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="阅读文献来源"><a href="#阅读文献来源" class="headerlink" title="阅读文献来源"></a>阅读文献来源</h1><p><strong>Voskarides N , Meij E , Reinanda R , et al. Weakly-supervised Contextualization of Knowledge Graph Facts[J]. 2018.</strong></p><p>第一作者是Nikos Voskarides，个人主页：<a href="https://nickvosk.github.io/，就读于美国弗吉尼亚大学（The" target="_blank" rel="noopener">https://nickvosk.github.io/，就读于美国弗吉尼亚大学（The</a> University of Virginia）。研究重点是使用结构化和非结构化信息源提升信息访问。目前的工作是通过描述和语境化内容，使最终用户更容易访问搜索领域的知识图谱。发表的paper：</p><ul><li><strong>Weakly-supervised Contextualization of Knowledge Graph Facts</strong> [<a href="https://arxiv.org/abs/1805.02393" target="_blank" rel="noopener">pdf</a>|<a href="https://www.techatbloomberg.com/research-weakly-supervised-contextualization-knowledge-graph-facts/" target="_blank" rel="noopener">data</a>]<br>Nikos Voskarides, Edgar Meij, Ridho Reinanda, Abhinav Khaitan, Miles Osborne, Giorgio Stefanoni, Kambadur Prabhanjan and Maarten de Rijke.<br>SIGIR 2018.</li><li><strong>Generating Descriptions of Entity Relationships</strong> [<a href="https://staff.fnwi.uva.nl/m.derijke/wp-content/papercite-data/pdf/voskarides-generating-2017.pdf" target="_blank" rel="noopener">pdf</a>|<a href="https://github.com/nickvosk/ecir2017-gder-dataset" target="_blank" rel="noopener">data</a>]<br>Nikos Voskarides, Edgar Meij, and Maarten de Rijke.<br>ECIR 2017.</li><li><strong>Learning to Explain Entity Relationships in Knowledge Graphs</strong> [<a href="https://www.aclweb.org/anthology/P15-1055" target="_blank" rel="noopener">pdf</a>|<a href="https://github.com/nickvosk/acl2015-dataset-learning-to-explain-entity-relationships" target="_blank" rel="noopener">data</a>]<br>Nikos Voskarides, Edgar Meij, Manos Tsagkias, Maarten de Rijke and Wouter Weerkamp.<br>ACL 2015.</li><li><strong>Query-dependent contextualization of streaming data</strong> [<a href="https://staff.fnwi.uva.nl/m.derijke/wp-content/papercite-data/pdf/voskarides-query-dependent-2014.pdf" target="_blank" rel="noopener">pdf</a>|<a href="https://nickvosk.github.io/resources/ecir2014_dataset.zip" target="_blank" rel="noopener">data</a>]<br>Nikos Voskarides, Daan Odijk, Manos Tsagkias, Wouter Weerkamp, and Maarten de Rijke.<br>ECIR 2014.</li></ul><p>Edgar Meij：个人主页：<a href="https://edgar.meij.pro/。" target="_blank" rel="noopener">https://edgar.meij.pro/。</a></p><p>Ridho Reinanda：<a href="https://www.linkedin.com/in/ridhoreinanda/" target="_blank" rel="noopener">https://www.linkedin.com/in/ridhoreinanda/</a></p><h1 id="本文解决的科学问题及研究动机"><a href="#本文解决的科学问题及研究动机" class="headerlink" title="本文解决的科学问题及研究动机"></a>本文解决的科学问题及研究动机</h1><h2 id="问题来源"><a href="#问题来源" class="headerlink" title="问题来源"></a>问题来源</h2><p>KG可以应用于搜索，查询理解，推荐和问答等。最近，[10,17]表明搜索引擎用户发现实体卡片（提供关于检索目标实体的信息）是有用的，就把检索目标实体相关的信息以一种实体卡片集合的方式呈现出来。之前有一些工作就专注于增量检索实体相关实体卡片。<br>但是用户对KG事实感兴趣，而不是对实体感兴趣。比如“谁创立了微软”=》“比尔盖茨”。通过提供检索相关的额外事实可以增加用户对事实的理解。比如额外相关的事实有“比尔盖茨的专业领域”“微软的创立日期”“微软的主要产业”“微软的联合创始人”</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1546824712938.png" alt="1546824712938"></p><p>检索相关事实还可以用于QA，自然语言生成。<br>KG事实语境化是通过额外有用的KG事实增加给定KG事实的一类任务。这个任务的难点在于KG的大尺寸：即使是在给定事实小范围邻居内探索其他相关的事实，都会产生数量非常大的候选实体。</p><h2 id="问题数学定义"><a href="#问题数学定义" class="headerlink" title="问题数学定义"></a>问题数学定义</h2><p>E为实体集合，包括非CVT实体$E_n$和CVT实体$E_c$（CVT是指复合值类型实体，比如讲一个人的家庭关系，有他的爸爸是谁，妈妈是谁，那么家庭关系就是一个CVT）。<br>P为谓语<br>K为KG，包含&lt;s,p,o&gt;，即主谓宾，s和o属于E，p属于P，K可被认为是带标签的有向图。<br>path路径在KG中的表示$&lt;s_0, p_0, t_0&gt;,…,&lt;s_m, p_m, t_m&gt;$，其中$t_i=s_{i+1}$<br>fact事实定义为K中的一条路径满足以下任一种情况：</p><ol><li>包含一个三元组，$s_0$属于E，$t_0$属于$E_n$</li><li>包含两个三元组，$s_0,t_1$属于$E_n$，$t_0=s_1$属于$E_c$<br> R关系的集合，r属于R，是有同样谓语不同实体的事实集合的标记。事实f可以表示为r&lt;s,t&gt;。</li></ol><p>给定查询事实$f_q$和K，目标是找到K中相关的事实集F，并对其通过与$f_q$的相关性进行排序。</p><h1 id="算法框架"><a href="#算法框架" class="headerlink" title="算法框架"></a>算法框架</h1><p>本文提出了一种神经事实语境化方法NFCM来解决这个任务。NFCM包括候选事实生成，语境相关候选事实排序。排序模型采用监督学习，特征包括两部分：自动生成的和人工特征。远程监督生成训练数据。具体来说：</p><h2 id="列举候选实体"><a href="#列举候选实体" class="headerlink" title="列举候选实体"></a>列举候选实体</h2><p>给定一个检索事实$f_q$，从K中得到候选事实集F。<br>因为KG的庞大，限制F在实体s和t的<strong>两跳邻居</strong>中取得。<br>对应算法如下：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1546824740444.png" alt="1546824740444"></p><p>三条规则：</p><ul><li>CVT不作为一跳</li><li>不把检索事实放入F</li><li>为了减少搜索空间，不扩张表示类或类型的实体<h2 id="事实排序"><a href="#事实排序" class="headerlink" title="事实排序"></a>事实排序</h2>对每个候选事实$f_c$属于F，建立$(f_q,f_c)$对，类似于检索-文档对，并使用函数u进行评分，越高代表相关度越高。得到排序后的候选事实集合F’。<br>接下来是学习函数u：<br>训练一个端到端的神经网络，使用mini-batch和随机梯度下降。<br>使用Adam优化参数，最小化平均pairwise均方误差：<br><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1546824756646.png" alt="1546824756646"><br>$x_1=(f_q,f_{c_1})$和$x_2=(f_q,f_{c_2})$是事实对，l(x)函数是事实对的相关性标记，值在01之间。<br><strong>神经网络结构：</strong></li></ul><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1546824771015.png" alt="1546824771015"></p><p>使用RNN编码查询事实$f_q$为$v_q$。不编码实体，取而代之编码实体的类型聚合。编码候选事实的时候，先列举到检索事实两跳距离内的所有路径，标为$A$。先用RNN编码A中所有路径，然后组合所有编码路径。<br>最后，使用多层感知机MLP-o集合三个输入（检索事实编码，路径编码，手工特征），RELU作为隐藏层激活函数，sigmoid作为输出层激活函数。</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>KG使用Freebase<br><strong>数据集：</strong>包括检索事实，候选事实，相关度标记。</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1546824788957.png" alt="1546824788957"></p><p>上面是通过远程监督从Freebase中提取到的数据集。<br>接下来是得到相关度标记：<br>从Wikipedia筛选得到主要实体在Freebase中的文章。第一步：使用[32]中的实体链接方法来增量有额外实体链接的文章。第二步：保留包含t的文章片段（包含t的句子和前后各一个句子）。最后从这些片段中统计相关度。<br><strong>手工标记评价数据集：</strong>使用crowdsourcing得到评价集合。<br><strong>评价指标：</strong>MAP，NDCG@5，NDCG@10和MRR。</p><h2 id="启发式baseline"><a href="#启发式baseline" class="headerlink" title="启发式baseline"></a>启发式baseline</h2><p>之前的工作没有解决这种问题的，因此本文设计了一些baseline：</p><ul><li>Fact informativeness（FI）：与检索事实无关</li><li>平均谓语相似度（APS）：如果两个事实的谓语相似，那么两个事实可能相关</li><li>平均实体相似度（AES）：如果两个事实的实体相似，那么两个事实可能相关<h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2>实验的设置为了回答下面四个问题：<br><strong>RQ1</strong>：NFCM和一些启发式baseline在众包数据集上的表现如何</li></ul><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1546824807991.png" alt="1546824807991"></p><p>NFCM表现大大超过baseline。这说明本文提出的任务不是很轻易就能够解决的。<br><strong>RQ2</strong>：NFCM和在众包数据集上使用远程监督得到的相关度标记的检索事实上使用打分函数得到的效果相比表现如何？（NFCM和远程监督得到的打分函数DistSup相比）</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1546824816782.png" alt="1546824816782"></p><p>NFCM在三个指标上都大大超过了DistSup。学习到的排序函数对该任务是有效的。<br><strong>RQ3</strong>：NFCM能从手工特征和自动学到的特征上受益吗？<br>调整LF（学习到的特征）和HF（手工特征）在验证集上的参数。</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1546824825392.png" alt="1546824825392"></p><p>NFCM大大超过HF和LF，意味着结合HF和LF可以获得更相关的结果。未来工作在更复杂的结合HF和LF。</p><p><strong>RQ4</strong>：NFCM的per-relationship表现怎么样？每对关系的实例数量是如何影响那个排名表现的？</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1546824844573.png" alt="1546824844573"></p><p>NFCM表现最好的关系是profession，最差的是awardNominated。awardNominated有很多候选事实，可能是差表现的原因之一。</p><h1 id="工作评价"><a href="#工作评价" class="headerlink" title="工作评价"></a>工作评价</h1><p>本文提出了一个新的任务：知识图谱事实语境化任务，以及提出了一个方法NFCM。NFCM首先通过查看两跳之内的邻居，产生检索事实的候选集合，然后通过监督机器学习排序候选事实。NFCM结合手工特征和深度学习自动学到的特征。使用远程监督来收集训练数据，通过使用和Freebase有大量重叠实体的Wikipedia。<br>实验结果表明：远程监督在这个任务中收集训练数据非常有效，NFCM效果很好，两种特征都对NFCM有重要作用。<br>未来旨在探索更复杂的特征结合方式（更好的模型）；探索其他产生训练集的方法，比如新闻文章和点击日志（更多的数据，更好的效果）；找到在搜索引擎页面上能够结合和呈现这些排序好的事实的方法（更好的落地）。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;阅读文献来源&quot;&gt;&lt;a href=&quot;#阅读文献来源&quot; class=&quot;headerlink&quot; title=&quot;阅读文献来源&quot;&gt;&lt;/a&gt;阅读文献来源&lt;/h1&gt;&lt;p&gt;&lt;strong&gt;Voskarides N , Meij E , Reinanda R , et al. We
      
    
    </summary>
    
      <category term="KG" scheme="http://yoursite.com/categories/KG/"/>
    
      <category term="nlp" scheme="http://yoursite.com/categories/KG/nlp/"/>
    
      <category term="事实语境化" scheme="http://yoursite.com/categories/KG/nlp/%E4%BA%8B%E5%AE%9E%E8%AF%AD%E5%A2%83%E5%8C%96/"/>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="论文阅读" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
      <category term="知识图谱" scheme="http://yoursite.com/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
      <category term="事实语境化" scheme="http://yoursite.com/tags/%E4%BA%8B%E5%AE%9E%E8%AF%AD%E5%A2%83%E5%8C%96/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读：Effective Deep Memory Networks for Distant Supervised Relation Extraction</title>
    <link href="http://yoursite.com/2019/01/02/2019-01/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AEffective%20Deep%20Memory%20Networks%20for%20Distant%20Supervised%20Relation%20Extraction/"/>
    <id>http://yoursite.com/2019/01/02/2019-01/论文阅读：Effective Deep Memory Networks for Distant Supervised Relation Extraction/</id>
    <published>2019-01-02T01:17:28.000Z</published>
    <updated>2019-01-02T01:18:38.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-阅读文献来源"><a href="#1-阅读文献来源" class="headerlink" title="1. 阅读文献来源"></a>1. 阅读文献来源</h1><p>Feng X, Guo J, Qin B, et al. Effective deep memory networks for distant supervised relation extraction[C]//Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI. 2017: 19-25.</p><p>作者单位来自哈尔滨工业大学</p><h1 id="2-论文解决的科学问题及研究动机"><a href="#2-论文解决的科学问题及研究动机" class="headerlink" title="2. 论文解决的科学问题及研究动机"></a>2. 论文解决的科学问题及研究动机</h1><h2 id="研究内容"><a href="#研究内容" class="headerlink" title="研究内容"></a>研究内容</h2><p>远程监督关系提取(RE)是一种不需要标记训练数据就能从文本中找到新的关系的有效方法。通常可以将其形式化为一个多实例多标签问题。</p><p>该文章介绍了一种新型的用于远距离监督RE的神经方法，并着重介绍了注意机制。</p><p>与基于特征的逻辑回归模型和组合神经模型(如CNN)不同，本文的方法包括两个主要的基于注意力的记忆组件，它们能够显式地捕捉每个上下文单词对实体对（entity pair）表示建模的重要性，以及关系之间的内在依赖关系。</p><p>这种重要程度和依赖关系是通过多个计算层来计算的，每个计算层都是外部存储器上的神经注意模型。</p><p>在真实数据集上的实验表明，本文的方法比各种基线更具有显著的、一致的性能。</p><h2 id="研究动机"><a href="#研究动机" class="headerlink" title="研究动机"></a>研究动机</h2><p>关系提取(RE)的目的是提取实体之间的语义关系。给出一个带注释的头实体$e_h$和尾实体$e_t$的句子$s$，RE的目的是预测$e_h$和$e_t$之间的关系。</p><p>RE是自然语言处理(NLP)中的一项基本任务，是构建结构化知识库的重要组成部分。</p><p>以前的方法包括基于特征的方法[Zhang et al., 2006; Li et al., 2012]和基于神经的方法[Nguyen and Grishman, 2015b]，能够基于人类标注提取高质量的关系。然而，注释的高成本通常会在规模和领域上限制现有的已标记的RE数据。</p><p>解决该问题的一个有效解决方案就是<strong>远程监管</strong>。远程监督能通过将关系数据库与文本进行对齐来自动生成训练数据[Mintz et al., 2009]。</p><p>为了方便文本的建模，神经网络在远程监督RE中得到了广泛的探索，并取得了最先进的结果[Zeng et al., 2015; Lin et al., 2016; Jiang et al., 2016]。</p><p>多种神经网络如卷积神经网络(CNN)和递归神经网络(RNN)等被采用来学习每个句子（实例）的表征，然后被用来作为关系分类中对应实体对的表征。</p><p>作者观察到：并不是所有的上下文词都对实体对的关系推断有同样的贡献；不同关系之间存在依赖关系(如蕴涵关系、冲突关系)，这是用隐式关系表达推断某些实例的关键线索。</p><p>因此，一个理想的解决方案不仅应该能够显式地捕捉不同上下文单词的重要性，而且应该能够自动地学习关系之间的依赖关系。</p><h1 id="3-所提算法创新的工作"><a href="#3-所提算法创新的工作" class="headerlink" title="3. 所提算法创新的工作"></a>3. 所提算法创新的工作</h1><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545794921586.png" alt="1545794921586"></p><h2 id="单词层记忆网络（Word-Level-Memory-Network）"><a href="#单词层记忆网络（Word-Level-Memory-Network）" class="headerlink" title="单词层记忆网络（Word-Level Memory Network）"></a>单词层记忆网络（Word-Level Memory Network）</h2><p>该层是一个单词层的记忆网络，它学习每个上下文单词相对于特定实体对的重要性/权重。</p><p>先将上下⽂句子编码成向量，比如存在句⼦$s={w_1,…,w_l}$，从中提取出实体对的词后得到<br>$$<br>{w_1,…,w_{e_h-1},w_{e_h+1},…,w_{e_t-1},w_{e_t+1},…,w_l}<br>$$<br>称其为外部记忆单元m。之后对上下⽂进行attention计算并求出相应字符的加权和求出上下文的向量表示。</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545790729160.png" alt="1545790729160"></p><p>如图所示，生成的上下文向量可以再次输入到下一层hop中计算新的attention并⽣成新的上下⽂向量表示，因此可以叠加多层的hop得到最终的基于注意力的上下文向量。</p><p>得到上下文向量后，将其与CNN模型得到的向量拼接在一起，得到关于该实体对的句子表征。</p><p><strong>单词注意力：</strong></p><p>注意机制的基本思想是，在计算上层表示时，对每一个较低的位置赋予一个权重(重要性)(Bahdanauet al.，2015)。直觉告诉我们语境词对句子语义的贡献是不相等的。此外，如果我们关注不同的实体对，单词的重要性应该是不同的。</p><p>我们通过注意力模型计算上层向量可以通过下述加权的方式。</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545809741451.png" alt="1545809741451"></p><p>其中m是外部记忆，α是权重，x是输出向量。</p><p>对于每个m，使用FNN来计算与实体对的语义相似度，评价函数如下：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545809748395.png" alt="1545809748395"></p><p>最后通过softmax来计算得到m最终的重要性分布。</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545809754585.png" alt="1545809754585"></p><h2 id="关系层基于注意力记忆网络（two-layer-relation-level-attention-based-memory-network）"><a href="#关系层基于注意力记忆网络（two-layer-relation-level-attention-based-memory-network）" class="headerlink" title="关系层基于注意力记忆网络（two-layer relation-level attention-based memory network）"></a>关系层基于注意力记忆网络（two-layer relation-level attention-based memory network）</h2><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545808833980.png" alt="1545808833980"></p><p>该层学习关系之间的依赖关系，将上下文向量编码（单词层记忆网络得到的输出）成多种关系向量。</p><p>如上图所示，我们为每对关系构建了一个两层记忆网络。</p><ol><li>第一层：依循[Lin et al., 2016]使用实例级别的注意力去挑选能够表达对应关系的句子。具体：随机选择一个向量作为关系的表征，然后通过注意力层来计算关系和每个句子之间的关联权重。</li><li>第二层：设计一个关系层的注意力模型，来学习关系之间的依赖关系。比如，如果一个人是一个公司的创立者（A，founder，B），那么A有大概率是B的股东（A，major_shareholders，B）。该层的输入是第一层的输出，该层的输出用于多标签关系分类。</li></ol><p><strong>关系注意力：</strong></p><p>探讨每个关系中所有句子的重要性，并学习关系之间的依赖关系。实体对的最终表示将是句子表示的组合。</p><ul><li><p>实例上的选择性注意力</p><p>最终表征取决于所有句子的表征，每个句子具有实体对是否包含关系的信息。因为，最终表征可以被计算为句子表征的加权和：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545810130725.png" alt="1545810130725"></p><p>然后学习$\beta$权重：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545810169928.png" alt="1545810169928"></p><p>其中$z_i$表明输入句子和预测关系之间的匹配度[Lin et al., 2016]：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545810322564.png" alt="1545810322564"></p></li><li><p>关系上的选择性注意力</p><p>为了探索所有关系的依赖，使用选择性注意力来计算每个关系之间的相似度。输入为前者输出：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545810445408.png" alt="1545810445408"></p><p>$\gamma_{ji}$是关系表征$R_j$和$R_i$之间的相似度，计算如下：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545810536552.png" alt="1545810536552"></p><p>其中$h_i$表明$R_j$和$R_i$之间的相似度，计算如下：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545810580889.png" alt="1545810580889"></p></li></ul><h2 id="远程监督关系抽取"><a href="#远程监督关系抽取" class="headerlink" title="远程监督关系抽取"></a>远程监督关系抽取</h2><p>对于每种关系对应的特征向量$R^*_i$，将其输入一个二分类器中通过logistic回归得到其是否存在关系的概率值。</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545810667088.png" alt="1545810667088"></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545810683481.png" alt="1545810683481"></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545810694669.png" alt="1545810694669"></p><h1 id="4-实验"><a href="#4-实验" class="headerlink" title="4. 实验"></a>4. 实验</h1><h2 id="数据集："><a href="#数据集：" class="headerlink" title="数据集："></a>数据集：</h2><p>NYT10：[Riedel et al., 2010]</p><p>该数据集是通过对齐Freebase与《纽约时报》(NYT)语料库生成的关系，2005年和2006年的句子用于训练，2007年的句子用于测试。</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545812077634.png" alt="1545812077634"></p><h2 id="对比算法实验"><a href="#对比算法实验" class="headerlink" title="对比算法实验"></a>对比算法实验</h2><ul><li>基于特征的方法：Mintz，Multir，MIML</li><li>基于神经的方法：PCNN，ATT</li></ul><p>结果：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545812150895.png" alt="1545812150895"></p><h2 id="两种记忆网络的影响"><a href="#两种记忆网络的影响" class="headerlink" title="两种记忆网络的影响"></a>两种记忆网络的影响</h2><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545812159417.png" alt="1545812159417"></p><h2 id="Hop数量的影响"><a href="#Hop数量的影响" class="headerlink" title="Hop数量的影响"></a>Hop数量的影响</h2><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545812214181.png" alt="1545812214181"></p><h1 id="5-作者工作评价"><a href="#5-作者工作评价" class="headerlink" title="5. 作者工作评价"></a>5. 作者工作评价</h1><p>本文设计了一种新的神经网络模型，该模型具有两个记忆网络，用于远程监督RE。在第一个词级记忆网络中，本文的模型捕捉上下文词的重要性，并自动对句子的语义表示建模。本文还设计了关系级记忆网络，以捕获关系之间的依赖关系，并结合多实例多标签学习。实验结果表明，与同类方法相比，该方法有明显的改进。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-阅读文献来源&quot;&gt;&lt;a href=&quot;#1-阅读文献来源&quot; class=&quot;headerlink&quot; title=&quot;1. 阅读文献来源&quot;&gt;&lt;/a&gt;1. 阅读文献来源&lt;/h1&gt;&lt;p&gt;Feng X, Guo J, Qin B, et al. Effective deep 
      
    
    </summary>
    
      <category term="nlp" scheme="http://yoursite.com/categories/nlp/"/>
    
      <category term="关系抽取" scheme="http://yoursite.com/categories/nlp/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/"/>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="论文阅读" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
      <category term="关系抽取" scheme="http://yoursite.com/tags/%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96/"/>
    
      <category term="远程监督" scheme="http://yoursite.com/tags/%E8%BF%9C%E7%A8%8B%E7%9B%91%E7%9D%A3/"/>
    
  </entry>
  
  <entry>
    <title>《CS224n: Natural Language Processing with Deep Learning》Assignments3: 理论推导部分</title>
    <link href="http://yoursite.com/2018/12/25/2018-12/%E3%80%8ACS224n-Natural-Language-Processing-with-Deep-Learning%E3%80%8BAssignments3-%E7%90%86%E8%AE%BA%E6%8E%A8%E5%AF%BC%E9%83%A8%E5%88%86/"/>
    <id>http://yoursite.com/2018/12/25/2018-12/《CS224n-Natural-Language-Processing-with-Deep-Learning》Assignments3-理论推导部分/</id>
    <published>2018-12-25T02:49:08.000Z</published>
    <updated>2020-03-10T06:34:29.022Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-3"><a href="#Assignment-3" class="headerlink" title="Assignment #3"></a>Assignment #3</h1><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/blog/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20200310143305.png" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-3&quot;&gt;&lt;a href=&quot;#Assignment-3&quot; class=&quot;headerlink&quot; title=&quot;Assignment #3&quot;&gt;&lt;/a&gt;Assignment #3&lt;/h1&gt;&lt;p&gt;&lt;img src=&quot;https://hellojet-b
      
    
    </summary>
    
      <category term="nlp" scheme="http://yoursite.com/categories/nlp/"/>
    
    
      <category term="CS224n" scheme="http://yoursite.com/tags/CS224n/"/>
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>论文简读：word sense disambiguation：a survey</title>
    <link href="http://yoursite.com/2018/12/24/2018-12/%E8%AE%BA%E6%96%87%E7%AE%80%E8%AF%BB%EF%BC%9Aword%20sense%20disambiguation%EF%BC%9Aa%20survey/"/>
    <id>http://yoursite.com/2018/12/24/2018-12/论文简读：word sense disambiguation：a survey/</id>
    <published>2018-12-24T01:44:09.000Z</published>
    <updated>2018-12-24T02:37:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>Pal A R , Saha D . Word sense disambiguation: a survey[J]. 2018.</p><p>更具体的版本可以看这个：</p><p>Navigli R . Word sense disambiguation: A survey[M]. ACM, 2009.</p><h1 id="词义消歧：Word-Sense-Disambiguation-WSD"><a href="#词义消歧：Word-Sense-Disambiguation-WSD" class="headerlink" title="词义消歧：Word Sense Disambiguation (WSD)"></a>词义消歧：Word Sense Disambiguation (WSD)</h1><p>在世界上所有的主要语言中，有很多词在不同的语境中表示不同的意思。</p><p>词义消歧是在特定语境中发现歧义词准确意义的一种技术。</p><p>词义消歧的方法主要分为三类:基于知识的、受监督的和无监督的方法。</p><h1 id="词义消歧的应用"><a href="#词义消歧的应用" class="headerlink" title="词义消歧的应用"></a>词义消歧的应用</h1><ul><li>机器翻译(MT):机器翻译(MT)需要WSD[14-17]，因为每一种语言中的词根据它们的上下文都有不同的翻译。例如，在英语句子中，“He got a goal”，“It was his goal in life”——“goal”这个词有不同的含义，这是语言翻译中的一个大问题。</li><li>信息检索(IR):在IR[18-23]系统中，解决查询中的歧义是最关键的问题。例如，查询中的“depression”一词可能具有与疾病、天气系统或经济学不同的含义。因此，在找到一个特定问题的答案之前，找到一个模糊词的确切含义是这方面最重要的问题。</li><li>信息提取(IE)和文本挖掘:WSD在生物信息学研究、命名实体识别系统等不同的研究工作中发挥着重要的信息提取作用。</li></ul><h1 id="词义消歧的方法"><a href="#词义消歧的方法" class="headerlink" title="词义消歧的方法*"></a>词义消歧的方法*</h1><h2 id="基于知识的WSD"><a href="#基于知识的WSD" class="headerlink" title="基于知识的WSD"></a>基于知识的WSD</h2><p>基于机器可读词典或词义库、词典等不同知识来源的基于知识的方法。Wordnet (Miller 1995)是本研究领域使用最多的机器可读词典。一般采用四种主要的基于知识的方法。</p><ul><li><p>LESK算法</p><p>第一个基于机器可读字典的词义消歧算法，取决于词典对一个句子中单词定义的重叠。</p><p>从句子中提取短语-&gt;从在线词典中收集信息(该词的不同意义和短语中其他有意义的词)-&gt;比较两者-&gt;最大重叠表示该歧义词的在此处的意义</p></li><li><p>语义相似度</p></li><li><p>选择偏好</p><p>找到词类之间可能的关系，使用知识库表示共同的词义。</p></li><li><p>启发式算法</p><p>三种启发式算法作为评价WSA系统的baseline：</p><p> 1) Most Frequent Sense,</p><p> 2) One Sense per Discourse and</p><p> 3) One Sense per Collocation</p></li></ul><h2 id="受监督的WSD"><a href="#受监督的WSD" class="headerlink" title="受监督的WSD"></a>受监督的WSD</h2><p>手工创建标注数据，通过分类器学习训练集。</p><ul><li><p>决策表</p><p>决策表[33-35]是一组“if-then-else”规则。在决策表中使用训练集来归纳给定单词的特征集。</p></li><li><p>决策树</p><p>决策树[36-38]用于表示树结构中递归划分训练数据集的分类规则。决策树的内部节点表示将应用于某个特征值的测试，每个分支表示测试的输出。叶节点表示单词的含义(如果可能的话)。</p></li><li><p>朴素贝叶斯</p><p>朴素贝叶斯分类器[39-41]是一种基于贝叶斯定理的概率分类器。</p></li><li><p>神经网络</p><p>在基于神经网络的计算模型[42- 45,64]中，人工神经元采用连接主义方法进行数据处理。输入是特征对，目标是将训练上下文划分为非重叠集合。接下来，为了产生更大的激活（目标函数），逐渐调整这些新形成的对和连接权重。</p></li><li><p>基于范例或基于实例的学习</p><p>该监督算法通过实例建立分类模型[40,46]。该模型将实例存储为特征空间中的点，并考虑新的实例进行分类。</p></li><li><p>支持向量机</p><p>基于支持向量机的算法[47-49]使用了结构风险最小化理论。这种方法的目的是用最大边距来区分正例子和负例子，边距是超平面到最近的正例子和负例子的距离。最接近超平面的正负例子称为支持向量。</p></li><li><p>集成方法</p><ul><li><p>多数投票</p></li><li><p>概率混合</p><p>该策略首先采用一阶分类器对目标词的意义进行置信度评价，然后进行归一化处理。结果得到了词义的概率分布。接下来，将这些概率相加，并将得分最高的词义视为所需词义。</p></li><li><p>基于排名的组合</p></li><li><p>AdaBoost</p><p>AdaBoost[51,52]是一种利用多个弱分类器的线性组合来创建强分类器的方法。该方法从以前的分类器中找到分类错误的实例，以便可以用于以后的分类器。从加权训练集中学习分类器，一开始所有的权重都是相等的。在每个步骤中，它对每个分类器执行特定的迭代。在每次迭代中，不正确的分类器的权重都会增加，以便后续的分类器可以关注那些不正确的示例。</p></li></ul></li></ul><h2 id="非监督的WSD"><a href="#非监督的WSD" class="headerlink" title="非监督的WSD"></a>非监督的WSD</h2><p>不依赖于外部知识来源或词义清单、机器可读的字典或标注好的数据集。</p><ul><li><p>上下文聚类</p><p>上下文聚类方法[56,57]是一种基于聚类技术的方法，在聚类技术中，首先创建上下文向量，然后将它们分组成簇，以识别单词的含义。</p></li><li><p>词聚类</p><p>这种技术在消歧词义方面类似于上下文聚类，但它将语义相同的词聚类。</p></li><li><p>共现图</p><p>该方法创建顶点V和边E的共现图，其中V表示文本中的单词，如果单词按照同一段落或文本中的语法在关系中共现，则添加E。对于给定的目标字，首先创建图形，并创建图形的邻接矩阵。然后，应用马尔科夫聚类方法来进行词义消歧。</p></li><li><p>基于生成树的方法</p></li></ul><h1 id="最新表现（2015）"><a href="#最新表现（2015）" class="headerlink" title="最新表现（2015）*"></a>最新表现（2015）*</h1><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545618622138.png" alt="1545618622138"></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545618662407.png" alt="1545618662407"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Pal A R , Saha D . Word sense disambiguation: a survey[J]. 2018.&lt;/p&gt;
&lt;p&gt;更具体的版本可以看这个：&lt;/p&gt;
&lt;p&gt;Navigli R . Word sense disambiguation: A surv
      
    
    </summary>
    
      <category term="nlp" scheme="http://yoursite.com/categories/nlp/"/>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="词义消歧" scheme="http://yoursite.com/tags/%E8%AF%8D%E4%B9%89%E6%B6%88%E6%AD%A7/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读：Linear Algebraic Structure of Word Senses, with Applications to Polysemy</title>
    <link href="http://yoursite.com/2018/12/19/2018-12/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ALinear%20Algebraic%20Structure%20of%20Word%20Senses,%20with%20Applications%20to%20Polysemy/"/>
    <id>http://yoursite.com/2018/12/19/2018-12/论文阅读：Linear Algebraic Structure of Word Senses, with Applications to Polysemy/</id>
    <published>2018-12-19T14:23:00.000Z</published>
    <updated>2018-12-19T14:23:28.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-阅读文献来源"><a href="#1-阅读文献来源" class="headerlink" title="1. 阅读文献来源"></a>1. 阅读文献来源</h1><p>Arora S, Li Y, Liang Y, et al. Linear algebraic structure of word senses, with applications to polysemy[J]. Transactions of the Association of Computational Linguistics, 2018, 6: 483-495.</p><p>Sanjeev Arora（大牛）：Professor of Computer Science，Princeton University，theoretical machine learningtheoretical computer science</p><p>Yingyu Liang： University of Wisconsin-Madison，machine learning</p><h1 id="2-论文解决的科学问题及研究动机"><a href="#2-论文解决的科学问题及研究动机" class="headerlink" title="2. 论文解决的科学问题及研究动机"></a>2. 论文解决的科学问题及研究动机</h1><p>Firth’s hypothesis：一个单词的意思可以通过周围词的分布得到（Firth，1957）。所以现在的词嵌入通过一个实值向量表示一个单词的意思。</p><p>比较经典的模型有(Turney et al., 2010)：词的共现矩阵统计、神经网络模型（word2vec)，使用非凸优化(Mikolov et al., 2013a;b)。</p><p><strong>而问题在于</strong>当一词多义的情况出现，一个词嵌入所表示的意义是不明确的。</p><p><strong>现行的解决思路：</strong></p><ul><li>通过内积来提取隐藏信息，效果不好</li><li>线性叠加，通过简单稀疏编码恢复</li></ul><p><strong>现行的解决方案（Word Sense Induction）：</strong></p><ul><li>通过Yarowsky (1995)中的几种范例方法来自动学习语义</li><li>通过聚类邻居词来识别多义</li><li>通过将嵌入扩张为更复杂的表示，而不是单个向量，来捕捉单词的含义(Mruphy et al., 2012; Huang et al., 2012)，但是在缺少WordNets的语言上没办法使用</li></ul><p>本文旨在提出一种从词嵌入中提取出不同意义，用来解决一词多义问题的方法。</p><p>通过本文算法可以进一步解决WSD（word sense disambiguation 语义消歧）问题，为其创造标注语料库或者WordNet。</p><h1 id="3-所提算法创新的工作"><a href="#3-所提算法创新的工作" class="headerlink" title="3. 所提算法创新的工作"></a>3. 所提算法创新的工作</h1><ul><li><p>本文使用语料库仅仅是为了生成词向量，而基于图的聚类方法在语料库上进行work，速度非常慢。</p></li><li><p>本文算法恢复得到的不同语义是通过话语原语相互联系的。</p></li><li><p>本文算法完全非监督，不需要大量标注集</p></li><li><p>使用了稀疏编码，曾经没有出现在解决一词多义词嵌入问题中</p></li><li><strong>通过话语原语（discourse atom）提取语义：</strong></li></ul><p>先说语义的线性结构，“tie”具有很多意思，其词嵌入应该约等于它的意思的加权和。（这个在作者16年工作中句子向量也提到过）。当然作者是做了一些实验发现这个的：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545224903660.png" alt="1545224903660"></p><p>这个实验通过合并两个单词创建一个多义词：删除这两个词并替换成一个词，进而计算该新词的词嵌入，计算方式和之前计算两个词的方法相同。改变两个词的频率，做大量实验，结果发现新词的向量总是接近两个词的张量子空间。</p><p>那么如何从词嵌入中提取不同的语义信息呢？</p><p>上述实验表明：<br>$$<br>v_{tie} = \alpha_1v_{tie1}+\alpha_2v_{tie2}+\alpha_3v_{tie3}+…<br>$$<br>但是这种组合是无穷的，我们还需要其他的信息帮助我们提取语义。</p><p><strong>借助不同词的语义关联</strong></p><p>比如，“tie”的“article of clothing”与shoe，jacket等相连。</p><p>为此，需要用到作者16年工作Arora et al. (2016)中的<strong>话语模型随机游走</strong>。</p><p>这个模型建立在(Mnih and Hinton, 2007)的逻辑线性主题模型上，原模型定义了主题c与词w之间的一个分布，而随机游走模型将c视为一个话语（在语料库的这个地方正在谈论什么），通过允许c游走来构建语料库：当走到c时，根据原模型的分布来生成一些词，这些词是与c在余弦上相近的。</p><p>有了以上的基础之后，tie1和tie2等对应于tie周围不同词的分布，也可以叫做不同的话语（话语就是一种词的分布）。比如，遇到“clothing”话语，高概率产生tie1（article of clothing），shoe，jacket等词。</p><p>进一步，<strong>“clothing”话语与shoe，jacket，tie1等有很高的内积。</strong>（这是根据原模型w关于c的分布的公式得到的）</p><p>接下来，就引入一个全局优化，目标是找出所有的话语原语：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545226941682.png" alt="1545226941682"></p><p>系数α非零（叫做硬稀疏约束），加号后面一项是噪声向量。</p><p>A和α都是未知的，所以该问题是非凸的，涉及到稀疏编码，本文使用标准k-SVD算法来解决做个问题。</p><p>这里的A就是话语原语。</p><p><strong>通俗解释：训练可以得到话语原语，通过话语原语可以重现一个单词向量。这些话语原语就是我们讲的一词多义中的多义。而如何解释某个原语意义就看与这个原语关联最近的一些词。</strong></p><h1 id="4-实验"><a href="#4-实验" class="headerlink" title="4. 实验"></a>4. 实验</h1><h2 id="话语原语实验"><a href="#话语原语实验" class="headerlink" title="话语原语实验"></a>话语原语实验</h2><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545210119277.png" alt="1545210119277"></p><p>上图包含了一些话语原语以及他们的近邻词，出现在一个话语中的单词通常彼此相近</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545210249432.png" alt="1545210249432"></p><p>上图是连接到“chair”和“bank”的五个话语原语，每个原语通过六个最近的词来表示。比如上面表中是连接到“chair”的五个原语1187,739,590,1322,1457.</p><p><strong>话语原语的层级结构：</strong></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545210621661.png" alt="1545210621661"></p><p>上图是科学领域使用原语的一个例子，biology和chemistry的线性组合与跨学科biochemistry相近。</p><h2 id="定量测试"><a href="#定量测试" class="headerlink" title="定量测试"></a>定量测试</h2><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545227648755.png" alt="1545227648755"></p><p>m代表正确的含义数和干扰项，所有的算法从中选择k个正确的含义。</p><p>m=20，k=4时，本文算法precision=0.63，recall=0.70。</p><p>在m20时，本文算法与非本语说话者表现相同。</p><h1 id="5-作者工作评价"><a href="#5-作者工作评价" class="headerlink" title="5. 作者工作评价"></a>5. 作者工作评价</h1><p>从(Arora et al . 2016年)获得的词嵌入，还有其他几个方法，已被证明包含这个词的不同意义，通过简单的稀疏编码可提取出。目前似乎名词做得比其他词性更好。本文方法的一个新颖之处在于，词义与2000个话语向量相关联。这使得该方法在NLP其他领域的任务，与WordNets一样有用。这种方法在半自动化模式下更有用，更准确，因为人工更能够识别词义。</p><p>对数可能是本文算法成功的关键，因为对数使得很少出现的词义有一个合理的权重。</p><h1 id="6-自己对该论文的评价"><a href="#6-自己对该论文的评价" class="headerlink" title="6. 自己对该论文的评价"></a>6. 自己对该论文的评价</h1><p>本文通过提出话语原语（discourse atom）的概念来表示一个词向量中包含的各种语义，在数学和实践上都证明了各种方法生成的词向量都近似于该词各种含义的向量线性组合这一点。这对nlp领域的发展具有重大的意义。</p><p>在实体链接任务中可以通过判断一个词的语义来更准确的链接到知识库中，可行性很高，具体来说：</p><p>通过语料库训练每个词的词向量以及话语原语向量，以及词向量和话语原语之间的余弦相似度（判断其含义是否近似），拿到一个mention之后，先判断该词与哪个话语原语有较高的相似度，可以作为一种辅助决策，也可以进一步根据上下文来判断该mention对应的实体。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-阅读文献来源&quot;&gt;&lt;a href=&quot;#1-阅读文献来源&quot; class=&quot;headerlink&quot; title=&quot;1. 阅读文献来源&quot;&gt;&lt;/a&gt;1. 阅读文献来源&lt;/h1&gt;&lt;p&gt;Arora S, Li Y, Liang Y, et al. Linear algebr
      
    
    </summary>
    
      <category term="nlp" scheme="http://yoursite.com/categories/nlp/"/>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="词嵌入" scheme="http://yoursite.com/tags/%E8%AF%8D%E5%B5%8C%E5%85%A5/"/>
    
      <category term="论文阅读" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读：A Simple but Tough-to-beat Baseline for Sentence Embeddings</title>
    <link href="http://yoursite.com/2018/12/17/2018-12/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AA%20Simple%20but%20Tough-to-beat%20Baseline%20for%20Sentence%20Embeddings/"/>
    <id>http://yoursite.com/2018/12/17/2018-12/论文阅读：A Simple but Tough-to-beat Baseline for Sentence Embeddings/</id>
    <published>2018-12-17T08:05:00.000Z</published>
    <updated>2018-12-17T08:06:17.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-阅读文献来源"><a href="#1-阅读文献来源" class="headerlink" title="1. 阅读文献来源"></a>1. 阅读文献来源</h1><p>Arora S, Liang Y, Ma T. A simple but tough-to-beat baseline for sentence embeddings[J]. 2016.</p><p>Sanjeev Arora：普林斯顿大学教授，研究领域是理论机器学习和理论计算机科学</p><p>Yingyu Liang：威斯康星大学</p><p><a href="https://github.com/PrincetonML/SIF" target="_blank" rel="noopener">源代码</a></p><h1 id="2-论文研究动机"><a href="#2-论文研究动机" class="headerlink" title="2. 论文研究动机"></a>2. 论文研究动机</h1><p>神经网络在计算词嵌入方面的成功也推动了研究者们探索生成句子或段落的语义嵌入的方法。</p><p>词嵌入作为NLP和IR中的基础构建块，捕捉词之间的相似度(Bengio et al., 2003; Collobert &amp; Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014)。</p><p>最近的工作尝试通过计算词嵌入来捕捉词序列（如短语，句子，段落等）的语义，这些方法从简单的词向量叠加到复杂的CNN，RNN(Iyyer et al., 2015; Le &amp; Mikolov, 2014; Kiros et al., 2015; Socher et al., 2011; Blunsom et al., 2014; Tai et al., 2015; Wang et al., 2016)</p><p>(Wieting et al., 2016)从标准词嵌入入手，在数据集(PPDB)的监督下对标准词嵌入进行修改，通过训练一个简单的词平均模型构建句子嵌入。然而，来自数据集的监督似乎至关重要，因为他们报告说，初始词嵌入的简单平均值并不能很好地工作。</p><p>本文在(Wieting et al., 2016)基础上进一步提出自己的工作。</p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p><strong>词嵌入</strong></p><p>词嵌入将词表示为低维空间中的连续向量，捕捉词的词汇和语义特性。它们可以从文本的神经网络模型的内部表示得到(Bengio et al., 2003; Collobert &amp; Weston, 2008; Mikolov et al., 2013a)或通过共现统计学的低秩近似(Deerwester et al., 1990; Pennington et al., 2014)。这两种方法是密切相关的(Levy &amp; Goldberg, 2014; Hashimoto et al., 2016; Arora et al., 2016)。</p><p><strong>短语/句子/段落嵌入</strong></p><p>以往的研究都是通过对向量和矩阵的运算来组合词嵌入来计算短语或句子嵌入(Mitchell &amp; Lapata, 2008; 2010; Blacoe &amp; Lapata, 2012)。其中coordinate-wise multiplication表现得很好。非加权平均对短语的表示方面做得很好(Mikolov et al., 2013a)。</p><p>另一种方法是在解析树上定义的递归神经网络(RNNs)，监督学习(Socher et al., 2011) 或者非监督 (Socher et al., 2014)。简单的RNNs可以看作是解析树被简单的线性链替换的特殊情况。</p><p>(Mikolov et al., 2013b)：Skip-gram模型被扩展为合并序列的潜在向量，或将序列视为一个基础单元。</p><p>(Le &amp; Mikolov, 2014) ：每个段落假设有一个潜在段落向量，影响段落中每个词的分布。</p><p>(Kiros et al., 2015)：Skip-thought重构某个句子周围的句子，将隐藏参数视为向量表示。</p><p>(Tai et al., 2015)：RNNs使用LSTM捕捉长距离依赖，用于建模句子。</p><p>(Blunsom et al., 2014)：CNN，使用动态pooling来处理变长句子，在语义预测和分类任务中表现良好。</p><h1 id="3-算法内容"><a href="#3-算法内容" class="headerlink" title="3. 算法内容"></a>3. 算法内容</h1><p>由词向量到句向量，本文采用了一种简单的无监督方法，简单到只有两步：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545033018436.png" alt="1545033018436"></p><p>第一步，对句子中的每个词向量，乘以一个独特的权值。该权值又a和词频p(w)控制，高频词权值会相对下降。</p><p>第二步，计算语料库所有句向量构成的矩阵的第一个主成分u，让每个句向量减去它在u上的投影（类似PCA）。</p><h1 id="4-实验"><a href="#4-实验" class="headerlink" title="4. 实验"></a>4. 实验</h1><h2 id="文本相似度任务"><a href="#文本相似度任务" class="headerlink" title="文本相似度任务"></a>文本相似度任务</h2><p><strong>数据集：</strong></p><p>22个上文本相似性数据集，包括来自SemEval语义文本相似度（STS）任务的所有数据集（2012-1015）(Agirre et al., 2012; 2013; 2014; Agir- rea et al., 2015)和the SemEval 2015 Twitter task (Xu et al., 2015) and the SemEval 2014 Seman- tic Relatedness task (Marelli et al., 2014).</p><p>任务目标：预测给定两个句子的相似度</p><p>评价标准：Pearson相关系数</p><p><strong>比较算法：</strong></p><ul><li>非监督：ST(Kiros et al., 2015), avg-GloVe(Pennington et al., 2014), tfidf-GloV</li><li>半监督：avg-PSL</li><li>监督：PP, PP-proj., DAN, RNN, iRNN, LSTM (o.g.), LSTM (no)</li></ul><p><strong>结果：</strong></p><ul><li>对比算法结果</li></ul><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545030207766.png" alt="1545030207766"></p><p>非监督中，GloVe+WR比avg-GloVe好10-30%，击败了baseline，比LSTM和RNN效果好，能与DAN一较高下。证明了GloVe+WR这个简单模型能比复杂精调监督模型表现要好。</p><p>半监督中，PSL+WR在四个任务中表现最好，另两个任务中也有较强竞争力，比avg-PSL baseline以及所有使用PSL初始化向量的监督模型表现要好，证明了本文算法的优越性。</p><ul><li>加权参数对性能的影响</li></ul><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545030609779.png" alt="1545030609779"></p><p>(a)表明加权参数a的调整比非加权平均有显著的提升。</p><p>(b)表明性能在四个数据集上表现差不多。</p><p>说明本文算法能够应用于不同类型的，在不同语料库中训练的词向量。跨领域性强。</p><h2 id="监督任务"><a href="#监督任务" class="headerlink" title="监督任务"></a>监督任务</h2><p><strong>数据集：</strong></p><p>the SICK similarity task</p><p>the SICK entailment task</p><p>the Stanford Sentiment Treebank (SST) binary classification task (Socher et al., 2013)</p><p><strong>对比算法：</strong>PP, DAN, RNN, and LSTM</p><p><strong>结果：</strong></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545031328044.png" alt="1545031328044"></p><p>本文算法在两项任务中表现最好。但是在第三个任务上表现没有RNN和LSTM好，原因：</p><ul><li>词向量，或者说意义分布假说，由于反义词问题，在捕捉语义方面有局限性。解决方法：(Maas et al., 2011)为语义分析学习更好的词嵌入</li><li>“not”等单词在语义分析中可能是很重要的。解决方法：为这个特殊的任务设计权重方案</li></ul><p><strong>句子中单词顺序带来的影响：</strong></p><p>本文方法的一个特别之处在于忽略句子顺序，与RNN和LSTM利用句子顺序信息不同。</p><p>本文方法效果这么好，那么是不是句子顺序在这些benchmark中不重要呢？</p><p>训练测试RNN/LSTM，使用顺序被打乱的句子。结果：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545031347843.png" alt="1545031347843"></p><p>结果下降很明显，所以本文方法在探测语义上比RNN和LSTM要好得多。</p><p>未来一个有趣的方向是结合这些方法的优势。</p><h1 id="5-作者工作评价"><a href="#5-作者工作评价" class="headerlink" title="5. 作者工作评价"></a>5. 作者工作评价</h1><p>该工作提供了一种简单的句子嵌入方法，基于随机游走模型中的话语向量生成文本(Arora et al., 2016)。该方法简单、无监督，但在各种文本相似度任务上性能明显优于基线，甚至优于某些复杂的监督方法，如RNN和LSTM模型。所获得的句子嵌入可以作为下游监督任务的特征，也可以得到比复杂方法更好或更具有可比性的结果。</p><h1 id="6-自己对该论文的评价"><a href="#6-自己对该论文的评价" class="headerlink" title="6. 自己对该论文的评价"></a>6. 自己对该论文的评价</h1><p>本文算法简单，并且高效，可以在大数据集上使用。并且算法采用无监督方式，不需要标注数据，对于不同领域具有很好的迁移性。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-阅读文献来源&quot;&gt;&lt;a href=&quot;#1-阅读文献来源&quot; class=&quot;headerlink&quot; title=&quot;1. 阅读文献来源&quot;&gt;&lt;/a&gt;1. 阅读文献来源&lt;/h1&gt;&lt;p&gt;Arora S, Liang Y, Ma T. A simple but tough-t
      
    
    </summary>
    
      <category term="nlp" scheme="http://yoursite.com/categories/nlp/"/>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="论文阅读" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
      <category term="句子嵌入" scheme="http://yoursite.com/tags/%E5%8F%A5%E5%AD%90%E5%B5%8C%E5%85%A5/"/>
    
  </entry>
  
  <entry>
    <title>cs224n-TensorFlow实战入门笔记</title>
    <link href="http://yoursite.com/2018/12/17/2018-12/cs224n-TensorFlow%E5%AE%9E%E6%88%98%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2018/12/17/2018-12/cs224n-TensorFlow实战入门笔记/</id>
    <published>2018-12-17T01:16:22.000Z</published>
    <updated>2018-12-18T08:49:22.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Deep-Learning-Frameworks"><a href="#Deep-Learning-Frameworks" class="headerlink" title="Deep Learning Frameworks"></a>Deep Learning Frameworks</h1><ul><li><p>扩展机器学习代码</p></li><li><p>自动计算梯度!</p></li><li><p>标准化用于共享的机器学习应用程序</p></li><li><p>不同的深度学习框架提供了不同的优势，范例，抽象层次，编程语言等</p></li><li><p>gpu并行处理的接口，代码加速</p></li></ul><h1 id="tensorflow"><a href="#tensorflow" class="headerlink" title="tensorflow"></a>tensorflow</h1><h2 id="主要思想"><a href="#主要思想" class="headerlink" title="主要思想"></a>主要思想</h2><p>将数值计算表示图来进行：始终记得任何TensorFlow程序的主干都将是一个图</p><p>图的节点是操作，有多个输入，一个输出，节点之间的边表示在它们之间流动的张量（n维数组）</p><p>流式图作为深度学习框架主干的优点：用小而简单的模型构建复杂的模型</p><h2 id="图"><a href="#图" class="headerlink" title="图"></a>图</h2><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545010562290.png" alt="1545010562290"></p><p><strong>节点类型：</strong></p><ul><li>变量：b,W</li></ul><p>变量是有状态节点，即保存多次执行过程中的当前值（允许不同的人进行保存，存储，发送给其他人）</p><p>重要的优点：b，W也是操作（根据图中节点是操作的定义）</p><ul><li>占位符：X</li></ul><p>在执行期间才会接受值的节点，仅仅分配一个数据类型和张量的大小</p><ul><li>数学操作节点：MatMul，Add，ReLU</li></ul><p><strong>代码：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">b = tf.Variable(tf.zeros((100,)))</span><br><span class="line">W = tf.Variable(tf.random_uniform((784, 100), -1, 1)</span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32, (100, 784))</span><br><span class="line"></span><br><span class="line">h=tf.nn.relu(tf.matmul(x, W) +b)</span><br></pre></td></tr></table></figure><p>b初始化为大小为0的100维向量</p><p>W是一个服从[-1,1]均匀分布，大小为784*100的变量</p><p>x没有被初始化为任何值，仅仅接受32位浮点数，大小为100*784的张量</p><p>h表示x和W进行矩阵乘法加上偏置b，然后进行ReLU运行的结果</p><p><strong>上面这里我们已经定义了一个图，接下来我们要将这个图部署到会话（session）上</strong></p><h2 id="会话"><a href="#会话" class="headerlink" title="会话"></a>会话</h2><p> <img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545011476849.png" alt="1545011476849"></p><p>会话：与特定执行上下文（如CPU或GPU）的绑定</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.run(fetches, feeds)</span><br></pre></td></tr></table></figure><p>Fetches：返回输出的图形节点列表，这些是我们感兴趣的实际计算值的节点</p><p>feeds：从图节点到我们想要在模型中运行的实际值的字典映射，即实际填入占位符内容的地方</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">b = tf.Variable(tf.zeros ( (<span class="number">100</span>,)))</span><br><span class="line">w= tf.Variable(tf.random-uniform((<span class="number">784</span>, <span class="number">100</span>),<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">x= tf.placeholder(tf.float32, (<span class="number">100</span>, <span class="number">784</span>))</span><br><span class="line">h= tf.nn.relu(tf.matmul(x, w) +b)</span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.initialize_all_variables())</span><br><span class="line">sess.run(h, &#123;x: np.random.random(<span class="number">100</span>, <span class="number">784</span>)&#125;)</span><br></pre></td></tr></table></figure><h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p><strong>定义loss node：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">prediction = tf.nn.softmax(...) #Output of neural network </span><br><span class="line">label = tf.placeholder(tf.float32, [100, 10])</span><br><span class="line">cross_entropy =-tf.reduce_sum(label * tf.log(prediction), axis=1)</span><br></pre></td></tr></table></figure><p>使用label（占位符）和prediction来建立交叉熵节点</p><p><strong>计算梯度：</strong></p><p>首先创建一个优化器对象：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.train.GradientDescentOptimizer</span><br></pre></td></tr></table></figure><p>优化器是一个抽象类，有许多实现，这里实现的是梯度下降优化器。</p><p>在此基础上增加优化操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.train.GradientDecentOptimizer(lr).minimize(cross_entropy)</span><br></pre></td></tr></table></figure><p>梯度如何计算呢？</p><p>TensorFlow中每个图节点都有一个附加的梯度操作，都有相对于输入预先构建的输出梯度，通过图使用链式法则进行反向传播计算，这是非常简单的，也是将数值计算作为图的主要优势</p><p>这一切都是自动进行的</p><p>代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">prediction = tf.nn.softmax(...)</span><br><span class="line">label = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">cross_entropy = tf.reduce mean(-tf.reduce sum(label * tf.log(prediction), </span><br><span class="line">reduction indices=[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(cross_entropy).</span><br></pre></td></tr></table></figure><p>创建变量train_step，接收梯度下降优化器，设置学习率为0.5，并最小化交叉熵</p><p><strong>在session上运行：</strong></p><p>创建session-&gt;建立训练计划-&gt;运行train_step</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.initialize_all_variables())</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">batchx, batch_label = data.next_batch()</span><br><span class="line">sess.run(train_step, feed dict=&#123;x: batch-x,label: batch-label&#125;)</span><br></pre></td></tr></table></figure><h1 id="API笔记"><a href="#API笔记" class="headerlink" title="API笔记"></a>API笔记</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Deep-Learning-Frameworks&quot;&gt;&lt;a href=&quot;#Deep-Learning-Frameworks&quot; class=&quot;headerlink&quot; title=&quot;Deep Learning Frameworks&quot;&gt;&lt;/a&gt;Deep Learning 
      
    
    </summary>
    
      <category term="cs224n" scheme="http://yoursite.com/categories/cs224n/"/>
    
    
      <category term="cs224n" scheme="http://yoursite.com/tags/cs224n/"/>
    
      <category term="TensorFlow" scheme="http://yoursite.com/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读：Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation</title>
    <link href="http://yoursite.com/2018/12/11/2018-12/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AJoint%20Learning%20of%20the%20Embedding%20of%20Words%20and%20Entities%20for%20Named%20Entity%20Disambiguation/"/>
    <id>http://yoursite.com/2018/12/11/2018-12/论文阅读：Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation/</id>
    <published>2018-12-11T13:03:28.000Z</published>
    <updated>2018-12-11T13:20:15.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-阅读文献来源"><a href="#1-阅读文献来源" class="headerlink" title="1. 阅读文献来源"></a>1. 阅读文献来源</h1><p>Yamada I, Shindo H, Takeda H, et al. Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation[J]. 2016:250-259.</p><h1 id="2-论文解决的科学问题及研究动机"><a href="#2-论文解决的科学问题及研究动机" class="headerlink" title="2. 论文解决的科学问题及研究动机"></a>2. 论文解决的科学问题及研究动机</h1><p>在NED（Named Entity Disambiguation）中的主要困难是实体指称词意义的模糊性。</p><p>早期研究集中于建模上下文语境，比如上下文与候选实体的百科页面的相似度(Bunescu and Pasca, 2006;Mihalcea and Csomai, 2007)。很多先进的方法通过使用更复杂的全局方法，所有指称词基于全局相关度进行同时消歧。</p><p>词向量逐渐流行(Mikolov et al., 2013a;Mikolov et al., 2013b; Pennington et al., 2014)。这种方法涉及到从大量非结构文本中学习单词的连续向量表达。当相似单词被放在低维向量空间中相邻的位置上时，这些向量可以捕捉单词之间的语义相似度.</p><h1 id="3-所提算法创新的工作"><a href="#3-所提算法创新的工作" class="headerlink" title="3. 所提算法创新的工作"></a>3. 所提算法创新的工作</h1><p><strong>传统skip-gram模型：</strong></p><p>Skip-gram模型的训练目标是给定目标单词的情况下找到有助于预测上下文词汇的单词表达。</p><p>形式化：给定T个单词的序列：<img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image002.gif" alt="img">。这个模型旨在最大化下面的目标函数：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image004.jpg" alt="img"></p><p>其中c是上下文窗口的大小，</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image006-1544533695373.gif" alt="img"></p><p>代表目标实体</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image008.gif" alt="img"></p><p>代表其上下文单词。</p><p>条件概率</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image010.gif" alt="img"></p><p>通过下面的softmax函数计算：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image012.jpg" alt="img"></p><p>其中W是词汇表中所有单词的集合，</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image014.gif" alt="img"></p><p>和</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image016.gif" alt="img"></p><p>代表单词</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image018.gif" alt="img"></p><p>在矩阵V和U中向量。<br>通过调整V来使得目标函数最大化，即给定目标实体上下文出现概率最大。这样本文就得到了一个代表T个单词的向量表达V。</p><p><strong>对skip-gram模型进行扩展：</strong></p><p>本文扩展矩阵V和U除了单词向量之外还包括实体向量，得到</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image020.gif" alt="img"></p><p>和</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image022.gif" alt="img"></p><p><strong>知识库图模型KB Graph Model：</strong></p><p>本文在KB中使用一个内部连接结构，使得模型学习到实体对之间的相关度。Wikipedia Link-based Measure（WLM）是一个基于连接结构测量实体相关度的方法。计算两个实体相关度的方法如下：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image024.jpg" alt="img"></p><p>其中E是KB中所有实体的集合，Ce是与实体e有连接的实体集合。直观上，WLM认为具有相似连接的实体是相关的。虽然WLM简单，但是比很多算法要好。</p><p>启发自WLM，知识库图模型将具有相似连接的实体放在向量空间中相近的位置，通过下面的目标函数来形式化这个过程：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image026.jpg" alt="img"></p><p>本文使用softmax函数来计算上述公式中的条件概率：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image028.jpg" alt="img"></p><p>对于给定的实体e，本文训练模型来预测连接Ce。因此，Ce在skip-gram模型中和上下文单词起到相似的作用。</p><p><strong>Anchor Context Model（锚文本模型）：</strong></p><p>如果本文仅考虑在skip-gram模型中加入KB图模型，实体和单词的向量不会相互作用，这些向量会被放在向量空间中不同的子空间中。为了解决这个问题，本文提出Anchor Context Model将相似的实体和单词放在向量空间中相近的位置。</p><p>思想：利用KB锚和上下文来训练模型。Wikipedia中有很多内部锚，利用这些，本文能从KB中直接获取到实体及其上下文单词的出现。</p><p>本文训练模型通过目标锚来预测一个实体指向的上下文单词。目标函数如下：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image030.jpg" alt="img"></p><p>其中A代表KB中的锚集合，其中每一个包含一对实体指称<br><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image032.gif" alt="img"><br>和它的上下文单词Q的集合。Q包含前c个单词和后c个单词。|A|表示KB中内部锚的数量。</p><p>条件概率通过softmax函数计算：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image034.jpg" alt="img"></p><p>使用提出的模型，本文通过将单词和具有相似上下文单词的实体放在向量空间中相邻的地方，来分配单词和实体的向量表达。</p><p><strong>结合</strong></p><p>通过3中提出的三种模型及它们的目标函数，本文将它们线性结合在一起，得到最终的目标函数：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image036.jpg" alt="img"></p><p>模型的训练意向最大化上述的函数，结果得到矩阵V被用于向量化单词和实体。</p><p>算法的一个问题是，在训练模型时，包含在softmax函数中的正则化项的计算是非常昂贵的，因为涉及到所有单词W或实体E的总和。为了解决这个问题，本文使用负采样（NEG）来将原始的目标函数转化为计算可行的目标函数。</p><p>NEG通过下面的目标函数定义：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image038.jpg" alt="img"></p><p>其中<br><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image040.jpg" alt="img"><br>是一个sigmoid函数，g是负样本的数量。本文替换公式（1）中的条件概率通过上面这个目标函数。因此，公式（1）的目标函数转化成了一个简单的二分类目标函数，来对从噪声分布Pneg(w)中提取的words（上文中翻译成单词，这里还是用words防止错误解读）进行辨别。同样的本文也可以对公式（4）和公式（6）进行同样的替换。</p><p>NEG将噪声分布Pneg(w)作为一个自由参数。</p><p>本文使用Wikipedia来训练所有的模型。通过在Wikipedia页面上迭代几次来进行同时优化得到最大的目标函数。本文使用随机梯度下降（SGD）进行优化。</p><h1 id="4-算法框架"><a href="#4-算法框架" class="headerlink" title="4. 算法框架"></a>4. 算法框架</h1><p><strong>使用词向量进行NED：</strong></p><p>任务：给定文档d中的实体指称词集合</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image042.gif" alt="img"></p><p>KB中的实体集合</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image044.gif" alt="img"></p><p>将指称词指向对应的实体。<br>本文将NED任务分成两个部分：候选集生成和指称消歧。</p><p><strong>指称消歧</strong></p><p>给定一个文档d和指称词m以及它的候选实体集合{ e1, e2,…, ek}，任务是从集合中选择最相关实体对m进行消歧。</p><p>提升效果的关键是有效建模上下文，本文提出两个模型，使用提出的词向量模型对上下文进行建模。本文使用监督学习将两个模型和标准NED特征结合起来。</p><p><strong>1）建模文本上下文：</strong></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image046.jpg" alt="img"></p><p>其中</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image048.gif" alt="img"></p><p>是指称词m的上下文集合，</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image050.gif" alt="img"></p><p>代表单词w的词向量。本文使用文档d中的所有名词作为上下文（&lt;<a href="https://opennlp.apache.org/）。" target="_blank" rel="noopener">https://opennlp.apache.org/）。</a><br>然后本文计算候选实体和文本上下文的相似度，通过</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image052.gif" alt="img"></p><p>和</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image054.gif" alt="img"></p><p>余弦相似度。</p><p><strong>2）建模相关度</strong></p><p>一个简单的两步方法：第一步，在非歧指称词上使用相关度得分训练机器学习模型；第二步，在预测到的实体分配上使用相关度得分重新训练模型。</p><p>评价相关度：先计算上下文实体（非歧义的实体）的词向量，计算上下文实体向量和目标实体e向量的相似度。为了获得上下文实体的词向量，本文平均他们的词向量：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image056.jpg" alt="img"></p><p>其中<img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image058.gif" alt="img">表示上下文实体集合。<br>为了评价相关度得分，本文重新使用上下文实体<img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image060.gif" alt="img">和实体<img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image054.gif" alt="img">之间的余弦相似度。</p><p><strong>3）学会排序</strong></p><p>本文采用监督机器学习方法来排序给定指称词m和文档d之后的候选实体集，目的是为了将标准NED特征和上下文信息结合起来。</p><p>梯度提升回归树（GBRT）（<a href="http://scikit-learn.org/stable/）：包含一组回归树，给定实例预测相关度得分。代价函数为logistic" target="_blank" rel="noopener">http://scikit-learn.org/stable/）：包含一组回归树，给定实例预测相关度得分。代价函数为logistic</a> loss。主要参数有迭代次数，学习速率和决策树的最大深度。</p><h1 id="5-实验"><a href="#5-实验" class="headerlink" title="5. 实验"></a>5. 实验</h1><p><strong>训练词向量的数据：</strong><a href="https://dumps.wikimedia.org/" target="_blank" rel="noopener">https://dumps.wikimedia.org/</a></p><p><strong>1）测试实体词向量的质量：</strong></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image062.jpg" alt="img"></p><p><strong>2）命名实体消歧</strong></p><p><strong>数据集：CoNLL（Hoffart et al. 2011）、TAC 2010（Ji et al. 2010）。</strong></p><p><strong>对比算法：Hoffart et al.、He et al.、Chisholm and Hachey、Pershina et al.</strong></p><p><strong>本文算法的结果：</strong></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image064.jpg" alt="img"></p><p><strong>表3是对比实验结果：</strong></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image066.jpg" alt="img"></p><p><strong>表4是本文算法不同特征学习的结果：</strong></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image068.jpg" alt="img"></p><p><strong>在加入基于本文的词向量模型的上下文特征时，效果大大的提升了。</strong></p><p><strong>错误分析：观察到在CoNLL测试集上，大约48.6%的错误率是由于metonymy mentions造成的。当一个流行的错误实体，匹配到指称词，本文的NED算法经常出错，这是由于本文的算法利用KB中的实体流行性数据。</strong></p><h1 id="6-工作评价"><a href="#6-工作评价" class="headerlink" title="6. 工作评价"></a>6. 工作评价</h1><p>本文提出了一种词嵌入方法，将文字和实体联合映射到相同的连续向量空间中。本文的方法使本文能够有效地对文本和全局的文本进行建模。此外，在这些背景下，本文的NED方法优于最先进的NED方法。</p><p><strong>在未来的工作中，本文打算通过利用关系知识来改进本文的模型，例如在知识图中（例如Freebase的关系。本文也想使本文的嵌入应用到其他的应用领域。</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-阅读文献来源&quot;&gt;&lt;a href=&quot;#1-阅读文献来源&quot; class=&quot;headerlink&quot; title=&quot;1. 阅读文献来源&quot;&gt;&lt;/a&gt;1. 阅读文献来源&lt;/h1&gt;&lt;p&gt;Yamada I, Shindo H, Takeda H, et al. Joint L
      
    
    </summary>
    
      <category term="nlp" scheme="http://yoursite.com/categories/nlp/"/>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="论文阅读" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
      <category term="知识图谱" scheme="http://yoursite.com/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
      <category term="实体链接" scheme="http://yoursite.com/tags/%E5%AE%9E%E4%BD%93%E9%93%BE%E6%8E%A5/"/>
    
  </entry>
  
</feed>

<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>hellojet</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-12-19T14:23:28.031Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>李洁厅</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>论文阅读：Linear Algebraic Structure of Word Senses, with Applications to Polysemy</title>
    <link href="http://yoursite.com/2018/12/19/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ALinear%20Algebraic%20Structure%20of%20Word%20Senses,%20with%20Applications%20to%20Polysemy/"/>
    <id>http://yoursite.com/2018/12/19/论文阅读：Linear Algebraic Structure of Word Senses, with Applications to Polysemy/</id>
    <published>2018-12-19T14:23:00.000Z</published>
    <updated>2018-12-19T14:23:28.031Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-阅读文献来源"><a href="#1-阅读文献来源" class="headerlink" title="1. 阅读文献来源"></a>1. 阅读文献来源</h1><p>Arora S, Li Y, Liang Y, et al. Linear algebraic structure of word senses, with applications to polysemy[J]. Transactions of the Association of Computational Linguistics, 2018, 6: 483-495.</p><p>Sanjeev Arora（大牛）：Professor of Computer Science，Princeton University，theoretical machine learningtheoretical computer science</p><p>Yingyu Liang： University of Wisconsin-Madison，machine learning</p><h1 id="2-论文解决的科学问题及研究动机"><a href="#2-论文解决的科学问题及研究动机" class="headerlink" title="2. 论文解决的科学问题及研究动机"></a>2. 论文解决的科学问题及研究动机</h1><p>Firth’s hypothesis：一个单词的意思可以通过周围词的分布得到（Firth，1957）。所以现在的词嵌入通过一个实值向量表示一个单词的意思。</p><p>比较经典的模型有(Turney et al., 2010)：词的共现矩阵统计、神经网络模型（word2vec)，使用非凸优化(Mikolov et al., 2013a;b)。</p><p><strong>而问题在于</strong>当一词多义的情况出现，一个词嵌入所表示的意义是不明确的。</p><p><strong>现行的解决思路：</strong></p><ul><li>通过内积来提取隐藏信息，效果不好</li><li>线性叠加，通过简单稀疏编码恢复</li></ul><p><strong>现行的解决方案（Word Sense Induction）：</strong></p><ul><li>通过Yarowsky (1995)中的几种范例方法来自动学习语义</li><li>通过聚类邻居词来识别多义</li><li>通过将嵌入扩张为更复杂的表示，而不是单个向量，来捕捉单词的含义(Mruphy et al., 2012; Huang et al., 2012)，但是在缺少WordNets的语言上没办法使用</li></ul><p>本文旨在提出一种从词嵌入中提取出不同意义，用来解决一词多义问题的方法。</p><p>通过本文算法可以进一步解决WSD（word sense disambiguation 语义消歧）问题，为其创造标注语料库或者WordNet。</p><h1 id="3-所提算法创新的工作"><a href="#3-所提算法创新的工作" class="headerlink" title="3. 所提算法创新的工作"></a>3. 所提算法创新的工作</h1><ul><li><p>本文使用语料库仅仅是为了生成词向量，而基于图的聚类方法在语料库上进行work，速度非常慢。</p></li><li><p>本文算法恢复得到的不同语义是通过话语原语相互联系的。</p></li><li><p>本文算法完全非监督，不需要大量标注集</p></li><li><p>使用了稀疏编码，曾经没有出现在解决一词多义词嵌入问题中</p></li><li><strong>通过话语原语（discourse atom）提取语义：</strong></li></ul><p>先说语义的线性结构，“tie”具有很多意思，其词嵌入应该约等于它的意思的加权和。（这个在作者16年工作中句子向量也提到过）。当然作者是做了一些实验发现这个的：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545224903660.png" alt="1545224903660"></p><p>这个实验通过合并两个单词创建一个多义词：删除这两个词并替换成一个词，进而计算该新词的词嵌入，计算方式和之前计算两个词的方法相同。改变两个词的频率，做大量实验，结果发现新词的向量总是接近两个词的张量子空间。</p><p>那么如何从词嵌入中提取不同的语义信息呢？</p><p>上述实验表明：<br>$$<br>v_{tie} = \alpha_1v_{tie1}+\alpha_2v_{tie2}+\alpha_3v_{tie3}+…<br>$$<br>但是这种组合是无穷的，我们还需要其他的信息帮助我们提取语义。</p><p><strong>借助不同词的语义关联</strong></p><p>比如，“tie”的“article of clothing”与shoe，jacket等相连。</p><p>为此，需要用到作者16年工作Arora et al. (2016)中的<strong>话语模型随机游走</strong>。</p><p>这个模型建立在(Mnih and Hinton, 2007)的逻辑线性主题模型上，原模型定义了主题c与词w之间的一个分布，而随机游走模型将c视为一个话语（在语料库的这个地方正在谈论什么），通过允许c游走来构建语料库：当走到c时，根据原模型的分布来生成一些词，这些词是与c在余弦上相近的。</p><p>有了以上的基础之后，tie1和tie2等对应于tie周围不同词的分布，也可以叫做不同的话语（话语就是一种词的分布）。比如，遇到“clothing”话语，高概率产生tie1（article of clothing），shoe，jacket等词。</p><p>进一步，<strong>“clothing”话语与shoe，jacket，tie1等有很高的内积。</strong>（这是根据原模型w关于c的分布的公式得到的）</p><p>接下来，就引入一个全局优化，目标是找出所有的话语原语：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545226941682.png" alt="1545226941682"></p><p>系数α非零（叫做硬稀疏约束），加号后面一项是噪声向量。</p><p>A和α都是未知的，所以该问题是非凸的，涉及到稀疏编码，本文使用标准k-SVD算法来解决做个问题。</p><p>这里的A就是话语原语。</p><p><strong>通俗解释：训练可以得到话语原语，通过话语原语可以重现一个单词向量。这些话语原语就是我们讲的一词多义中的多义。而如何解释某个原语意义就看与这个原语关联最近的一些词。</strong></p><h1 id="4-实验"><a href="#4-实验" class="headerlink" title="4. 实验"></a>4. 实验</h1><h2 id="话语原语实验"><a href="#话语原语实验" class="headerlink" title="话语原语实验"></a>话语原语实验</h2><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545210119277.png" alt="1545210119277"></p><p>上图包含了一些话语原语以及他们的近邻词，出现在一个话语中的单词通常彼此相近</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545210249432.png" alt="1545210249432"></p><p>上图是连接到“chair”和“bank”的五个话语原语，每个原语通过六个最近的词来表示。比如上面表中是连接到“chair”的五个原语1187,739,590,1322,1457.</p><p><strong>话语原语的层级结构：</strong></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545210621661.png" alt="1545210621661"></p><p>上图是科学领域使用原语的一个例子，biology和chemistry的线性组合与跨学科biochemistry相近。</p><h2 id="定量测试"><a href="#定量测试" class="headerlink" title="定量测试"></a>定量测试</h2><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545227648755.png" alt="1545227648755"></p><p>m代表正确的含义数和干扰项，所有的算法从中选择k个正确的含义。</p><p>m=20，k=4时，本文算法precision=0.63，recall=0.70。</p><p>在m20时，本文算法与非本语说话者表现相同。</p><h1 id="5-作者工作评价"><a href="#5-作者工作评价" class="headerlink" title="5. 作者工作评价"></a>5. 作者工作评价</h1><p>从(Arora et al . 2016年)获得的词嵌入，还有其他几个方法，已被证明包含这个词的不同意义，通过简单的稀疏编码可提取出。目前似乎名词做得比其他词性更好。本文方法的一个新颖之处在于，词义与2000个话语向量相关联。这使得该方法在NLP其他领域的任务，与WordNets一样有用。这种方法在半自动化模式下更有用，更准确，因为人工更能够识别词义。</p><p>对数可能是本文算法成功的关键，因为对数使得很少出现的词义有一个合理的权重。</p><h1 id="6-自己对该论文的评价"><a href="#6-自己对该论文的评价" class="headerlink" title="6. 自己对该论文的评价"></a>6. 自己对该论文的评价</h1><p>本文通过提出话语原语（discourse atom）的概念来表示一个词向量中包含的各种语义，在数学和实践上都证明了各种方法生成的词向量都近似于该词各种含义的向量线性组合这一点。这对nlp领域的发展具有重大的意义。</p><p>在实体链接任务中可以通过判断一个词的语义来更准确的链接到知识库中，可行性很高，具体来说：</p><p>通过语料库训练每个词的词向量以及话语原语向量，以及词向量和话语原语之间的余弦相似度（判断其含义是否近似），拿到一个mention之后，先判断该词与哪个话语原语有较高的相似度，可以作为一种辅助决策，也可以进一步根据上下文来判断该mention对应的实体。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-阅读文献来源&quot;&gt;&lt;a href=&quot;#1-阅读文献来源&quot; class=&quot;headerlink&quot; title=&quot;1. 阅读文献来源&quot;&gt;&lt;/a&gt;1. 阅读文献来源&lt;/h1&gt;&lt;p&gt;Arora S, Li Y, Liang Y, et al. Linear algebr
      
    
    </summary>
    
      <category term="nlp" scheme="http://yoursite.com/categories/nlp/"/>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="论文阅读" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
      <category term="词嵌入" scheme="http://yoursite.com/tags/%E8%AF%8D%E5%B5%8C%E5%85%A5/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读：A Simple but Tough-to-beat Baseline for Sentence Embeddings</title>
    <link href="http://yoursite.com/2018/12/17/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AA%20Simple%20but%20Tough-to-beat%20Baseline%20for%20Sentence%20Embeddings/"/>
    <id>http://yoursite.com/2018/12/17/论文阅读：A Simple but Tough-to-beat Baseline for Sentence Embeddings/</id>
    <published>2018-12-17T08:05:00.000Z</published>
    <updated>2018-12-17T08:06:17.223Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-阅读文献来源"><a href="#1-阅读文献来源" class="headerlink" title="1. 阅读文献来源"></a>1. 阅读文献来源</h1><p>Arora S, Liang Y, Ma T. A simple but tough-to-beat baseline for sentence embeddings[J]. 2016.</p><p>Sanjeev Arora：普林斯顿大学教授，研究领域是理论机器学习和理论计算机科学</p><p>Yingyu Liang：威斯康星大学</p><p><a href="https://github.com/PrincetonML/SIF" target="_blank" rel="noopener">源代码</a></p><h1 id="2-论文研究动机"><a href="#2-论文研究动机" class="headerlink" title="2. 论文研究动机"></a>2. 论文研究动机</h1><p>神经网络在计算词嵌入方面的成功也推动了研究者们探索生成句子或段落的语义嵌入的方法。</p><p>词嵌入作为NLP和IR中的基础构建块，捕捉词之间的相似度(Bengio et al., 2003; Collobert &amp; Weston, 2008; Mikolov et al., 2013a; Pennington et al., 2014)。</p><p>最近的工作尝试通过计算词嵌入来捕捉词序列（如短语，句子，段落等）的语义，这些方法从简单的词向量叠加到复杂的CNN，RNN(Iyyer et al., 2015; Le &amp; Mikolov, 2014; Kiros et al., 2015; Socher et al., 2011; Blunsom et al., 2014; Tai et al., 2015; Wang et al., 2016)</p><p>(Wieting et al., 2016)从标准词嵌入入手，在数据集(PPDB)的监督下对标准词嵌入进行修改，通过训练一个简单的词平均模型构建句子嵌入。然而，来自数据集的监督似乎至关重要，因为他们报告说，初始词嵌入的简单平均值并不能很好地工作。</p><p>本文在(Wieting et al., 2016)基础上进一步提出自己的工作。</p><h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p><strong>词嵌入</strong></p><p>词嵌入将词表示为低维空间中的连续向量，捕捉词的词汇和语义特性。它们可以从文本的神经网络模型的内部表示得到(Bengio et al., 2003; Collobert &amp; Weston, 2008; Mikolov et al., 2013a)或通过共现统计学的低秩近似(Deerwester et al., 1990; Pennington et al., 2014)。这两种方法是密切相关的(Levy &amp; Goldberg, 2014; Hashimoto et al., 2016; Arora et al., 2016)。</p><p><strong>短语/句子/段落嵌入</strong></p><p>以往的研究都是通过对向量和矩阵的运算来组合词嵌入来计算短语或句子嵌入(Mitchell &amp; Lapata, 2008; 2010; Blacoe &amp; Lapata, 2012)。其中coordinate-wise multiplication表现得很好。非加权平均对短语的表示方面做得很好(Mikolov et al., 2013a)。</p><p>另一种方法是在解析树上定义的递归神经网络(RNNs)，监督学习(Socher et al., 2011) 或者非监督 (Socher et al., 2014)。简单的RNNs可以看作是解析树被简单的线性链替换的特殊情况。</p><p>(Mikolov et al., 2013b)：Skip-gram模型被扩展为合并序列的潜在向量，或将序列视为一个基础单元。</p><p>(Le &amp; Mikolov, 2014) ：每个段落假设有一个潜在段落向量，影响段落中每个词的分布。</p><p>(Kiros et al., 2015)：Skip-thought重构某个句子周围的句子，将隐藏参数视为向量表示。</p><p>(Tai et al., 2015)：RNNs使用LSTM捕捉长距离依赖，用于建模句子。</p><p>(Blunsom et al., 2014)：CNN，使用动态pooling来处理变长句子，在语义预测和分类任务中表现良好。</p><h1 id="3-算法内容"><a href="#3-算法内容" class="headerlink" title="3. 算法内容"></a>3. 算法内容</h1><p>由词向量到句向量，本文采用了一种简单的无监督方法，简单到只有两步：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545033018436.png" alt="1545033018436"></p><p>第一步，对句子中的每个词向量，乘以一个独特的权值。该权值又a和词频p(w)控制，高频词权值会相对下降。</p><p>第二步，计算语料库所有句向量构成的矩阵的第一个主成分u，让每个句向量减去它在u上的投影（类似PCA）。</p><h1 id="4-实验"><a href="#4-实验" class="headerlink" title="4. 实验"></a>4. 实验</h1><h2 id="文本相似度任务"><a href="#文本相似度任务" class="headerlink" title="文本相似度任务"></a>文本相似度任务</h2><p><strong>数据集：</strong></p><p>22个上文本相似性数据集，包括来自SemEval语义文本相似度（STS）任务的所有数据集（2012-1015）(Agirre et al., 2012; 2013; 2014; Agir- rea et al., 2015)和the SemEval 2015 Twitter task (Xu et al., 2015) and the SemEval 2014 Seman- tic Relatedness task (Marelli et al., 2014).</p><p>任务目标：预测给定两个句子的相似度</p><p>评价标准：Pearson相关系数</p><p><strong>比较算法：</strong></p><ul><li>非监督：ST(Kiros et al., 2015), avg-GloVe(Pennington et al., 2014), tfidf-GloV</li><li>半监督：avg-PSL</li><li>监督：PP, PP-proj., DAN, RNN, iRNN, LSTM (o.g.), LSTM (no)</li></ul><p><strong>结果：</strong></p><ul><li>对比算法结果</li></ul><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545030207766.png" alt="1545030207766"></p><p>非监督中，GloVe+WR比avg-GloVe好10-30%，击败了baseline，比LSTM和RNN效果好，能与DAN一较高下。证明了GloVe+WR这个简单模型能比复杂精调监督模型表现要好。</p><p>半监督中，PSL+WR在四个任务中表现最好，另两个任务中也有较强竞争力，比avg-PSL baseline以及所有使用PSL初始化向量的监督模型表现要好，证明了本文算法的优越性。</p><ul><li>加权参数对性能的影响</li></ul><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545030609779.png" alt="1545030609779"></p><p>(a)表明加权参数a的调整比非加权平均有显著的提升。</p><p>(b)表明性能在四个数据集上表现差不多。</p><p>说明本文算法能够应用于不同类型的，在不同语料库中训练的词向量。跨领域性强。</p><h2 id="监督任务"><a href="#监督任务" class="headerlink" title="监督任务"></a>监督任务</h2><p><strong>数据集：</strong></p><p>the SICK similarity task</p><p>the SICK entailment task</p><p>the Stanford Sentiment Treebank (SST) binary classification task (Socher et al., 2013)</p><p><strong>对比算法：</strong>PP, DAN, RNN, and LSTM</p><p><strong>结果：</strong></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545031328044.png" alt="1545031328044"></p><p>本文算法在两项任务中表现最好。但是在第三个任务上表现没有RNN和LSTM好，原因：</p><ul><li>词向量，或者说意义分布假说，由于反义词问题，在捕捉语义方面有局限性。解决方法：(Maas et al., 2011)为语义分析学习更好的词嵌入</li><li>“not”等单词在语义分析中可能是很重要的。解决方法：为这个特殊的任务设计权重方案</li></ul><p><strong>句子中单词顺序带来的影响：</strong></p><p>本文方法的一个特别之处在于忽略句子顺序，与RNN和LSTM利用句子顺序信息不同。</p><p>本文方法效果这么好，那么是不是句子顺序在这些benchmark中不重要呢？</p><p>训练测试RNN/LSTM，使用顺序被打乱的句子。结果：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545031347843.png" alt="1545031347843"></p><p>结果下降很明显，所以本文方法在探测语义上比RNN和LSTM要好得多。</p><p>未来一个有趣的方向是结合这些方法的优势。</p><h1 id="5-作者工作评价"><a href="#5-作者工作评价" class="headerlink" title="5. 作者工作评价"></a>5. 作者工作评价</h1><p>该工作提供了一种简单的句子嵌入方法，基于随机游走模型中的话语向量生成文本(Arora et al., 2016)。该方法简单、无监督，但在各种文本相似度任务上性能明显优于基线，甚至优于某些复杂的监督方法，如RNN和LSTM模型。所获得的句子嵌入可以作为下游监督任务的特征，也可以得到比复杂方法更好或更具有可比性的结果。</p><h1 id="6-自己对该论文的评价"><a href="#6-自己对该论文的评价" class="headerlink" title="6. 自己对该论文的评价"></a>6. 自己对该论文的评价</h1><p>本文算法简单，并且高效，可以在大数据集上使用。并且算法采用无监督方式，不需要标注数据，对于不同领域具有很好的迁移性。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-阅读文献来源&quot;&gt;&lt;a href=&quot;#1-阅读文献来源&quot; class=&quot;headerlink&quot; title=&quot;1. 阅读文献来源&quot;&gt;&lt;/a&gt;1. 阅读文献来源&lt;/h1&gt;&lt;p&gt;Arora S, Liang Y, Ma T. A simple but tough-t
      
    
    </summary>
    
      <category term="nlp" scheme="http://yoursite.com/categories/nlp/"/>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="句子嵌入" scheme="http://yoursite.com/tags/%E5%8F%A5%E5%AD%90%E5%B5%8C%E5%85%A5/"/>
    
      <category term="论文阅读" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>cs224n-TensorFlow实战入门笔记</title>
    <link href="http://yoursite.com/2018/12/17/cs224n-TensorFlow%E5%AE%9E%E6%88%98%E5%85%A5%E9%97%A8%E7%AC%94%E8%AE%B0/"/>
    <id>http://yoursite.com/2018/12/17/cs224n-TensorFlow实战入门笔记/</id>
    <published>2018-12-17T01:16:22.000Z</published>
    <updated>2018-12-18T08:49:22.355Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Deep-Learning-Frameworks"><a href="#Deep-Learning-Frameworks" class="headerlink" title="Deep Learning Frameworks"></a>Deep Learning Frameworks</h1><ul><li><p>扩展机器学习代码</p></li><li><p>自动计算梯度!</p></li><li><p>标准化用于共享的机器学习应用程序</p></li><li><p>不同的深度学习框架提供了不同的优势，范例，抽象层次，编程语言等</p></li><li><p>gpu并行处理的接口，代码加速</p></li></ul><h1 id="tensorflow"><a href="#tensorflow" class="headerlink" title="tensorflow"></a>tensorflow</h1><h2 id="主要思想"><a href="#主要思想" class="headerlink" title="主要思想"></a>主要思想</h2><p>将数值计算表示图来进行：始终记得任何TensorFlow程序的主干都将是一个图</p><p>图的节点是操作，有多个输入，一个输出，节点之间的边表示在它们之间流动的张量（n维数组）</p><p>流式图作为深度学习框架主干的优点：用小而简单的模型构建复杂的模型</p><h2 id="图"><a href="#图" class="headerlink" title="图"></a>图</h2><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545010562290.png" alt="1545010562290"></p><p><strong>节点类型：</strong></p><ul><li>变量：b,W</li></ul><p>变量是有状态节点，即保存多次执行过程中的当前值（允许不同的人进行保存，存储，发送给其他人）</p><p>重要的优点：b，W也是操作（根据图中节点是操作的定义）</p><ul><li>占位符：X</li></ul><p>在执行期间才会接受值的节点，仅仅分配一个数据类型和张量的大小</p><ul><li>数学操作节点：MatMul，Add，ReLU</li></ul><p><strong>代码：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">import tensorflow as tf</span><br><span class="line"></span><br><span class="line">b = tf.Variable(tf.zeros((100,)))</span><br><span class="line">W = tf.Variable(tf.random_uniform((784, 100), -1, 1)</span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32, (100, 784))</span><br><span class="line"></span><br><span class="line">h=tf.nn.relu(tf.matmul(x, W) +b)</span><br></pre></td></tr></table></figure><p>b初始化为大小为0的100维向量</p><p>W是一个服从[-1,1]均匀分布，大小为784*100的变量</p><p>x没有被初始化为任何值，仅仅接受32位浮点数，大小为100*784的张量</p><p>h表示x和W进行矩阵乘法加上偏置b，然后进行ReLU运行的结果</p><p><strong>上面这里我们已经定义了一个图，接下来我们要将这个图部署到会话（session）上</strong></p><h2 id="会话"><a href="#会话" class="headerlink" title="会话"></a>会话</h2><p> <img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1545011476849.png" alt="1545011476849"></p><p>会话：与特定执行上下文（如CPU或GPU）的绑定</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sess.run(fetches, feeds)</span><br></pre></td></tr></table></figure><p>Fetches：返回输出的图形节点列表，这些是我们感兴趣的实际计算值的节点</p><p>feeds：从图节点到我们想要在模型中运行的实际值的字典映射，即实际填入占位符内容的地方</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">b = tf.Variable(tf.zeros ( (<span class="number">100</span>,)))</span><br><span class="line">w= tf.Variable(tf.random-uniform((<span class="number">784</span>, <span class="number">100</span>),<span class="number">-1</span>, <span class="number">1</span>))</span><br><span class="line">x= tf.placeholder(tf.float32, (<span class="number">100</span>, <span class="number">784</span>))</span><br><span class="line">h= tf.nn.relu(tf.matmul(x, w) +b)</span><br><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.initialize_all_variables())</span><br><span class="line">sess.run(h, &#123;x: np.random.random(<span class="number">100</span>, <span class="number">784</span>)&#125;)</span><br></pre></td></tr></table></figure><h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><p><strong>定义loss node：</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">prediction = tf.nn.softmax(...) #Output of neural network </span><br><span class="line">label = tf.placeholder(tf.float32, [100, 10])</span><br><span class="line">cross_entropy =-tf.reduce_sum(label * tf.log(prediction), axis=1)</span><br></pre></td></tr></table></figure><p>使用label（占位符）和prediction来建立交叉熵节点</p><p><strong>计算梯度：</strong></p><p>首先创建一个优化器对象：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.train.GradientDescentOptimizer</span><br></pre></td></tr></table></figure><p>优化器是一个抽象类，有许多实现，这里实现的是梯度下降优化器。</p><p>在此基础上增加优化操作：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tf.train.GradientDecentOptimizer(lr).minimize(cross_entropy)</span><br></pre></td></tr></table></figure><p>梯度如何计算呢？</p><p>TensorFlow中每个图节点都有一个附加的梯度操作，都有相对于输入预先构建的输出梯度，通过图使用链式法则进行反向传播计算，这是非常简单的，也是将数值计算作为图的主要优势</p><p>这一切都是自动进行的</p><p>代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">prediction = tf.nn.softmax(...)</span><br><span class="line">label = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">10</span>])</span><br><span class="line"></span><br><span class="line">cross_entropy = tf.reduce mean(-tf.reduce sum(label * tf.log(prediction), </span><br><span class="line">reduction indices=[<span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.5</span>).minimize(cross_entropy).</span><br></pre></td></tr></table></figure><p>创建变量train_step，接收梯度下降优化器，设置学习率为0.5，并最小化交叉熵</p><p><strong>在session上运行：</strong></p><p>创建session-&gt;建立训练计划-&gt;运行train_step</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(tf.initialize_all_variables())</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">batchx, batch_label = data.next_batch()</span><br><span class="line">sess.run(train_step, feed dict=&#123;x: batch-x,label: batch-label&#125;)</span><br></pre></td></tr></table></figure><h1 id="API笔记"><a href="#API笔记" class="headerlink" title="API笔记"></a>API笔记</h1>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Deep-Learning-Frameworks&quot;&gt;&lt;a href=&quot;#Deep-Learning-Frameworks&quot; class=&quot;headerlink&quot; title=&quot;Deep Learning Frameworks&quot;&gt;&lt;/a&gt;Deep Learning 
      
    
    </summary>
    
      <category term="cs224n" scheme="http://yoursite.com/categories/cs224n/"/>
    
    
      <category term="cs224n" scheme="http://yoursite.com/tags/cs224n/"/>
    
      <category term="TensorFlow" scheme="http://yoursite.com/tags/TensorFlow/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读：Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation</title>
    <link href="http://yoursite.com/2018/12/11/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9AJoint%20Learning%20of%20the%20Embedding%20of%20Words%20and%20Entities%20for%20Named%20Entity%20Disambiguation/"/>
    <id>http://yoursite.com/2018/12/11/论文阅读：Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation/</id>
    <published>2018-12-11T13:03:28.000Z</published>
    <updated>2018-12-11T13:20:15.093Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-阅读文献来源"><a href="#1-阅读文献来源" class="headerlink" title="1. 阅读文献来源"></a>1. 阅读文献来源</h1><p>Yamada I, Shindo H, Takeda H, et al. Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation[J]. 2016:250-259.</p><h1 id="2-论文解决的科学问题及研究动机"><a href="#2-论文解决的科学问题及研究动机" class="headerlink" title="2. 论文解决的科学问题及研究动机"></a>2. 论文解决的科学问题及研究动机</h1><p>在NED（Named Entity Disambiguation）中的主要困难是实体指称词意义的模糊性。</p><p>早期研究集中于建模上下文语境，比如上下文与候选实体的百科页面的相似度(Bunescu and Pasca, 2006;Mihalcea and Csomai, 2007)。很多先进的方法通过使用更复杂的全局方法，所有指称词基于全局相关度进行同时消歧。</p><p>词向量逐渐流行(Mikolov et al., 2013a;Mikolov et al., 2013b; Pennington et al., 2014)。这种方法涉及到从大量非结构文本中学习单词的连续向量表达。当相似单词被放在低维向量空间中相邻的位置上时，这些向量可以捕捉单词之间的语义相似度.</p><h1 id="3-所提算法创新的工作"><a href="#3-所提算法创新的工作" class="headerlink" title="3. 所提算法创新的工作"></a>3. 所提算法创新的工作</h1><p><strong>传统skip-gram模型：</strong></p><p>Skip-gram模型的训练目标是给定目标单词的情况下找到有助于预测上下文词汇的单词表达。</p><p>形式化：给定T个单词的序列：<img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image002.gif" alt="img">。这个模型旨在最大化下面的目标函数：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image004.jpg" alt="img"></p><p>其中c是上下文窗口的大小，</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image006-1544533695373.gif" alt="img"></p><p>代表目标实体</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image008.gif" alt="img"></p><p>代表其上下文单词。</p><p>条件概率</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image010.gif" alt="img"></p><p>通过下面的softmax函数计算：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image012.jpg" alt="img"></p><p>其中W是词汇表中所有单词的集合，</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image014.gif" alt="img"></p><p>和</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image016.gif" alt="img"></p><p>代表单词</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image018.gif" alt="img"></p><p>在矩阵V和U中向量。<br>通过调整V来使得目标函数最大化，即给定目标实体上下文出现概率最大。这样本文就得到了一个代表T个单词的向量表达V。</p><p><strong>对skip-gram模型进行扩展：</strong></p><p>本文扩展矩阵V和U除了单词向量之外还包括实体向量，得到</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image020.gif" alt="img"></p><p>和</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image022.gif" alt="img"></p><p><strong>知识库图模型KB Graph Model：</strong></p><p>本文在KB中使用一个内部连接结构，使得模型学习到实体对之间的相关度。Wikipedia Link-based Measure（WLM）是一个基于连接结构测量实体相关度的方法。计算两个实体相关度的方法如下：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image024.jpg" alt="img"></p><p>其中E是KB中所有实体的集合，Ce是与实体e有连接的实体集合。直观上，WLM认为具有相似连接的实体是相关的。虽然WLM简单，但是比很多算法要好。</p><p>启发自WLM，知识库图模型将具有相似连接的实体放在向量空间中相近的位置，通过下面的目标函数来形式化这个过程：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image026.jpg" alt="img"></p><p>本文使用softmax函数来计算上述公式中的条件概率：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image028.jpg" alt="img"></p><p>对于给定的实体e，本文训练模型来预测连接Ce。因此，Ce在skip-gram模型中和上下文单词起到相似的作用。</p><p><strong>Anchor Context Model（锚文本模型）：</strong></p><p>如果本文仅考虑在skip-gram模型中加入KB图模型，实体和单词的向量不会相互作用，这些向量会被放在向量空间中不同的子空间中。为了解决这个问题，本文提出Anchor Context Model将相似的实体和单词放在向量空间中相近的位置。</p><p>思想：利用KB锚和上下文来训练模型。Wikipedia中有很多内部锚，利用这些，本文能从KB中直接获取到实体及其上下文单词的出现。</p><p>本文训练模型通过目标锚来预测一个实体指向的上下文单词。目标函数如下：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image030.jpg" alt="img"></p><p>其中A代表KB中的锚集合，其中每一个包含一对实体指称<br><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image032.gif" alt="img"><br>和它的上下文单词Q的集合。Q包含前c个单词和后c个单词。|A|表示KB中内部锚的数量。</p><p>条件概率通过softmax函数计算：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image034.jpg" alt="img"></p><p>使用提出的模型，本文通过将单词和具有相似上下文单词的实体放在向量空间中相邻的地方，来分配单词和实体的向量表达。</p><p><strong>结合</strong></p><p>通过3中提出的三种模型及它们的目标函数，本文将它们线性结合在一起，得到最终的目标函数：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image036.jpg" alt="img"></p><p>模型的训练意向最大化上述的函数，结果得到矩阵V被用于向量化单词和实体。</p><p>算法的一个问题是，在训练模型时，包含在softmax函数中的正则化项的计算是非常昂贵的，因为涉及到所有单词W或实体E的总和。为了解决这个问题，本文使用负采样（NEG）来将原始的目标函数转化为计算可行的目标函数。</p><p>NEG通过下面的目标函数定义：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image038.jpg" alt="img"></p><p>其中<br><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image040.jpg" alt="img"><br>是一个sigmoid函数，g是负样本的数量。本文替换公式（1）中的条件概率通过上面这个目标函数。因此，公式（1）的目标函数转化成了一个简单的二分类目标函数，来对从噪声分布Pneg(w)中提取的words（上文中翻译成单词，这里还是用words防止错误解读）进行辨别。同样的本文也可以对公式（4）和公式（6）进行同样的替换。</p><p>NEG将噪声分布Pneg(w)作为一个自由参数。</p><p>本文使用Wikipedia来训练所有的模型。通过在Wikipedia页面上迭代几次来进行同时优化得到最大的目标函数。本文使用随机梯度下降（SGD）进行优化。</p><h1 id="4-算法框架"><a href="#4-算法框架" class="headerlink" title="4. 算法框架"></a>4. 算法框架</h1><p><strong>使用词向量进行NED：</strong></p><p>任务：给定文档d中的实体指称词集合</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image042.gif" alt="img"></p><p>KB中的实体集合</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image044.gif" alt="img"></p><p>将指称词指向对应的实体。<br>本文将NED任务分成两个部分：候选集生成和指称消歧。</p><p><strong>指称消歧</strong></p><p>给定一个文档d和指称词m以及它的候选实体集合{ e1, e2,…, ek}，任务是从集合中选择最相关实体对m进行消歧。</p><p>提升效果的关键是有效建模上下文，本文提出两个模型，使用提出的词向量模型对上下文进行建模。本文使用监督学习将两个模型和标准NED特征结合起来。</p><p><strong>1）建模文本上下文：</strong></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image046.jpg" alt="img"></p><p>其中</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image048.gif" alt="img"></p><p>是指称词m的上下文集合，</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image050.gif" alt="img"></p><p>代表单词w的词向量。本文使用文档d中的所有名词作为上下文（&lt;<a href="https://opennlp.apache.org/）。" target="_blank" rel="noopener">https://opennlp.apache.org/）。</a><br>然后本文计算候选实体和文本上下文的相似度，通过</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image052.gif" alt="img"></p><p>和</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image054.gif" alt="img"></p><p>余弦相似度。</p><p><strong>2）建模相关度</strong></p><p>一个简单的两步方法：第一步，在非歧指称词上使用相关度得分训练机器学习模型；第二步，在预测到的实体分配上使用相关度得分重新训练模型。</p><p>评价相关度：先计算上下文实体（非歧义的实体）的词向量，计算上下文实体向量和目标实体e向量的相似度。为了获得上下文实体的词向量，本文平均他们的词向量：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image056.jpg" alt="img"></p><p>其中<img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image058.gif" alt="img">表示上下文实体集合。<br>为了评价相关度得分，本文重新使用上下文实体<img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image060.gif" alt="img">和实体<img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image054.gif" alt="img">之间的余弦相似度。</p><p><strong>3）学会排序</strong></p><p>本文采用监督机器学习方法来排序给定指称词m和文档d之后的候选实体集，目的是为了将标准NED特征和上下文信息结合起来。</p><p>梯度提升回归树（GBRT）（<a href="http://scikit-learn.org/stable/）：包含一组回归树，给定实例预测相关度得分。代价函数为logistic" target="_blank" rel="noopener">http://scikit-learn.org/stable/）：包含一组回归树，给定实例预测相关度得分。代价函数为logistic</a> loss。主要参数有迭代次数，学习速率和决策树的最大深度。</p><h1 id="5-实验"><a href="#5-实验" class="headerlink" title="5. 实验"></a>5. 实验</h1><p><strong>训练词向量的数据：</strong><a href="https://dumps.wikimedia.org/" target="_blank" rel="noopener">https://dumps.wikimedia.org/</a></p><p><strong>1）测试实体词向量的质量：</strong></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image062.jpg" alt="img"></p><p><strong>2）命名实体消歧</strong></p><p><strong>数据集：CoNLL（Hoffart et al. 2011）、TAC 2010（Ji et al. 2010）。</strong></p><p><strong>对比算法：Hoffart et al.、He et al.、Chisholm and Hachey、Pershina et al.</strong></p><p><strong>本文算法的结果：</strong></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image064.jpg" alt="img"></p><p><strong>表3是对比实验结果：</strong></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image066.jpg" alt="img"></p><p><strong>表4是本文算法不同特征学习的结果：</strong></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image068.jpg" alt="img"></p><p><strong>在加入基于本文的词向量模型的上下文特征时，效果大大的提升了。</strong></p><p><strong>错误分析：观察到在CoNLL测试集上，大约48.6%的错误率是由于metonymy mentions造成的。当一个流行的错误实体，匹配到指称词，本文的NED算法经常出错，这是由于本文的算法利用KB中的实体流行性数据。</strong></p><h1 id="6-工作评价"><a href="#6-工作评价" class="headerlink" title="6. 工作评价"></a>6. 工作评价</h1><p>本文提出了一种词嵌入方法，将文字和实体联合映射到相同的连续向量空间中。本文的方法使本文能够有效地对文本和全局的文本进行建模。此外，在这些背景下，本文的NED方法优于最先进的NED方法。</p><p><strong>在未来的工作中，本文打算通过利用关系知识来改进本文的模型，例如在知识图中（例如Freebase的关系。本文也想使本文的嵌入应用到其他的应用领域。</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-阅读文献来源&quot;&gt;&lt;a href=&quot;#1-阅读文献来源&quot; class=&quot;headerlink&quot; title=&quot;1. 阅读文献来源&quot;&gt;&lt;/a&gt;1. 阅读文献来源&lt;/h1&gt;&lt;p&gt;Yamada I, Shindo H, Takeda H, et al. Joint L
      
    
    </summary>
    
      <category term="nlp" scheme="http://yoursite.com/categories/nlp/"/>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="论文阅读" scheme="http://yoursite.com/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
      <category term="实体链接" scheme="http://yoursite.com/tags/%E5%AE%9E%E4%BD%93%E9%93%BE%E6%8E%A5/"/>
    
      <category term="知识图谱" scheme="http://yoursite.com/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
  </entry>
  
  <entry>
    <title>Zero-Shot应用于跨语言实体链接</title>
    <link href="http://yoursite.com/2018/12/06/Zero-Shot%E5%BA%94%E7%94%A8%E4%BA%8E%E8%B7%A8%E8%AF%AD%E8%A8%80%E5%AE%9E%E4%BD%93%E9%93%BE%E6%8E%A5/"/>
    <id>http://yoursite.com/2018/12/06/Zero-Shot应用于跨语言实体链接/</id>
    <published>2018-12-06T13:32:01.000Z</published>
    <updated>2018-12-06T13:41:31.037Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Palatucci-M-Pomerleau-D-Hinton-G-E-et-al-Zero-shot-learning-with-semantic-output-codes-C-Advances-in-neural-information-processing-systems-2009-1410-1418"><a href="#Palatucci-M-Pomerleau-D-Hinton-G-E-et-al-Zero-shot-learning-with-semantic-output-codes-C-Advances-in-neural-information-processing-systems-2009-1410-1418" class="headerlink" title="Palatucci M, Pomerleau D, Hinton G E, et al. Zero-shot learning with semantic output codes[C], Advances in neural information processing systems. 2009: 1410-1418."></a>Palatucci M, Pomerleau D, Hinton G E, et al. Zero-shot learning with semantic output codes[C], Advances in neural information processing systems. 2009: 1410-1418.</h1><h2 id="什么是zero-shot-learning"><a href="#什么是zero-shot-learning" class="headerlink" title="什么是zero-shot learning"></a>什么是zero-shot learning</h2><p>机器学习的算法通过学习分类器在很多领域获得了极大的成功。这些分类器被训练近似一个目标函数f: X-&gt;Y，给定标注数据，这些标注数据包括Y的所有可能值。</p><p>但是在某些领域，Y可以取很多值，如果要得到包含所有Y类别的标注数据，是非常困难的。因此，<strong>需要分类器自己去学习并得到在标注数据中不存在的Y值</strong>，这就是zero-shot learning。</p><h2 id="zero-shot-learning应用于神经活动解码"><a href="#zero-shot-learning应用于神经活动解码" class="headerlink" title="zero-shot learning应用于神经活动解码"></a>zero-shot learning应用于神经活动解码</h2><p>在神经活动解码中，目标是通过观察一个人的神经活动图像来确定这个人正在思考的词或物体。对可能出现的每一个单词进行神经训练图像的采集是一件非常困难的事情，因此，要构建一个实用的神经解码器，必须有一种方法来推断出训练集之外的单词。</p><p>这个问题类似于自动语音识别的挑战，在自动语音识别中，需要在分类器训练过程中不显式包含单词的情况下识别它们。为了实现词汇的独立性，语音识别系统通常采用基于音素的识别策略(Waibel, 1989)。音素是构成语言词汇的组成部分。语音识别系统的成功之处是，它利用了一组相对较小的音素识别器，并与将单词表示为音素组合的大型知识库相结合。</p><p><strong>为了将类似的方法应用于神经活动解码，必须发现如何从神经活动中推断词义的组成部分</strong>。虽然没有明确的共识,大脑如何编码语义信息(Plaut, 2002)，但是有几个提出的表示方法可以应用于神经活动知识库，从而使神经译码器识别大量可能出现的单词，即使这些词没有在训练集中出现。</p><h2 id="使用语义知识进行分类"><a href="#使用语义知识进行分类" class="headerlink" title="使用语义知识进行分类"></a>使用语义知识进行分类</h2><p>使用语义知识来推断新的类别</p><p>利用从语义知识库中得到的中间特征集来表示zero-shot learner：将每个类别用一个语义特征向量来表示，这样的向量可以表示大量的类别。那么模型初步就是：input data-&gt;semantic features-&gt;classes。作者需要做的是学习到输入数据和语义特征的关系以及语义特征如何映射到新的单词上。</p><h3 id="语义特征空间"><a href="#语义特征空间" class="headerlink" title="语义特征空间"></a>语义特征空间</h3><p>p维的语义特征空间是一个度量空间，其中每个p维对语义属性的值进行编码。这些属性可以通过性质来分类，也可以是实值数据。</p><p>比如：</p><p>is it furry? does it have a tail? can it breathe underwater? is it carnivorous? is it slow moving?</p><p>可以得到一个5维特征向量。输入dog可以得到{1,1,0,1,0}</p><h3 id="语义知识库"><a href="#语义知识库" class="headerlink" title="语义知识库"></a>语义知识库</h3><p>知识库K包含M个样本，这些样本表示为<br>$$<br>({f,y})_{1:M}<br>$$<br>其中f是p为语义空间内的一个点，y是集合Y中的一个类别标签。类别标签和语义特征空间一一对应进行编码。</p><h3 id="语义输出编码分类器"><a href="#语义输出编码分类器" class="headerlink" title="语义输出编码分类器"></a>语义输出编码分类器</h3><p>一个语义输出编码分类器<br>$$<br>H:X^d-&gt;Y<br>$$<br>从d为输入空间中映射到一个标签，H包含两个过程：<br>$$<br>H = L(S(·))<br>$$</p><p>$$<br>S: X^d-&gt;F^p<br>$$</p><p>$$<br>L:F^p-&gt;Y<br>$$</p><p>zero-shot classifier先将d维输入空间映射到p维语义空间，再映射为一个输出类别。</p><p><strong>一些细节</strong>：</p><p>学习S时，需要将标注数据从{x,y}转化为{x,f}，f指的是语义特征编码</p><p>得到的f及时有一些小偏差，L也可以矫正过来</p><p><strong>总结</strong>：</p><p>通过使用丰富的类别语义编码，分类器可以推断和识别新的类别。</p><h2 id="模型设计"><a href="#模型设计" class="headerlink" title="模型设计"></a>模型设计</h2><ul><li>使用多输出线性回归来学习语义输出码分类器的S(·)映射</li><li>在训练过程中，使用voxel-stability-criterion来将d从20,000降低500</li><li>L(·)使用1近邻分类器</li></ul><h1 id="Rijhwani-S-Xie-J-Neubig-G-et-al-Zero-shot-Neural-Transfer-for-Cross-lingual-Entity-Linking-J-arXiv-preprint-arXiv-1811-04154-2018"><a href="#Rijhwani-S-Xie-J-Neubig-G-et-al-Zero-shot-Neural-Transfer-for-Cross-lingual-Entity-Linking-J-arXiv-preprint-arXiv-1811-04154-2018" class="headerlink" title="Rijhwani S, Xie J, Neubig G, et al. Zero-shot Neural Transfer for Cross-lingual Entity Linking[J]. arXiv preprint arXiv:1811.04154, 2018."></a>Rijhwani S, Xie J, Neubig G, et al. Zero-shot Neural Transfer for Cross-lingual Entity Linking[J]. arXiv preprint arXiv:1811.04154, 2018.</h1><h2 id="跨语言实体链接"><a href="#跨语言实体链接" class="headerlink" title="跨语言实体链接"></a>跨语言实体链接</h2><p>跨语言实体链接将源语言中提到的实体映射到另一种(目标)语言的结构化知识库中的相应条目。</p><p>以往的研究主要依靠双语词汇资源来弥补源语言和目标语言之间的差距，但是对于许多低资源语言来说，这些资源是稀缺的或不可用的。</p><p>举个例子，马拉地语(“波兰”)作为输入实体，链接到英语知识库(KB)中的相应条目。</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1544081173449.png" alt="1544081173449"></p><h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><p>在单语言中，字符串相似度和Wikipedia锚文本可以很好的用来进行实体链接。但在跨语言中，这行不通，因为跨语言的实体大部分是不相似的。</p><p>对于跨语言实体链接任务，(Tsai and Roth 2016; Pan et al. 2017; Tsai and Roth 2018)使用双语资源来弥补这一差距，包括词汇和维基百科的跨语言链接。然而,世界上绝大多数的语言资源缺乏。为了这种低资源语言(LRLs)也可以进行跨语言实体链接，必须设计不过多依赖LRLs中的词汇或其他资源的方法。</p><p>自2011年以来，tac-kbp在实体链接方面的任务一直以中文/西班牙语链接到英语为特色(Ji, Grishman, and Dang 2011; Ji, Nothman, and Hachey 2014; Ji et al. 2017)。</p><p>McNamee et al. (2011) 介绍了跨语言实体链接任务，并设计了一种基于Wikipedia语言链接的候选实体检索技术。</p><p>Tsai and Roth (2016) 在EL上对12个语言使用词嵌入。</p><p>Pan et al. (2017)在282对语言对的EL中，使用逐字翻译进行大量的多语言链接。</p><p>Tsai and Roth (2018)提出更好的名称翻译（name translation），以提高现有基于翻译的EL技术的性能。</p><p><strong>Sil et al. (2017)提出的神经模型基于多语言词嵌入和维基百科链接，在TAC2015数据集上实现了SOTA的结果。</strong></p><h2 id="本文工作"><a href="#本文工作" class="headerlink" title="本文工作"></a>本文工作</h2><p>本文提出了pivot-based entity linking(PBEL)：基于低资源语言有紧密相关的高资源语言（HRLs）这一直觉。</p><p>比如马拉地语和老挝语资源相对较少，但与资源丰富的北印度语和泰语属于同一语系。</p><p>本文利用这些HRLs中提供的双语词汇和结构化信息来改进LRLs的实体链接性能。</p><h3 id="算法创新"><a href="#算法创新" class="headerlink" title="算法创新"></a>算法创新</h3><p><strong>Zero-shot transfer of neural entity linking models：</strong></p><p>使用一个高资源语言HRL和英语之间的双语词典，训练一个将HRL中的实体链接到英语知识库中的字符级别的神经模型。这个模型可以迁移到为LRL执行实体链接，不需要有任务具体的语言微调。</p><p>这种迁移学习方案在密切相关的语言之间使用时，已经成功地应用于其他任务，如形态标记和机器翻译(Zoph et al. 2016;Cotterell and Heigold 2017)。</p><p><strong>Pivoting：</strong></p><p>实体并不是直接从一个LRL链接到英语，而是先链接到一个密切相关的HRL。然后，使用HRL中现成的双语词汇来获得相应的英语实体链接。</p><p>即使用HRL作为LRL和英语之间的一个中间轴。</p><p><strong>The use of phonological representations for cross-lingual EL：</strong></p><p>当HRL和LRL不使用同样的书写系统的时候，字符级别的模型迁移将会失败。</p><p>使用国际音标表（IPA）来建立不同语言之间的联系，如前面的图中”Poland”在马拉地语和印度语中的发音是非常相似的。</p><p>本文提出两种表示：</p><ul><li>Phoneme embeddings音素嵌入</li></ul><p>使用Epitran将HRL和英语之间所有相对应的训练数据转换为IPA, Epitran是一个图形-音素系统，支持超过55种语言(Mortensen, Dalmia, and Littell 2018)。</p><ul><li>Articulatory feature embeddings发音特征嵌入</li></ul><p>使用PanPhone转化IPA训练数据为发音特征(Mortensen et al. 2016)。发音特征可以潜在地捕获在IPA中可能不明显的发音的重要特征，正如改进低资源命名实体识别(Mortensen et al. 2016)所展示的。</p><h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><p><strong>1. 实体相似编码器：训练两个神经编码器HRL encoder和English encoder使得HRL和English中对应的向量表示相似</strong></p><p>使用字符级双向LSTM(Bi-LSTM)将实体编码到一个连续的向量空间。</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1544086794670.png" alt="1544086794670"></p><p>这个模型的训练目标是最大化HRL和English中对等实体的余弦相似度(很像Mitra and Craswell 2017在神经信息精索方面的工作)。</p><p>每个实体都是字符的序列，下面是HRL和English中两个对等的实体：<br>$$<br>e_{HRL} = &lt;c_1,c_2,…,c_M&gt;<br>$$</p><p>$$<br>e_{en}=&lt;k_1,k_2,…,k_N&gt;<br>$$</p><p>对每个字符，可以得到一个固定大小的字符嵌入，然后将其输入双向LSTM中，最后的状态组成编码实体向量：<br>$$<br>V_{HRL}=HRL-Bi-LSTM(&lt;c_1,c_2,…,c_M&gt;)<br>$$</p><p>$$<br>V_{en}=English-Bi_LSTM(&lt;k_1,k_2,…,k_N&gt;)<br>$$</p><p>相似度计算：<br>$$<br>sim(e_{HRL},e_{en})=cosine(V_{HRL},V_{en})<br>$$<br>本文使用(Collobert et al. 2011)中已有的工作：使用最大边际损失的负采样来训练编码器，为了更有效的训练一个可以针对给定mention排序KB实体的模型。</p><p>损失函数：<br>$$<br>L=max(0,sim(e_{HRL},e_{en})-sim(e_{HRL},e^*_{en})+\lambda)<br>$$<br><strong>2. Pivoting for Candidate Generation：候选实体生成</strong></p><p><strong>3. Zero-shot Transfer to LRL：使用HRL encoder对mention进行编码，使用English encoder对英语实体进行编码，计算两者之间的相似度</strong></p><p>如果使用与LRL足够相似的HRL来训练模型，那么实体编码器就可以有效地预测mention和英语知识库实体之间的相似性。本文使用HRL-Bi-LSTM来编码mention，使用English-Bi-LSTM来编码英语知识库实体，然后求相似度：<br>$$<br>sim(m,e_{en})=cosine(V_m,V_{en})<br>$$<br><strong>4. Pivoting：使用HRL encoder分别对mention和HRL实体进行编码，计算两者之间的相似度</strong></p><p>使用HRL作为LRL和英语之间的中间枢轴，考虑英语实体在HRL中相对应的实体，求相似度：<br>$$<br>sim(m,e_{HRL})=cosine(V_m,V_{HRL})<br>$$<br><strong>5. 取较大值最为mention和英语实体之间的相似度</strong><br>$$<br>score(m,e_{en})=max(sim(m,e_{en}),sim(m,e_{HRL}))<br>$$</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><strong>两个实验任务：</strong></p><ul><li>跨语言知识库标题链接</li></ul><p>测试集：从Wikipedia中LRL和English相对应的title中得到。</p><p>测试九个LRL，来自不同的语言家族：Tigrinya (ti), Lao (lo), Uyghur (ug), Telugu (te), Punjabi (pa), Javanese (jv), Marathi (mr), Bengali (bn) and Ukrainian (uk)</p><p>使用53个HRLs作为潜在迁移语言。</p><ul><li>跨语言全链接</li></ul><p>测试集：使用来自DARPA LORELEI程序的标注文档（<a href="https://www.darpa.mil/program/low-resource-languages-for-emergent-incidents），两个极低资源的语言-Tigrinya和Oromo。" target="_blank" rel="noopener">https://www.darpa.mil/program/low-resource-languages-for-emergent-incidents），两个极低资源的语言-Tigrinya和Oromo。</a></p><p><strong>实体相似度评分模型：</strong></p><p>考虑三个模型用来进行跨语言实体链接评分：其中两个是基于SOTA的单语言或跨语言实体链接方法的文献(Ji and Grishman 2011; Pan et al. 2017; Sil et al. 2017)，因为需要具有LRL和英语的双语词典，所以不适合我们的zero-shot；第三个是字符级别的神经解码器。</p><ul><li>EXACT：(Sil et al. 2017)精确匹配知识库用于SOTA单语EL系统。</li><li>TRANS：该基线是Pan et al.(2017)在SOTA低资源EL系统中使用的候选检索技术，该技术试图将mention翻译成英语，以预测实体链接。</li><li>ENCODE：训练一个相似度编码器(Neubig et al. 2017)，使用English和HRL之间相对应的Wikipedia标题。然后使用HRL-Bi-LSTM来迁移编码mention。</li></ul><p><strong>结果：</strong></p><ul><li>跨语言知识库标题链接</li></ul><p>下面是不同模型带来的Accuracy结果，ENCODE用了两种模型，一种是MANUAL，即手动选择HRL，另一种是BEST-53，从53个HRL个挑选结果最好的作为和LRL同一语系。而PBEL（本文算法）还采用了一种模型MULTI，对单个LRL，使用53个轴语言，这些语言进行非加权或者系统距离加权结合。</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1544100032867.png" alt="1544100032867"></p><ul><li>跨语言全链接</li></ul><p>挑选合适的HRL来训练ENCODE和PBEL：Amharic for Tigrinya and Somali for Oromo.</p><p>训练数据是HRL和English之间的相对应实体。</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1544100045952.png" alt="1544100045952"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文工作有潜力通过语言家族来进行低资源实体链接。</p><p>本文使用了zero-shot迁移，在baseline上提升了17%的效果。</p><p>未来工作在于训练对于一个给定的命名实体mention，来预测最好的轴语言，可以替代文中所用到的系统距离加权。</p><p>还有个不足之处在于对每个语言训练一个编码器。通用多语言编码器在翻译等领域已经取得了成功(Johnson et al. 2016; Ha, Niehues, andWaibel 2016) and，预计可以提升本文的模型适应大量语言。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Palatucci-M-Pomerleau-D-Hinton-G-E-et-al-Zero-shot-learning-with-semantic-output-codes-C-Advances-in-neural-information-processing-s
      
    
    </summary>
    
      <category term="nlp" scheme="http://yoursite.com/categories/nlp/"/>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="实体链接" scheme="http://yoursite.com/tags/%E5%AE%9E%E4%BD%93%E9%93%BE%E6%8E%A5/"/>
    
      <category term="知识图谱" scheme="http://yoursite.com/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
  </entry>
  
  <entry>
    <title>《CS224n: Natural Language Processing with Deep Learning》Assignments1: 理论推导部分</title>
    <link href="http://yoursite.com/2018/12/05/%E3%80%8ACS224n-Natural-Language-Processing-with-Deep-Learning%E3%80%8BAssignments1-%E7%90%86%E8%AE%BA%E6%8E%A8%E5%AF%BC%E9%83%A8%E5%88%86/"/>
    <id>http://yoursite.com/2018/12/05/《CS224n-Natural-Language-Processing-with-Deep-Learning》Assignments1-理论推导部分/</id>
    <published>2018-12-05T02:49:08.000Z</published>
    <updated>2018-12-05T08:12:12.088Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Assignment-1"><a href="#Assignment-1" class="headerlink" title="Assignment #1"></a>Assignment #1</h1><h2 id="1-Softmax"><a href="#1-Softmax" class="headerlink" title="1 Softmax"></a>1 Softmax</h2><h3 id="a"><a href="#a" class="headerlink" title="(a)"></a>(a)</h3><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1543996643888.png" alt="1543996643888"></p><h2 id="2-Neural-Network-Basics"><a href="#2-Neural-Network-Basics" class="headerlink" title="2 Neural Network Basics"></a>2 Neural Network Basics</h2><h3 id="a-1"><a href="#a-1" class="headerlink" title="(a)"></a>(a)</h3><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1543996660008.png" alt="1543996660008"></p><h3 id="b"><a href="#b" class="headerlink" title="(b)"></a>(b)</h3><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1543996682477.png" alt="1543996682477"></p><h3 id="c"><a href="#c" class="headerlink" title="(c)"></a>(c)</h3><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1543996692851.png" alt="1543996692851"></p><h3 id="d"><a href="#d" class="headerlink" title="(d)"></a>(d)</h3><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1543996699758.png" alt="1543996699758"></p><h2 id="3-word2vec"><a href="#3-word2vec" class="headerlink" title="3 word2vec"></a>3 word2vec</h2><h3 id="a-2"><a href="#a-2" class="headerlink" title="(a)"></a>(a)</h3><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1543996771099.png" alt="1543996771099"></p><h3 id="b-1"><a href="#b-1" class="headerlink" title="(b)"></a>(b)</h3><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1543996782774.png" alt="1543996782774"></p><h3 id="c-1"><a href="#c-1" class="headerlink" title="(c)"></a>(c)</h3><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1543996789539.png" alt="1543996789539"></p><h3 id="d-1"><a href="#d-1" class="headerlink" title="(d)"></a>(d)</h3><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1543996801191.png" alt="1543996801191"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;Assignment-1&quot;&gt;&lt;a href=&quot;#Assignment-1&quot; class=&quot;headerlink&quot; title=&quot;Assignment #1&quot;&gt;&lt;/a&gt;Assignment #1&lt;/h1&gt;&lt;h2 id=&quot;1-Softmax&quot;&gt;&lt;a href=&quot;#1-
      
    
    </summary>
    
      <category term="nlp" scheme="http://yoursite.com/categories/nlp/"/>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="CS224n" scheme="http://yoursite.com/tags/CS224n/"/>
    
  </entry>
  
  <entry>
    <title>论文阅读：Neural Cross-Lingual Entity Linking</title>
    <link href="http://yoursite.com/2018/12/03/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB%EF%BC%9ANeural%20Cross-Lingual%20Entity%20Linking/"/>
    <id>http://yoursite.com/2018/12/03/论文阅读：Neural Cross-Lingual Entity Linking/</id>
    <published>2018-12-03T03:21:05.000Z</published>
    <updated>2018-12-03T04:29:11.193Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1-阅读文献来源"><a href="#1-阅读文献来源" class="headerlink" title="1. 阅读文献来源"></a>1. 阅读文献来源</h1><p> Sil A , Kundu G , Florian R , et al. Neural Cross-Lingual Entity Linking[J]. 2017.</p><p>Avirup Sil : Research Scientist &amp; NLP Chair at IBM Research AI</p><p>这篇论文在CoNLL（YAGO）和TAC2010上的结果非常好，在CoNLL上与deeptype不相上下，在TAC2010上结果稍差。</p><h1 id="2-论文解决的科学问题及研究动机"><a href="#2-论文解决的科学问题及研究动机" class="headerlink" title="2. 论文解决的科学问题及研究动机"></a>2. 论文解决的科学问题及研究动机</h1><p><strong>本文的introduction主要讨论了细粒度的相似度的问题并给出解决方案</strong>：</p><p>对于实体链接的一些不明确情况，需要计算mention的上下文和候选实体的标题页之间的细粒度的相似性。</p><p>考虑以下例子:</p><p>e1: Alexander Douglas Smith is an American football quar- terback for the Kansas City Chiefs of the National Football League (NFL). </p><p>e2: Edwin Alexander “Alex” Smith is an American football tight end who was drafted by the Tampa Bay Buccaneers in the third round of the 2005 NFL Draft.</p><p>e3: Alexander Smith was a Scottish-American professional golfer who played in the late 19th and early 20th century. </p><p>q: Last year, while not one of the NFL’s very best quarter- backs, Alex Smith did lead the team to a strong 12-4 season.</p><p>e3中的Alexander Smith是一名golfer，显然与q中的Alexander Smith不同，这个比较好区分；但是对于e1和e2中的Alexander Smith，都提到了American football players，甚至提到了关键词：NEL。这个就比较难区分了。需要进行细粒度的相似度计算。</p><p>本文提出训练最先进的(SOTA)相似模型，用于计算mention的上下文和维基百科的消歧候选页面之间的相似度，以期正确地解决上述那些含糊的情况。出于这个目的，<strong>作者抽取了不同粒度水平的信息(针对entity coreference chain和surrounding mentions)</strong>，使用了以下方法：a combination of convolutional neural networks (CNN), LSTMs (Hochreiter and Schmidhuber 1997), Lexical Composition and Decomposition (Wang, Mi, and Ittycheriah 2016), Multi-Perspective Context Matching (MPCM) (Wang et al. 2016), and Neural Tensor Networks (Socher et al. 2013a; 2013c) 来编码这些信息并进行实体链接。</p><p>TAC社区对<strong>跨语言EL</strong>也非常感兴趣（Tsai and Roth 2016; Sil and Florian 2016）：如在西班牙语或汉语等外语文献中的mention，人们需要在英语维基百科中找到相应的链接。限制条件：we have extremely limited (or possibly even no) linguis- tic resources and no machine translation technology。</p><p>作者提到了该领域之前的一个工作：Tsai and Roth 2016提出了一种使用多语言嵌入的cross-lingual wikifier。然而，他们的模型需要针对每种新语言进行重新训练，因此并不完全适合TAC任务。</p><p><strong>作者提出了一种zero shot learning technique</strong>(Palatucci et al. 2009: Socher et al. 2013b) for their neural EL model：一旦使用english训练，就可以应用于跨语言的EL，而无需再训练。</p><h1 id="3-算法框架"><a href="#3-算法框架" class="headerlink" title="3. 算法框架"></a>3. 算法框架</h1><h2 id="1-Fast-Match-Search生成候选实体"><a href="#1-Fast-Match-Search生成候选实体" class="headerlink" title="1. Fast Match Search生成候选实体"></a>1. Fast Match Search生成候选实体</h2><p>快速匹配搜索的目标是生成候选实体。过程其实很简单：提取链接锚文本映射到其目标维基百科标题的信息。为了对mention生成对应的候选实体，作者只需要检索上面提取到的anchor-title就可以，<strong>候选实体集合被认为是锚文本m最频繁链接的实体集</strong>。比如泰坦尼克最频繁链接到的实体是：电影和船，那么下次碰到泰坦尼克时，他的候选实体就是泰坦尼克（电影）和泰坦尼克（船)。</p><p>另外，作者从相关目标语言的wikipedia页面中提取anchor-title信息，然后得到非英语链接回英语wiki的候选实体信息。</p><h2 id="2-词嵌入"><a href="#2-词嵌入" class="headerlink" title="2. 词嵌入"></a>2. 词嵌入</h2><p>分为<strong>单语言的词嵌入</strong>和<strong>跨语言的词嵌入</strong>。</p><ul><li><p>单语言的词嵌入：</p><pre><code>the widely used CBOW word2vec model (Mikolov et al. 2013)</code></pre></li><li><p>多语言的词嵌入：</p><ul><li><p>Canonical Correlation Analysis 典型相关分析 (CCA)</p><p>这种技术基于Faruqui and Dyer 2014， 首先在不同语言的文本上执行SVD，然后在平行语料中对齐的单词向量对上执行CCA。</p></li><li><p>MultiCCA</p><p>(Ammar et al. 2016) ：基于CCA，使用线性算子将每种语言（除了英语）的预训练单语言词嵌入投射到预训练英语词嵌入。</p></li><li><p>Weighted Least Squares 加权最小平方 (LS)</p><p>(Mikolov, Le, and Sutskever 2013)：其他语言的词嵌入直接投射到英语上，使用多变量回归得到的映射。</p></li></ul></li><li><p>wikipedia页面嵌入</p></li></ul><p>对wikipedia的整个页面（pages or called links）进行嵌入</p><p>第一步，<strong>计算维基百科页面中所有词的加权平均值</strong>。使用每个单词的idf作为他向量的权重，来减少频繁词的影响。计算公式如下：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/typora_images/1543489755502.png" alt="1543489755502"></p><p>其中 <img src="http://chart.googleapis.com/chart?cht=tx&chl= e_w" style="border:none;"> 是词向量，<img src="http://chart.googleapis.com/chart?cht=tx&chl= idf_w" style="border:none;"> 是单词w的逆文档频率IDF。</p><p>第二步：在上面得到的词嵌入上训练一个全连接<em>tanh</em>激活层，目的是将mention上下文和Wikipedia页面放入相似空间中。</p><h2 id="3-对上下文建模"><a href="#3-对上下文建模" class="headerlink" title="3. 对上下文建模"></a>3. 对上下文建模</h2><p>对文档D中的mention m的表示进行编码。编码后的表示（representation）将会与wiki页面向量进行比较（通过余弦相似度），比较结果放入更高层的网络。</p><p>notice：文档D对m的消歧不都有用，作者选择m附近的的句子来表示m。</p><p>编码m的过程：</p><p>作者先执行一个共指消解系统(Luo et al. 2004) ，然后建立m<strong>基于句子上下文的表示</strong>，以及关于mention出现的窗口内的单词的<strong>细粒度的上下文编码</strong>。</p><ul><li>对句子进行建模<ol><li>收集包含mention或者实体共指链的所有句子</li><li>结合这些句子形成包含mention所有实例的句子序列</li><li>使用CNN从这些变长句子上产生固定大小的向量<ol><li>先将每个单词嵌入d维向量空间，使用前面说的嵌入技术</li><li>然后使用CNN将这些单词映射为一个固定长度的向量</li></ol></li><li>应用<em>tanh</em>非线性层，并用mean-pooling将结果进行聚合</li><li>对Wikipedia页面的第一段也应用CNN来进行编码，编码结果视为该实体页面的编码。（与使用CNN编码整个Wikipedia页面不同）</li></ol></li><li>细粒度上下文建模</li></ul><p><strong>上面那种句子建模会忽视某些相关模式</strong>（introduction中举的那个例子就是），没有利用好mention附近的words，这些words是mention意义的强烈指示。作者将窗口大小为n（n=4）的单词作为mention的上下文：</p><p>在这些窗口上使用LSTMs：正向对左边窗口进行，反向对右边窗口进行(Cheng, Dong, and Lapata 2016)；使用mean-pooling作为融合策略。这些压缩后的表示将会被平均，然后使用神经张量网络进行结合，使用如下公式：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/typora_images/1543563100460.png" alt="1543563100460"></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/typora_images/1543563130211.png" alt="1543563130211"></p><p>l和r分别是左右上下文的总体向量，W是k切片的张量，f是非线性激活函数（这里是sigmoid）。NTN的输出是一个向量。</p><h2 id="4-跨语言神经实体链接神经模型架构"><a href="#4-跨语言神经实体链接神经模型架构" class="headerlink" title="4. 跨语言神经实体链接神经模型架构"></a>4. 跨语言神经实体链接神经模型架构</h2><p>目标：perform “zero shot learning” (Socher et al. 2013b; Palatucci et al. 2009) for cross-lingual EL</p><p>方法：在英语数据上训练一个模型，使用这个模型来对其他语言进行解码，假设有英语和目标语言的跨语言词嵌入。</p><p>模型：mention上下文和候选实体wiki页面的几种相似度作为输入，放入一个前向神经层H，权重<img src="http://chart.googleapis.com/chart?cht=tx&chl= W_h" style="border:none;">，偏置<img src="http://chart.googleapis.com/chart?cht=tx&chl= b_h" style="border:none;">，和一个sigmoid非线性激活函数。H的输出h通过公式来计算：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/typora_images/1543564299277.png" alt="1543564299277"></p><p>二分类器P(C|m,D,l)表示C=1（正确链接），C=0（不正确链接）的概率：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/typora_images/1543564386728.png" alt="1543564386728"></p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/typora_images/1543559723954.png" alt="1543559723954"></p><p><strong>1. 特征提取层</strong></p><p>这层对相似度进行编码</p><ul><li><strong>相似性特征（比较上下文表示）</strong><ul><li>”句子上下文-wiki页面“ 的相似度</li><li>“句子山下文-wiki第一段” 的相似度</li><li>“细粒度上下文-wiki页面” 的相似度</li><li>语言内部特征：LIEL系统(Sil and Florian 2016)里描述的loca features</li></ul></li></ul><p><strong>2. 多角度装箱层</strong></p><p>用多个高斯径向基函数作为输入，进行平滑装箱到向量中（灵感来自(Liu et al. 2016)）。上面的相似度特征输入到该层，将这些特征映射到高维向量。与(Liu et al. 2016)不同的是可以自动学习输入数值重要的部分。</p><ul><li><p><strong>语义相似和不相似</strong></p><ul><li>词汇分解和组合（LDC）</li></ul></li></ul><ul><li>多角度上下文匹配（MPCM）</li></ul><p><strong>3. 训练和解码</strong></p><p>二分类训练集的设置：使用fast match strategy生成mention对应的正确链接和不正确链接实体页面。</p><p>训练过程使用随机梯度下降</p><p>解码就是比较候选实体链接正确的概率。</p><h1 id="4-实验"><a href="#4-实验" class="headerlink" title="4. 实验"></a>4. 实验</h1><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><ul><li>English (CoNLL &amp; TAC): <ul><li>The CoNLL dataset (Hoffart et al. 2011) </li><li>TAC 2010 source</li></ul></li><li>Cross-Lingual (TAC)：<ul><li>TAC 2015 Tri-Lingual Entity Linking datasets</li></ul></li></ul><p>For the CoNLL experiments, in addition to the Wikipedia anchor-title index, we also use a alias-entity map- ping previously used by (Pershina, He, and Grishman 2015; Globerson et al. 2016; Yamada et al. 2016). We also use the mappings provided by (Hoffart et al. 2011) obtained by ex- tending the “means” tables of YAGO (Hoffart et al. 2013).</p><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>英语EL：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/typora_images/1543565503898.png" alt="1543565503898"></p><p>跨语言EL：</p><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/typora_images/1543565543687.png" alt="1543565543687"></p><h1 id="5-作者工作评价"><a href="#5-作者工作评价" class="headerlink" title="5. 作者工作评价"></a>5. 作者工作评价</h1><p>其他研究在单语言或跨语言的表现上都实现了SOTA，但不是都（而本文的结果是都好）。</p><p>作者提出的模型可以仅在英语上训练，就不需要其他训练就可以应用于其他语言，只需要有跨语言的词嵌入。</p><p>作者有效利用了相似性模型（LDC，MPCM），和神经张量网络，来捕捉mention和wiki页面的相似与不相似。</p><p>zero-shot learning在跨语言EM上的应用，对低资源的语言很有用。</p><h1 id="6-自己对该论文的评价"><a href="#6-自己对该论文的评价" class="headerlink" title="6. 自己对该论文的评价"></a>6. 自己对该论文的评价</h1><h2 id="算法创新点"><a href="#算法创新点" class="headerlink" title="算法创新点"></a>算法创新点</h2><ol><li>在候选实体的生成过程中，仅靠mention的字符串匹配的效率是很低的。（这个之前在实验中已经得到验证，TAC KBP的mention通过EL索引查找wiki中的候选实体的召回率只有50%上下）本文提出的快速匹配利用wiki页面中的锚文本数据，具有可信性和高召回率。</li><li>细粒度的相似度的提出：作者不但利用包含mention的句子和wiki页面的相似度，还加入了细粒度的相似度，这几种相似度作为神经网络的输入，避免了句子中不相关单词对mention消歧的影响。</li><li>将zero-shot learning应用到跨语言实体链接中，有效的解决了TAC竞赛给出的语言资料少的缺点，对比较少人用的语言链接到英语wiki也具有现实意义。</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;1-阅读文献来源&quot;&gt;&lt;a href=&quot;#1-阅读文献来源&quot; class=&quot;headerlink&quot; title=&quot;1. 阅读文献来源&quot;&gt;&lt;/a&gt;1. 阅读文献来源&lt;/h1&gt;&lt;p&gt; Sil A , Kundu G , Florian R , et al. Neural
      
    
    </summary>
    
      <category term="nlp" scheme="http://yoursite.com/categories/nlp/"/>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
      <category term="实体链接" scheme="http://yoursite.com/tags/%E5%AE%9E%E4%BD%93%E9%93%BE%E6%8E%A5/"/>
    
      <category term="知识图谱" scheme="http://yoursite.com/tags/%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1/"/>
    
  </entry>
  
  <entry>
    <title>[转载]自然语言处理如何入门？————周明博士</title>
    <link href="http://yoursite.com/2018/11/30/%E8%BD%AC%E8%BD%BD-%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%A6%82%E4%BD%95%E5%85%A5%E9%97%A8%EF%BC%9F%E2%80%94%E2%80%94%E5%91%A8%E6%98%8E%E5%8D%9A%E5%A3%AB/"/>
    <id>http://yoursite.com/2018/11/30/转载-自然语言处理如何入门？——周明博士/</id>
    <published>2018-11-30T06:43:05.000Z</published>
    <updated>2018-11-30T06:55:50.276Z</updated>
    
    <content type="html"><![CDATA[<p>作者：微软亚洲研究院<br>链接：<a href="https://www.zhihu.com/question/19895141/answer/149475410" target="_blank" rel="noopener">https://www.zhihu.com/question/19895141/answer/149475410</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p><p><strong>自然语言处理</strong>（简称NLP），是研究计算机处理人类语言的一门技术，包括：<br><strong>1.句法语义分析</strong>：对于给定的句子，进行分词、词性标记、命名实体识别和链接、句法分析、语义角色识别和多义词消歧。</p><p><strong>2.信息抽取</strong>：从给定文本中抽取重要的信息，比如，时间、地点、人物、事件、原因、结果、数字、日期、货币、专有名词等等。通俗说来，就是要了解谁在什么时候、什么原因、对谁、做了什么事、有什么结果。涉及到实体识别、时间抽取、因果关系抽取等关键技术。</p><p><strong>3.文本挖掘（或者文本数据挖掘）</strong>：包括文本聚类、分类、信息抽取、摘要、情感分析以及对挖掘的信息和知识的可视化、交互式的表达界面。目前主流的技术都是基于统计机器学习的。</p><p><strong>4.机器翻译</strong>：把输入的源语言文本通过自动翻译获得另外一种语言的文本。根据输入媒介不同，可以细分为文本翻译、语音翻译、手语翻译、图形翻译等。机器翻译从最早的基于规则的方法到二十年前的基于统计的方法，再到今天的基于神经网络（编码-解码）的方法，逐渐形成了一套比较严谨的方法体系。</p><p><strong>5.信息检索</strong>：对大规模的文档进行索引。可简单对文档中的词汇，赋之以不同的权重来建立索引，也可利用1，2，3的技术来建立更加深层的索引。在查询的时候，对输入的查询表达式比如一个检索词或者一个句子进行分析，然后在索引里面查找匹配的候选文档，再根据一个排序机制把候选文档排序，最后输出排序得分最高的文档。</p><p><strong>6.问答系统</strong>： 对一个自然语言表达的问题，由问答系统给出一个精准的答案。需要对自然语言查询语句进行某种程度的语义分析，包括实体链接、关系识别，形成逻辑表达式，然后到知识库中查找可能的候选答案并通过一个排序机制找出最佳的答案。</p><p><strong>7.对话系统</strong>：系统通过一系列的对话，跟用户进行聊天、回答、完成某一项任务。涉及到用户意图理解、通用聊天引擎、问答引擎、对话管理等技术。此外，为了体现上下文相关，要具备多轮对话能力。同时，为了体现个性化，要开发用户画像以及基于用户画像的个性化回复。</p><p>随着深度学习在图像识别、语音识别领域的大放异彩，人们对深度学习在NLP的价值也寄予厚望。再加上AlphaGo的成功，人工智能的研究和应用变得炙手可热。自然语言处理作为人工智能领域的认知智能，成为目前大家关注的焦点。很多研究生都在进入自然语言领域，寄望未来在人工智能方向大展身手。但是，大家常常遇到一些问题。俗话说，万事开头难。如果第一件事情成功了，学生就能建立信心，找到窍门，今后越做越好。否则，也可能就灰心丧气，甚至离开这个领域。这里针对给出我个人的建议，希望我的这些粗浅观点能够引起大家更深层次的讨论。</p><p><strong>建议1：如何在NLP领域快速学会第一个技能？</strong></p><p>我的建议是：找到一个开源项目，比如机器翻译或者深度学习的项目。理解开源项目的任务，编译通过该项目发布的示范程序，得到与项目示范程序一致的结果。然后再深入理解开源项目示范程序的算法。自己编程实现一下这个示范程序的算法。再按照项目提供的标准测试集测试自己实现的程序。如果输出的结果与项目中出现的结果不一致，就要仔细查验自己的程序，反复修改，直到结果与示范程序基本一致。如果还是不行，就大胆给项目的作者写信请教。在此基础上，再看看自己能否进一步完善算法或者实现，取得比示范程序更好的结果。</p><p><strong>建议2：如何选择第一个好题目？</strong></p><p>工程型研究生，选题很多都是老师给定的。需要采取比较实用的方法，扎扎实实地动手实现。可能不需要多少理论创新，但是需要较强的实现能力和综合创新能力。而学术型研究生需要取得一流的研究成果，因此选题需要有一定的创新。我这里给出如下的几点建议。</p><ul><li>先找到自己喜欢的研究领域。你找到一本最近的ACL会议论文集, 从中找到一个你比较喜欢的领域。在选题的时候，多注意选择蓝海的领域。这是因为蓝海的领域，相对比较新，容易出成果。</li><li>充分调研这个领域目前的发展状况。包括如下几个方面的调研：方法方面，是否有一套比较清晰的数学体系和机器学习体系；数据方面，有没有一个大家公认的标准训练集和测试集；研究团队，是否有著名团队和人士参加。如果以上几个方面的调研结论不是太清晰，作为初学者可能不要轻易进入。</li><li>在确认进入一个领域之后，按照建议一所述，需要找到本领域的开源项目或者工具，仔细研究一遍现有的主要流派和方法，先入门。</li><li>反复阅读本领域最新发表的文章，多阅读本领域牛人发表的文章。在深入了解已有工作的基础上，探讨还有没有一些地方可以推翻、改进、综合、迁移。注意做实验的时候，不要贪多，每次实验只需要验证一个想法。每次实验之后，必须要进行分析存在的错误，找出原因。</li><li>对成功的实验，进一步探讨如何改进算法。注意实验数据必须是业界公认的数据。</li><li>与已有的算法进行比较，体会能够得出比较一般性的结论。如果有，则去写一篇文章，否则，应该换一个新的选题。</li></ul><p><strong>建议3：如何写出第一篇论文？</strong></p><ul><li>接上一个问题，如果想法不错，且被实验所证明，就可开始写第一篇论文了。</li><li>确定论文的题目。在定题目的时候，一般不要“…系统”、“…研究与实践”，要避免太长的题目，因为不好体现要点。题目要具体，有深度，突出算法。</li><li>写论文摘要。要突出本文针对什么重要问题，提出了什么方法，跟已有工作相比，具有什么优势。实验结果表明，达到了什么水准，解决了什么问题。</li><li>写引言。首先讲出本项工作的背景，这个问题的定义，它具有什么重要性。然后介绍对这个问题，现有的方法是什么，有什么优点。但是（注意但是）现有的方法仍然有很多缺陷或者挑战。比如（注意比如），有什么问题。本文针对这个问题，受什么方法（谁的工作）之启发，提出了什么新的方法并做了如下几个方面的研究。然后对每个方面分门别类加以叙述，最后说明实验的结论。再说本文有几条贡献，一般写三条足矣。然后说说文章的章节组织，以及本文的重点。有的时候东西太多，篇幅有限，只能介绍最重要的部分，不需要面面俱到。</li><li>相关工作。对相关工作做一个梳理，按照流派划分，对主要的最多三个流派做一个简单介绍。介绍其原理，然后说明其局限性。</li><li>然后可设立两个章节介绍自己的工作。第一个章节是算法描述。包括问题定义，数学符号，算法描述。文章的主要公式基本都在这里。有时候要给出简明的推导过程。如果借鉴了别人的理论和算法，要给出清晰的引文信息。在此基础上，由于一般是基于机器学习或者深度学习的方法，要介绍你的模型训练方法和解码方法。第二章就是实验环节。一般要给出实验的目的，要检验什么，实验的方法，数据从哪里来，多大规模。最好数据是用公开评测数据，便于别人重复你的工作。然后对每个实验给出所需的技术参数，并报告实验结果。同时为了与已有工作比较，需要引用已有工作的结果，必要的时候需要重现重要的工作并报告结果。用实验数据说话，说明你比人家的方法要好。要对实验结果好好分析你的工作与别人的工作的不同及各自利弊，并说明其原因。对于目前尚不太好的地方，要分析问题之所在，并将其列为未来的工作。</li><li>结论。对本文的贡献再一次总结。既要从理论、方法上加以总结和提炼，也要说明在实验上的贡献和结论。所做的结论，要让读者感到信服，同时指出未来的研究方向。</li><li>参考文献。给出所有重要相关工作的论文。记住，漏掉了一篇重要的参考文献（或者牛人的工作），基本上就没有被录取的希望了。</li><li>写完第一稿，然后就是再改三遍。</li><li>把文章交给同一个项目组的人士，请他们从算法新颖度、创新性和实验规模和结论方面，以挑剔的眼光，审核你的文章。自己针对薄弱环节，进一步改进，重点加强算法深度和工作创新性。</li><li>然后请不同项目组的人士审阅。如果他们看不明白，说明文章的可读性不够。你需要修改篇章结构、进行文字润色，增加文章可读性。</li><li>如投ACL等国际会议，最好再请英文专业或者母语人士提炼文字。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;作者：微软亚洲研究院&lt;br&gt;链接：&lt;a href=&quot;https://www.zhihu.com/question/19895141/answer/149475410&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.zhihu.com/
      
    
    </summary>
    
    
      <category term="自然语言处理" scheme="http://yoursite.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2018/11/29/hello-world/"/>
    <id>http://yoursite.com/2018/11/29/hello-world/</id>
    <published>2018-11-29T04:23:55.292Z</published>
    <updated>2018-11-29T04:23:55.292Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>

<!DOCTYPE html>



  


<html class="theme-next gemini use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Blog, NLP">





  <link rel="alternate" href="/atom.xml" title="hellojet" type="application/atom+xml">






<meta name="description" content="余生很长 但求无憾 以梦为马 不负韶华">
<meta name="keywords" content="blog | nlp | knowledge graph">
<meta property="og:type" content="website">
<meta property="og:title" content="hellojet">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="hellojet">
<meta property="og:description" content="余生很长 但求无憾 以梦为马 不负韶华">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="hellojet">
<meta name="twitter:description" content="余生很长 但求无憾 以梦为马 不负韶华">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Gemini',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/">





  <title>hellojet</title>
  








</head>

<body itemscope="" itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">hellojet</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br>
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-schedule">
          <a href="/schedule/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-calendar"></i> <br>
            
            日程表
          </a>
        </li>
      
        
        <li class="menu-item menu-item-commonweal">
          <a href="/404/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-heartbeat"></i> <br>
            
            公益404
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="搜索..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/11/论文阅读：Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="李洁厅">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hellojet">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/11/论文阅读：Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation/" itemprop="url">论文阅读：Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-11T21:03:28+08:00">
                2018-12-11
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/nlp/" itemprop="url" rel="index">
                    <span itemprop="name">nlp</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="1-阅读文献来源"><a href="#1-阅读文献来源" class="headerlink" title="1. 阅读文献来源"></a>1. 阅读文献来源</h1><p>Yamada I, Shindo H, Takeda H, et al. Joint Learning of the Embedding of Words and Entities for Named Entity Disambiguation[J]. 2016:250-259.</p>
<h1 id="2-论文解决的科学问题及研究动机"><a href="#2-论文解决的科学问题及研究动机" class="headerlink" title="2. 论文解决的科学问题及研究动机"></a>2. 论文解决的科学问题及研究动机</h1><p>在NED（Named Entity Disambiguation）中的主要困难是实体指称词意义的模糊性。</p>
<p>早期研究集中于建模上下文语境，比如上下文与候选实体的百科页面的相似度(Bunescu and Pasca, 2006;Mihalcea and Csomai, 2007)。很多先进的方法通过使用更复杂的全局方法，所有指称词基于全局相关度进行同时消歧。</p>
<p>词向量逐渐流行(Mikolov et al., 2013a;Mikolov et al., 2013b; Pennington et al., 2014)。这种方法涉及到从大量非结构文本中学习单词的连续向量表达。当相似单词被放在低维向量空间中相邻的位置上时，这些向量可以捕捉单词之间的语义相似度.</p>
<h1 id="3-所提算法创新的工作"><a href="#3-所提算法创新的工作" class="headerlink" title="3. 所提算法创新的工作"></a>3. 所提算法创新的工作</h1><p><strong>传统skip-gram模型：</strong></p>
<p>Skip-gram模型的训练目标是给定目标单词的情况下找到有助于预测上下文词汇的单词表达。</p>
<p>形式化：给定T个单词的序列：<img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image002.gif" alt="img">。这个模型旨在最大化下面的目标函数：</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image004.jpg" alt="img"></p>
<p>其中c是上下文窗口的大小，</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image006-1544533695373.gif" alt="img"></p>
<p>代表目标实体</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image008.gif" alt="img"></p>
<p>代表其上下文单词。</p>
<p>条件概率</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image010.gif" alt="img"></p>
<p>通过下面的softmax函数计算：</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image012.jpg" alt="img"></p>
<p>其中W是词汇表中所有单词的集合，</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image014.gif" alt="img"></p>
<p>和</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image016.gif" alt="img"></p>
<p>代表单词</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image018.gif" alt="img"></p>
<p>在矩阵V和U中向量。<br>通过调整V来使得目标函数最大化，即给定目标实体上下文出现概率最大。这样本文就得到了一个代表T个单词的向量表达V。</p>
<p><strong>对skip-gram模型进行扩展：</strong></p>
<p>本文扩展矩阵V和U除了单词向量之外还包括实体向量，得到</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image020.gif" alt="img"></p>
<p>和</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image022.gif" alt="img"></p>
<p><strong>知识库图模型KB Graph Model：</strong></p>
<p>本文在KB中使用一个内部连接结构，使得模型学习到实体对之间的相关度。Wikipedia Link-based Measure（WLM）是一个基于连接结构测量实体相关度的方法。计算两个实体相关度的方法如下：</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image024.jpg" alt="img"></p>
<p>其中E是KB中所有实体的集合，Ce是与实体e有连接的实体集合。直观上，WLM认为具有相似连接的实体是相关的。虽然WLM简单，但是比很多算法要好。</p>
<p>启发自WLM，知识库图模型将具有相似连接的实体放在向量空间中相近的位置，通过下面的目标函数来形式化这个过程：</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image026.jpg" alt="img"></p>
<p>本文使用softmax函数来计算上述公式中的条件概率：</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image028.jpg" alt="img"></p>
<p>对于给定的实体e，本文训练模型来预测连接Ce。因此，Ce在skip-gram模型中和上下文单词起到相似的作用。</p>
<p><strong>Anchor Context Model（锚文本模型）：</strong></p>
<p>如果本文仅考虑在skip-gram模型中加入KB图模型，实体和单词的向量不会相互作用，这些向量会被放在向量空间中不同的子空间中。为了解决这个问题，本文提出Anchor Context Model将相似的实体和单词放在向量空间中相近的位置。</p>
<p>思想：利用KB锚和上下文来训练模型。Wikipedia中有很多内部锚，利用这些，本文能从KB中直接获取到实体及其上下文单词的出现。</p>
<p>本文训练模型通过目标锚来预测一个实体指向的上下文单词。目标函数如下：</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image030.jpg" alt="img"></p>
<p>其中A代表KB中的锚集合，其中每一个包含一对实体指称<br><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image032.gif" alt="img"><br>和它的上下文单词Q的集合。Q包含前c个单词和后c个单词。|A|表示KB中内部锚的数量。</p>
<p>条件概率通过softmax函数计算：</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image034.jpg" alt="img"></p>
<p>使用提出的模型，本文通过将单词和具有相似上下文单词的实体放在向量空间中相邻的地方，来分配单词和实体的向量表达。</p>
<p><strong>结合</strong></p>
<p>通过3中提出的三种模型及它们的目标函数，本文将它们线性结合在一起，得到最终的目标函数：</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image036.jpg" alt="img"></p>
<p>模型的训练意向最大化上述的函数，结果得到矩阵V被用于向量化单词和实体。</p>
<p>算法的一个问题是，在训练模型时，包含在softmax函数中的正则化项的计算是非常昂贵的，因为涉及到所有单词W或实体E的总和。为了解决这个问题，本文使用负采样（NEG）来将原始的目标函数转化为计算可行的目标函数。</p>
<p>NEG通过下面的目标函数定义：</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image038.jpg" alt="img"></p>
<p>其中<br><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image040.jpg" alt="img"><br>是一个sigmoid函数，g是负样本的数量。本文替换公式（1）中的条件概率通过上面这个目标函数。因此，公式（1）的目标函数转化成了一个简单的二分类目标函数，来对从噪声分布Pneg(w)中提取的words（上文中翻译成单词，这里还是用words防止错误解读）进行辨别。同样的本文也可以对公式（4）和公式（6）进行同样的替换。</p>
<p>NEG将噪声分布Pneg(w)作为一个自由参数。</p>
<p>本文使用Wikipedia来训练所有的模型。通过在Wikipedia页面上迭代几次来进行同时优化得到最大的目标函数。本文使用随机梯度下降（SGD）进行优化。</p>
<h1 id="4-算法框架"><a href="#4-算法框架" class="headerlink" title="4. 算法框架"></a>4. 算法框架</h1><p><strong>使用词向量进行NED：</strong></p>
<p>任务：给定文档d中的实体指称词集合</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image042.gif" alt="img"></p>
<p>KB中的实体集合</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image044.gif" alt="img"></p>
<p>将指称词指向对应的实体。<br>本文将NED任务分成两个部分：候选集生成和指称消歧。</p>
<p><strong>指称消歧</strong></p>
<p>给定一个文档d和指称词m以及它的候选实体集合{ e1, e2,…, ek}，任务是从集合中选择最相关实体对m进行消歧。</p>
<p>提升效果的关键是有效建模上下文，本文提出两个模型，使用提出的词向量模型对上下文进行建模。本文使用监督学习将两个模型和标准NED特征结合起来。</p>
<p><strong>1）建模文本上下文：</strong></p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image046.jpg" alt="img"></p>
<p>其中</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image048.gif" alt="img"></p>
<p>是指称词m的上下文集合，</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image050.gif" alt="img"></p>
<p>代表单词w的词向量。本文使用文档d中的所有名词作为上下文（&lt;<a href="https://opennlp.apache.org/）。" target="_blank" rel="noopener">https://opennlp.apache.org/）。</a><br>然后本文计算候选实体和文本上下文的相似度，通过</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image052.gif" alt="img"></p>
<p>和</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image054.gif" alt="img"></p>
<p>余弦相似度。</p>
<p><strong>2）建模相关度</strong></p>
<p>一个简单的两步方法：第一步，在非歧指称词上使用相关度得分训练机器学习模型；第二步，在预测到的实体分配上使用相关度得分重新训练模型。</p>
<p>评价相关度：先计算上下文实体（非歧义的实体）的词向量，计算上下文实体向量和目标实体e向量的相似度。为了获得上下文实体的词向量，本文平均他们的词向量：</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image056.jpg" alt="img"></p>
<p>其中<img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image058.gif" alt="img">表示上下文实体集合。<br>为了评价相关度得分，本文重新使用上下文实体<img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image060.gif" alt="img">和实体<img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image054.gif" alt="img">之间的余弦相似度。</p>
<p><strong>3）学会排序</strong></p>
<p>本文采用监督机器学习方法来排序给定指称词m和文档d之后的候选实体集，目的是为了将标准NED特征和上下文信息结合起来。</p>
<p>梯度提升回归树（GBRT）（<a href="http://scikit-learn.org/stable/）：包含一组回归树，给定实例预测相关度得分。代价函数为logistic" target="_blank" rel="noopener">http://scikit-learn.org/stable/）：包含一组回归树，给定实例预测相关度得分。代价函数为logistic</a> loss。主要参数有迭代次数，学习速率和决策树的最大深度。</p>
<h1 id="5-实验"><a href="#5-实验" class="headerlink" title="5. 实验"></a>5. 实验</h1><p><strong>训练词向量的数据：</strong><a href="https://dumps.wikimedia.org/" target="_blank" rel="noopener">https://dumps.wikimedia.org/</a></p>
<p><strong>1）测试实体词向量的质量：</strong></p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image062.jpg" alt="img"></p>
<p><strong>2）命名实体消歧</strong></p>
<p><strong>数据集：CoNLL（Hoffart et al. 2011）、TAC 2010（Ji et al. 2010）。</strong></p>
<p><strong>对比算法：Hoffart et al.、He et al.、Chisholm and Hachey、Pershina et al.</strong></p>
<p><strong>本文算法的结果：</strong></p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image064.jpg" alt="img"></p>
<p><strong>表3是对比实验结果：</strong></p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image066.jpg" alt="img"></p>
<p><strong>表4是本文算法不同特征学习的结果：</strong></p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/clip_image068.jpg" alt="img"></p>
<p><strong>在加入基于本文的词向量模型的上下文特征时，效果大大的提升了。</strong></p>
<p><strong>错误分析：观察到在CoNLL测试集上，大约48.6%的错误率是由于metonymy mentions造成的。当一个流行的错误实体，匹配到指称词，本文的NED算法经常出错，这是由于本文的算法利用KB中的实体流行性数据。</strong></p>
<h1 id="6-工作评价"><a href="#6-工作评价" class="headerlink" title="6. 工作评价"></a>6. 工作评价</h1><p>本文提出了一种词嵌入方法，将文字和实体联合映射到相同的连续向量空间中。本文的方法使本文能够有效地对文本和全局的文本进行建模。此外，在这些背景下，本文的NED方法优于最先进的NED方法。</p>
<p><strong>在未来的工作中，本文打算通过利用关系知识来改进本文的模型，例如在知识图中（例如Freebase的关系。本文也想使本文的嵌入应用到其他的应用领域。</strong></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/06/Zero-Shot应用于跨语言实体链接/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="李洁厅">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hellojet">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/06/Zero-Shot应用于跨语言实体链接/" itemprop="url">Zero-Shot应用于跨语言实体链接</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-06T21:32:01+08:00">
                2018-12-06
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/nlp/" itemprop="url" rel="index">
                    <span itemprop="name">nlp</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Palatucci-M-Pomerleau-D-Hinton-G-E-et-al-Zero-shot-learning-with-semantic-output-codes-C-Advances-in-neural-information-processing-systems-2009-1410-1418"><a href="#Palatucci-M-Pomerleau-D-Hinton-G-E-et-al-Zero-shot-learning-with-semantic-output-codes-C-Advances-in-neural-information-processing-systems-2009-1410-1418" class="headerlink" title="Palatucci M, Pomerleau D, Hinton G E, et al. Zero-shot learning with semantic output codes[C], Advances in neural information processing systems. 2009: 1410-1418."></a>Palatucci M, Pomerleau D, Hinton G E, et al. Zero-shot learning with semantic output codes[C], Advances in neural information processing systems. 2009: 1410-1418.</h1><h2 id="什么是zero-shot-learning"><a href="#什么是zero-shot-learning" class="headerlink" title="什么是zero-shot learning"></a>什么是zero-shot learning</h2><p>机器学习的算法通过学习分类器在很多领域获得了极大的成功。这些分类器被训练近似一个目标函数f: X-&gt;Y，给定标注数据，这些标注数据包括Y的所有可能值。</p>
<p>但是在某些领域，Y可以取很多值，如果要得到包含所有Y类别的标注数据，是非常困难的。因此，<strong>需要分类器自己去学习并得到在标注数据中不存在的Y值</strong>，这就是zero-shot learning。</p>
<h2 id="zero-shot-learning应用于神经活动解码"><a href="#zero-shot-learning应用于神经活动解码" class="headerlink" title="zero-shot learning应用于神经活动解码"></a>zero-shot learning应用于神经活动解码</h2><p>在神经活动解码中，目标是通过观察一个人的神经活动图像来确定这个人正在思考的词或物体。对可能出现的每一个单词进行神经训练图像的采集是一件非常困难的事情，因此，要构建一个实用的神经解码器，必须有一种方法来推断出训练集之外的单词。</p>
<p>这个问题类似于自动语音识别的挑战，在自动语音识别中，需要在分类器训练过程中不显式包含单词的情况下识别它们。为了实现词汇的独立性，语音识别系统通常采用基于音素的识别策略(Waibel, 1989)。音素是构成语言词汇的组成部分。语音识别系统的成功之处是，它利用了一组相对较小的音素识别器，并与将单词表示为音素组合的大型知识库相结合。</p>
<p><strong>为了将类似的方法应用于神经活动解码，必须发现如何从神经活动中推断词义的组成部分</strong>。虽然没有明确的共识,大脑如何编码语义信息(Plaut, 2002)，但是有几个提出的表示方法可以应用于神经活动知识库，从而使神经译码器识别大量可能出现的单词，即使这些词没有在训练集中出现。</p>
<h2 id="使用语义知识进行分类"><a href="#使用语义知识进行分类" class="headerlink" title="使用语义知识进行分类"></a>使用语义知识进行分类</h2><p>使用语义知识来推断新的类别</p>
<p>利用从语义知识库中得到的中间特征集来表示zero-shot learner：将每个类别用一个语义特征向量来表示，这样的向量可以表示大量的类别。那么模型初步就是：input data-&gt;semantic features-&gt;classes。作者需要做的是学习到输入数据和语义特征的关系以及语义特征如何映射到新的单词上。</p>
<h3 id="语义特征空间"><a href="#语义特征空间" class="headerlink" title="语义特征空间"></a>语义特征空间</h3><p>p维的语义特征空间是一个度量空间，其中每个p维对语义属性的值进行编码。这些属性可以通过性质来分类，也可以是实值数据。</p>
<p>比如：</p>
<p>is it furry? does it have a tail? can it breathe underwater? is it carnivorous? is it slow moving?</p>
<p>可以得到一个5维特征向量。输入dog可以得到{1,1,0,1,0}</p>
<h3 id="语义知识库"><a href="#语义知识库" class="headerlink" title="语义知识库"></a>语义知识库</h3><p>知识库K包含M个样本，这些样本表示为<br>$$<br>({f,y})_{1:M}<br>$$<br>其中f是p为语义空间内的一个点，y是集合Y中的一个类别标签。类别标签和语义特征空间一一对应进行编码。</p>
<h3 id="语义输出编码分类器"><a href="#语义输出编码分类器" class="headerlink" title="语义输出编码分类器"></a>语义输出编码分类器</h3><p>一个语义输出编码分类器<br>$$<br>H:X^d-&gt;Y<br>$$<br>从d为输入空间中映射到一个标签，H包含两个过程：<br>$$<br>H = L(S(·))<br>$$</p>
<p>$$<br>S: X^d-&gt;F^p<br>$$</p>
<p>$$<br>L:F^p-&gt;Y<br>$$</p>
<p>zero-shot classifier先将d维输入空间映射到p维语义空间，再映射为一个输出类别。</p>
<p><strong>一些细节</strong>：</p>
<p>学习S时，需要将标注数据从{x,y}转化为{x,f}，f指的是语义特征编码</p>
<p>得到的f及时有一些小偏差，L也可以矫正过来</p>
<p><strong>总结</strong>：</p>
<p>通过使用丰富的类别语义编码，分类器可以推断和识别新的类别。</p>
<h2 id="模型设计"><a href="#模型设计" class="headerlink" title="模型设计"></a>模型设计</h2><ul>
<li>使用多输出线性回归来学习语义输出码分类器的S(·)映射</li>
<li>在训练过程中，使用voxel-stability-criterion来将d从20,000降低500</li>
<li>L(·)使用1近邻分类器</li>
</ul>
<h1 id="Rijhwani-S-Xie-J-Neubig-G-et-al-Zero-shot-Neural-Transfer-for-Cross-lingual-Entity-Linking-J-arXiv-preprint-arXiv-1811-04154-2018"><a href="#Rijhwani-S-Xie-J-Neubig-G-et-al-Zero-shot-Neural-Transfer-for-Cross-lingual-Entity-Linking-J-arXiv-preprint-arXiv-1811-04154-2018" class="headerlink" title="Rijhwani S, Xie J, Neubig G, et al. Zero-shot Neural Transfer for Cross-lingual Entity Linking[J]. arXiv preprint arXiv:1811.04154, 2018."></a>Rijhwani S, Xie J, Neubig G, et al. Zero-shot Neural Transfer for Cross-lingual Entity Linking[J]. arXiv preprint arXiv:1811.04154, 2018.</h1><h2 id="跨语言实体链接"><a href="#跨语言实体链接" class="headerlink" title="跨语言实体链接"></a>跨语言实体链接</h2><p>跨语言实体链接将源语言中提到的实体映射到另一种(目标)语言的结构化知识库中的相应条目。</p>
<p>以往的研究主要依靠双语词汇资源来弥补源语言和目标语言之间的差距，但是对于许多低资源语言来说，这些资源是稀缺的或不可用的。</p>
<p>举个例子，马拉地语(“波兰”)作为输入实体，链接到英语知识库(KB)中的相应条目。</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1544081173449.png" alt="1544081173449"></p>
<h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><p>在单语言中，字符串相似度和Wikipedia锚文本可以很好的用来进行实体链接。但在跨语言中，这行不通，因为跨语言的实体大部分是不相似的。</p>
<p>对于跨语言实体链接任务，(Tsai and Roth 2016; Pan et al. 2017; Tsai and Roth 2018)使用双语资源来弥补这一差距，包括词汇和维基百科的跨语言链接。然而,世界上绝大多数的语言资源缺乏。为了这种低资源语言(LRLs)也可以进行跨语言实体链接，必须设计不过多依赖LRLs中的词汇或其他资源的方法。</p>
<p>自2011年以来，tac-kbp在实体链接方面的任务一直以中文/西班牙语链接到英语为特色(Ji, Grishman, and Dang 2011; Ji, Nothman, and Hachey 2014; Ji et al. 2017)。</p>
<p>McNamee et al. (2011) 介绍了跨语言实体链接任务，并设计了一种基于Wikipedia语言链接的候选实体检索技术。</p>
<p>Tsai and Roth (2016) 在EL上对12个语言使用词嵌入。</p>
<p>Pan et al. (2017)在282对语言对的EL中，使用逐字翻译进行大量的多语言链接。</p>
<p>Tsai and Roth (2018)提出更好的名称翻译（name translation），以提高现有基于翻译的EL技术的性能。</p>
<p><strong>Sil et al. (2017)提出的神经模型基于多语言词嵌入和维基百科链接，在TAC2015数据集上实现了SOTA的结果。</strong></p>
<h2 id="本文工作"><a href="#本文工作" class="headerlink" title="本文工作"></a>本文工作</h2><p>本文提出了pivot-based entity linking(PBEL)：基于低资源语言有紧密相关的高资源语言（HRLs）这一直觉。</p>
<p>比如马拉地语和老挝语资源相对较少，但与资源丰富的北印度语和泰语属于同一语系。</p>
<p>本文利用这些HRLs中提供的双语词汇和结构化信息来改进LRLs的实体链接性能。</p>
<h3 id="算法创新"><a href="#算法创新" class="headerlink" title="算法创新"></a>算法创新</h3><p><strong>Zero-shot transfer of neural entity linking models：</strong></p>
<p>使用一个高资源语言HRL和英语之间的双语词典，训练一个将HRL中的实体链接到英语知识库中的字符级别的神经模型。这个模型可以迁移到为LRL执行实体链接，不需要有任务具体的语言微调。</p>
<p>这种迁移学习方案在密切相关的语言之间使用时，已经成功地应用于其他任务，如形态标记和机器翻译(Zoph et al. 2016;Cotterell and Heigold 2017)。</p>
<p><strong>Pivoting：</strong></p>
<p>实体并不是直接从一个LRL链接到英语，而是先链接到一个密切相关的HRL。然后，使用HRL中现成的双语词汇来获得相应的英语实体链接。</p>
<p>即使用HRL作为LRL和英语之间的一个中间轴。</p>
<p><strong>The use of phonological representations for cross-lingual EL：</strong></p>
<p>当HRL和LRL不使用同样的书写系统的时候，字符级别的模型迁移将会失败。</p>
<p>使用国际音标表（IPA）来建立不同语言之间的联系，如前面的图中”Poland”在马拉地语和印度语中的发音是非常相似的。</p>
<p>本文提出两种表示：</p>
<ul>
<li>Phoneme embeddings音素嵌入</li>
</ul>
<p>使用Epitran将HRL和英语之间所有相对应的训练数据转换为IPA, Epitran是一个图形-音素系统，支持超过55种语言(Mortensen, Dalmia, and Littell 2018)。</p>
<ul>
<li>Articulatory feature embeddings发音特征嵌入</li>
</ul>
<p>使用PanPhone转化IPA训练数据为发音特征(Mortensen et al. 2016)。发音特征可以潜在地捕获在IPA中可能不明显的发音的重要特征，正如改进低资源命名实体识别(Mortensen et al. 2016)所展示的。</p>
<h3 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h3><p><strong>1. 实体相似编码器：训练两个神经编码器HRL encoder和English encoder使得HRL和English中对应的向量表示相似</strong></p>
<p>使用字符级双向LSTM(Bi-LSTM)将实体编码到一个连续的向量空间。</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1544086794670.png" alt="1544086794670"></p>
<p>这个模型的训练目标是最大化HRL和English中对等实体的余弦相似度(很像Mitra and Craswell 2017在神经信息精索方面的工作)。</p>
<p>每个实体都是字符的序列，下面是HRL和English中两个对等的实体：<br>$$<br>e_{HRL} = &lt;c_1,c_2,…,c_M&gt;<br>$$</p>
<p>$$<br>e_{en}=&lt;k_1,k_2,…,k_N&gt;<br>$$</p>
<p>对每个字符，可以得到一个固定大小的字符嵌入，然后将其输入双向LSTM中，最后的状态组成编码实体向量：<br>$$<br>V_{HRL}=HRL-Bi-LSTM(&lt;c_1,c_2,…,c_M&gt;)<br>$$</p>
<p>$$<br>V_{en}=English-Bi_LSTM(&lt;k_1,k_2,…,k_N&gt;)<br>$$</p>
<p>相似度计算：<br>$$<br>sim(e_{HRL},e_{en})=cosine(V_{HRL},V_{en})<br>$$<br>本文使用(Collobert et al. 2011)中已有的工作：使用最大边际损失的负采样来训练编码器，为了更有效的训练一个可以针对给定mention排序KB实体的模型。</p>
<p>损失函数：<br>$$<br>L=max(0,sim(e_{HRL},e_{en})-sim(e_{HRL},e^*_{en})+\lambda)<br>$$<br><strong>2. Pivoting for Candidate Generation：候选实体生成</strong></p>
<p><strong>3. Zero-shot Transfer to LRL：使用HRL encoder对mention进行编码，使用English encoder对英语实体进行编码，计算两者之间的相似度</strong></p>
<p>如果使用与LRL足够相似的HRL来训练模型，那么实体编码器就可以有效地预测mention和英语知识库实体之间的相似性。本文使用HRL-Bi-LSTM来编码mention，使用English-Bi-LSTM来编码英语知识库实体，然后求相似度：<br>$$<br>sim(m,e_{en})=cosine(V_m,V_{en})<br>$$<br><strong>4. Pivoting：使用HRL encoder分别对mention和HRL实体进行编码，计算两者之间的相似度</strong></p>
<p>使用HRL作为LRL和英语之间的中间枢轴，考虑英语实体在HRL中相对应的实体，求相似度：<br>$$<br>sim(m,e_{HRL})=cosine(V_m,V_{HRL})<br>$$<br><strong>5. 取较大值最为mention和英语实体之间的相似度</strong><br>$$<br>score(m,e_{en})=max(sim(m,e_{en}),sim(m,e_{HRL}))<br>$$</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p><strong>两个实验任务：</strong></p>
<ul>
<li>跨语言知识库标题链接</li>
</ul>
<p>测试集：从Wikipedia中LRL和English相对应的title中得到。</p>
<p>测试九个LRL，来自不同的语言家族：Tigrinya (ti), Lao (lo), Uyghur (ug), Telugu (te), Punjabi (pa), Javanese (jv), Marathi (mr), Bengali (bn) and Ukrainian (uk)</p>
<p>使用53个HRLs作为潜在迁移语言。</p>
<ul>
<li>跨语言全链接</li>
</ul>
<p>测试集：使用来自DARPA LORELEI程序的标注文档（<a href="https://www.darpa.mil/program/low-resource-languages-for-emergent-incidents），两个极低资源的语言-Tigrinya和Oromo。" target="_blank" rel="noopener">https://www.darpa.mil/program/low-resource-languages-for-emergent-incidents），两个极低资源的语言-Tigrinya和Oromo。</a></p>
<p><strong>实体相似度评分模型：</strong></p>
<p>考虑三个模型用来进行跨语言实体链接评分：其中两个是基于SOTA的单语言或跨语言实体链接方法的文献(Ji and Grishman 2011; Pan et al. 2017; Sil et al. 2017)，因为需要具有LRL和英语的双语词典，所以不适合我们的zero-shot；第三个是字符级别的神经解码器。</p>
<ul>
<li>EXACT：(Sil et al. 2017)精确匹配知识库用于SOTA单语EL系统。</li>
<li>TRANS：该基线是Pan et al.(2017)在SOTA低资源EL系统中使用的候选检索技术，该技术试图将mention翻译成英语，以预测实体链接。</li>
<li>ENCODE：训练一个相似度编码器(Neubig et al. 2017)，使用English和HRL之间相对应的Wikipedia标题。然后使用HRL-Bi-LSTM来迁移编码mention。</li>
</ul>
<p><strong>结果：</strong></p>
<ul>
<li>跨语言知识库标题链接</li>
</ul>
<p>下面是不同模型带来的Accuracy结果，ENCODE用了两种模型，一种是MANUAL，即手动选择HRL，另一种是BEST-53，从53个HRL个挑选结果最好的作为和LRL同一语系。而PBEL（本文算法）还采用了一种模型MULTI，对单个LRL，使用53个轴语言，这些语言进行非加权或者系统距离加权结合。</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1544100032867.png" alt="1544100032867"></p>
<ul>
<li>跨语言全链接</li>
</ul>
<p>挑选合适的HRL来训练ENCODE和PBEL：Amharic for Tigrinya and Somali for Oromo.</p>
<p>训练数据是HRL和English之间的相对应实体。</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1544100045952.png" alt="1544100045952"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文工作有潜力通过语言家族来进行低资源实体链接。</p>
<p>本文使用了zero-shot迁移，在baseline上提升了17%的效果。</p>
<p>未来工作在于训练对于一个给定的命名实体mention，来预测最好的轴语言，可以替代文中所用到的系统距离加权。</p>
<p>还有个不足之处在于对每个语言训练一个编码器。通用多语言编码器在翻译等领域已经取得了成功(Johnson et al. 2016; Ha, Niehues, andWaibel 2016) and，预计可以提升本文的模型适应大量语言。</p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/05/《CS224n-Natural-Language-Processing-with-Deep-Learning》Assignments1-理论推导部分/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="李洁厅">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hellojet">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/05/《CS224n-Natural-Language-Processing-with-Deep-Learning》Assignments1-理论推导部分/" itemprop="url">《CS224n: Natural Language Processing with Deep Learning》Assignments1: 理论推导部分</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-05T10:49:08+08:00">
                2018-12-05
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/nlp/" itemprop="url" rel="index">
                    <span itemprop="name">nlp</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Assignment-1"><a href="#Assignment-1" class="headerlink" title="Assignment #1"></a>Assignment #1</h1><h2 id="1-Softmax"><a href="#1-Softmax" class="headerlink" title="1 Softmax"></a>1 Softmax</h2><h3 id="a"><a href="#a" class="headerlink" title="(a)"></a>(a)</h3><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1543996643888.png" alt="1543996643888"></p>
<h2 id="2-Neural-Network-Basics"><a href="#2-Neural-Network-Basics" class="headerlink" title="2 Neural Network Basics"></a>2 Neural Network Basics</h2><h3 id="a-1"><a href="#a-1" class="headerlink" title="(a)"></a>(a)</h3><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1543996660008.png" alt="1543996660008"></p>
<h3 id="b"><a href="#b" class="headerlink" title="(b)"></a>(b)</h3><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1543996682477.png" alt="1543996682477"></p>
<h3 id="c"><a href="#c" class="headerlink" title="(c)"></a>(c)</h3><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1543996692851.png" alt="1543996692851"></p>
<h3 id="d"><a href="#d" class="headerlink" title="(d)"></a>(d)</h3><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1543996699758.png" alt="1543996699758"></p>
<h2 id="3-word2vec"><a href="#3-word2vec" class="headerlink" title="3 word2vec"></a>3 word2vec</h2><h3 id="a-2"><a href="#a-2" class="headerlink" title="(a)"></a>(a)</h3><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1543996771099.png" alt="1543996771099"></p>
<h3 id="b-1"><a href="#b-1" class="headerlink" title="(b)"></a>(b)</h3><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1543996782774.png" alt="1543996782774"></p>
<h3 id="c-1"><a href="#c-1" class="headerlink" title="(c)"></a>(c)</h3><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1543996789539.png" alt="1543996789539"></p>
<h3 id="d-1"><a href="#d-1" class="headerlink" title="(d)"></a>(d)</h3><p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/1543996801191.png" alt="1543996801191"></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/03/论文阅读：Neural Cross-Lingual Entity Linking/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="李洁厅">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hellojet">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/12/03/论文阅读：Neural Cross-Lingual Entity Linking/" itemprop="url">论文阅读：Neural Cross-Lingual Entity Linking</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-03T11:21:05+08:00">
                2018-12-03
              </time>
            

            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/nlp/" itemprop="url" rel="index">
                    <span itemprop="name">nlp</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="1-阅读文献来源"><a href="#1-阅读文献来源" class="headerlink" title="1. 阅读文献来源"></a>1. 阅读文献来源</h1><p> Sil A , Kundu G , Florian R , et al. Neural Cross-Lingual Entity Linking[J]. 2017.</p>
<p>Avirup Sil : Research Scientist &amp; NLP Chair at IBM Research AI</p>
<p>这篇论文在CoNLL（YAGO）和TAC2010上的结果非常好，在CoNLL上与deeptype不相上下，在TAC2010上结果稍差。</p>
<h1 id="2-论文解决的科学问题及研究动机"><a href="#2-论文解决的科学问题及研究动机" class="headerlink" title="2. 论文解决的科学问题及研究动机"></a>2. 论文解决的科学问题及研究动机</h1><p><strong>本文的introduction主要讨论了细粒度的相似度的问题并给出解决方案</strong>：</p>
<p>对于实体链接的一些不明确情况，需要计算mention的上下文和候选实体的标题页之间的细粒度的相似性。</p>
<p>考虑以下例子:</p>
<p>e1: Alexander Douglas Smith is an American football quar- terback for the Kansas City Chiefs of the National Football League (NFL). </p>
<p>e2: Edwin Alexander “Alex” Smith is an American football tight end who was drafted by the Tampa Bay Buccaneers in the third round of the 2005 NFL Draft.</p>
<p>e3: Alexander Smith was a Scottish-American professional golfer who played in the late 19th and early 20th century. </p>
<p>q: Last year, while not one of the NFL’s very best quarter- backs, Alex Smith did lead the team to a strong 12-4 season.</p>
<p>e3中的Alexander Smith是一名golfer，显然与q中的Alexander Smith不同，这个比较好区分；但是对于e1和e2中的Alexander Smith，都提到了American football players，甚至提到了关键词：NEL。这个就比较难区分了。需要进行细粒度的相似度计算。</p>
<p>本文提出训练最先进的(SOTA)相似模型，用于计算mention的上下文和维基百科的消歧候选页面之间的相似度，以期正确地解决上述那些含糊的情况。出于这个目的，<strong>作者抽取了不同粒度水平的信息(针对entity coreference chain和surrounding mentions)</strong>，使用了以下方法：a combination of convolutional neural networks (CNN), LSTMs (Hochreiter and Schmidhuber 1997), Lexical Composition and Decomposition (Wang, Mi, and Ittycheriah 2016), Multi-Perspective Context Matching (MPCM) (Wang et al. 2016), and Neural Tensor Networks (Socher et al. 2013a; 2013c) 来编码这些信息并进行实体链接。</p>
<p>TAC社区对<strong>跨语言EL</strong>也非常感兴趣（Tsai and Roth 2016; Sil and Florian 2016）：如在西班牙语或汉语等外语文献中的mention，人们需要在英语维基百科中找到相应的链接。限制条件：we have extremely limited (or possibly even no) linguis- tic resources and no machine translation technology。</p>
<p>作者提到了该领域之前的一个工作：Tsai and Roth 2016提出了一种使用多语言嵌入的cross-lingual wikifier。然而，他们的模型需要针对每种新语言进行重新训练，因此并不完全适合TAC任务。</p>
<p><strong>作者提出了一种zero shot learning technique</strong>(Palatucci et al. 2009: Socher et al. 2013b) for their neural EL model：一旦使用english训练，就可以应用于跨语言的EL，而无需再训练。</p>
<h1 id="3-算法框架"><a href="#3-算法框架" class="headerlink" title="3. 算法框架"></a>3. 算法框架</h1><h2 id="1-Fast-Match-Search生成候选实体"><a href="#1-Fast-Match-Search生成候选实体" class="headerlink" title="1. Fast Match Search生成候选实体"></a>1. Fast Match Search生成候选实体</h2><p>快速匹配搜索的目标是生成候选实体。过程其实很简单：提取链接锚文本映射到其目标维基百科标题的信息。为了对mention生成对应的候选实体，作者只需要检索上面提取到的anchor-title就可以，<strong>候选实体集合被认为是锚文本m最频繁链接的实体集</strong>。比如泰坦尼克最频繁链接到的实体是：电影和船，那么下次碰到泰坦尼克时，他的候选实体就是泰坦尼克（电影）和泰坦尼克（船)。</p>
<p>另外，作者从相关目标语言的wikipedia页面中提取anchor-title信息，然后得到非英语链接回英语wiki的候选实体信息。</p>
<h2 id="2-词嵌入"><a href="#2-词嵌入" class="headerlink" title="2. 词嵌入"></a>2. 词嵌入</h2><p>分为<strong>单语言的词嵌入</strong>和<strong>跨语言的词嵌入</strong>。</p>
<ul>
<li><p>单语言的词嵌入：</p>
<pre><code>the widely used CBOW word2vec model (Mikolov et al. 2013)
</code></pre></li>
<li><p>多语言的词嵌入：</p>
<ul>
<li><p>Canonical Correlation Analysis 典型相关分析 (CCA)</p>
<p>这种技术基于Faruqui and Dyer 2014， 首先在不同语言的文本上执行SVD，然后在平行语料中对齐的单词向量对上执行CCA。</p>
</li>
<li><p>MultiCCA</p>
<p>(Ammar et al. 2016) ：基于CCA，使用线性算子将每种语言（除了英语）的预训练单语言词嵌入投射到预训练英语词嵌入。</p>
</li>
<li><p>Weighted Least Squares 加权最小平方 (LS)</p>
<p>(Mikolov, Le, and Sutskever 2013)：其他语言的词嵌入直接投射到英语上，使用多变量回归得到的映射。</p>
</li>
</ul>
</li>
<li><p>wikipedia页面嵌入</p>
</li>
</ul>
<p>对wikipedia的整个页面（pages or called links）进行嵌入</p>
<p>第一步，<strong>计算维基百科页面中所有词的加权平均值</strong>。使用每个单词的idf作为他向量的权重，来减少频繁词的影响。计算公式如下：</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/typora_images/1543489755502.png" alt="1543489755502"></p>
<p>其中 <img src="http://chart.googleapis.com/chart?cht=tx&chl= e_w" style="border:none;"> 是词向量，<img src="http://chart.googleapis.com/chart?cht=tx&chl= idf_w" style="border:none;"> 是单词w的逆文档频率IDF。</p>
<p>第二步：在上面得到的词嵌入上训练一个全连接<em>tanh</em>激活层，目的是将mention上下文和Wikipedia页面放入相似空间中。</p>
<h2 id="3-对上下文建模"><a href="#3-对上下文建模" class="headerlink" title="3. 对上下文建模"></a>3. 对上下文建模</h2><p>对文档D中的mention m的表示进行编码。编码后的表示（representation）将会与wiki页面向量进行比较（通过余弦相似度），比较结果放入更高层的网络。</p>
<p>notice：文档D对m的消歧不都有用，作者选择m附近的的句子来表示m。</p>
<p>编码m的过程：</p>
<p>作者先执行一个共指消解系统(Luo et al. 2004) ，然后建立m<strong>基于句子上下文的表示</strong>，以及关于mention出现的窗口内的单词的<strong>细粒度的上下文编码</strong>。</p>
<ul>
<li>对句子进行建模<ol>
<li>收集包含mention或者实体共指链的所有句子</li>
<li>结合这些句子形成包含mention所有实例的句子序列</li>
<li>使用CNN从这些变长句子上产生固定大小的向量<ol>
<li>先将每个单词嵌入d维向量空间，使用前面说的嵌入技术</li>
<li>然后使用CNN将这些单词映射为一个固定长度的向量</li>
</ol>
</li>
<li>应用<em>tanh</em>非线性层，并用mean-pooling将结果进行聚合</li>
<li>对Wikipedia页面的第一段也应用CNN来进行编码，编码结果视为该实体页面的编码。（与使用CNN编码整个Wikipedia页面不同）</li>
</ol>
</li>
<li>细粒度上下文建模</li>
</ul>
<p><strong>上面那种句子建模会忽视某些相关模式</strong>（introduction中举的那个例子就是），没有利用好mention附近的words，这些words是mention意义的强烈指示。作者将窗口大小为n（n=4）的单词作为mention的上下文：</p>
<p>在这些窗口上使用LSTMs：正向对左边窗口进行，反向对右边窗口进行(Cheng, Dong, and Lapata 2016)；使用mean-pooling作为融合策略。这些压缩后的表示将会被平均，然后使用神经张量网络进行结合，使用如下公式：</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/typora_images/1543563100460.png" alt="1543563100460"></p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/typora_images/1543563130211.png" alt="1543563130211"></p>
<p>l和r分别是左右上下文的总体向量，W是k切片的张量，f是非线性激活函数（这里是sigmoid）。NTN的输出是一个向量。</p>
<h2 id="4-跨语言神经实体链接神经模型架构"><a href="#4-跨语言神经实体链接神经模型架构" class="headerlink" title="4. 跨语言神经实体链接神经模型架构"></a>4. 跨语言神经实体链接神经模型架构</h2><p>目标：perform “zero shot learning” (Socher et al. 2013b; Palatucci et al. 2009) for cross-lingual EL</p>
<p>方法：在英语数据上训练一个模型，使用这个模型来对其他语言进行解码，假设有英语和目标语言的跨语言词嵌入。</p>
<p>模型：mention上下文和候选实体wiki页面的几种相似度作为输入，放入一个前向神经层H，权重<img src="http://chart.googleapis.com/chart?cht=tx&chl= W_h" style="border:none;">，偏置<img src="http://chart.googleapis.com/chart?cht=tx&chl= b_h" style="border:none;">，和一个sigmoid非线性激活函数。H的输出h通过公式来计算：</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/typora_images/1543564299277.png" alt="1543564299277"></p>
<p>二分类器P(C|m,D,l)表示C=1（正确链接），C=0（不正确链接）的概率：</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/typora_images/1543564386728.png" alt="1543564386728"></p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/typora_images/1543559723954.png" alt="1543559723954"></p>
<p><strong>1. 特征提取层</strong></p>
<p>这层对相似度进行编码</p>
<ul>
<li><strong>相似性特征（比较上下文表示）</strong><ul>
<li>”句子上下文-wiki页面“ 的相似度</li>
<li>“句子山下文-wiki第一段” 的相似度</li>
<li>“细粒度上下文-wiki页面” 的相似度</li>
<li>语言内部特征：LIEL系统(Sil and Florian 2016)里描述的loca features</li>
</ul>
</li>
</ul>
<p><strong>2. 多角度装箱层</strong></p>
<p>用多个高斯径向基函数作为输入，进行平滑装箱到向量中（灵感来自(Liu et al. 2016)）。上面的相似度特征输入到该层，将这些特征映射到高维向量。与(Liu et al. 2016)不同的是可以自动学习输入数值重要的部分。</p>
<ul>
<li><p><strong>语义相似和不相似</strong></p>
<ul>
<li>词汇分解和组合（LDC）</li>
</ul>
</li>
</ul>
<ul>
<li>多角度上下文匹配（MPCM）</li>
</ul>
<p><strong>3. 训练和解码</strong></p>
<p>二分类训练集的设置：使用fast match strategy生成mention对应的正确链接和不正确链接实体页面。</p>
<p>训练过程使用随机梯度下降</p>
<p>解码就是比较候选实体链接正确的概率。</p>
<h1 id="4-实验"><a href="#4-实验" class="headerlink" title="4. 实验"></a>4. 实验</h1><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><ul>
<li>English (CoNLL &amp; TAC): <ul>
<li>The CoNLL dataset (Hoffart et al. 2011) </li>
<li>TAC 2010 source</li>
</ul>
</li>
<li>Cross-Lingual (TAC)：<ul>
<li>TAC 2015 Tri-Lingual Entity Linking datasets</li>
</ul>
</li>
</ul>
<p>For the CoNLL experiments, in addition to the Wikipedia anchor-title index, we also use a alias-entity map- ping previously used by (Pershina, He, and Grishman 2015; Globerson et al. 2016; Yamada et al. 2016). We also use the mappings provided by (Hoffart et al. 2011) obtained by ex- tending the “means” tables of YAGO (Hoffart et al. 2013).</p>
<h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p>英语EL：</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/typora_images/1543565503898.png" alt="1543565503898"></p>
<p>跨语言EL：</p>
<p><img src="https://hellojet-blog-1251889946.cos.ap-shanghai.myqcloud.com/typora_images/1543565543687.png" alt="1543565543687"></p>
<h1 id="5-作者工作评价"><a href="#5-作者工作评价" class="headerlink" title="5. 作者工作评价"></a>5. 作者工作评价</h1><p>其他研究在单语言或跨语言的表现上都实现了SOTA，但不是都（而本文的结果是都好）。</p>
<p>作者提出的模型可以仅在英语上训练，就不需要其他训练就可以应用于其他语言，只需要有跨语言的词嵌入。</p>
<p>作者有效利用了相似性模型（LDC，MPCM），和神经张量网络，来捕捉mention和wiki页面的相似与不相似。</p>
<p>zero-shot learning在跨语言EM上的应用，对低资源的语言很有用。</p>
<h1 id="6-自己对该论文的评价"><a href="#6-自己对该论文的评价" class="headerlink" title="6. 自己对该论文的评价"></a>6. 自己对该论文的评价</h1><h2 id="算法创新点"><a href="#算法创新点" class="headerlink" title="算法创新点"></a>算法创新点</h2><ol>
<li>在候选实体的生成过程中，仅靠mention的字符串匹配的效率是很低的。（这个之前在实验中已经得到验证，TAC KBP的mention通过EL索引查找wiki中的候选实体的召回率只有50%上下）本文提出的快速匹配利用wiki页面中的锚文本数据，具有可信性和高召回率。</li>
<li>细粒度的相似度的提出：作者不但利用包含mention的句子和wiki页面的相似度，还加入了细粒度的相似度，这几种相似度作为神经网络的输入，避免了句子中不相关单词对mention消歧的影响。</li>
<li>将zero-shot learning应用到跨语言实体链接中，有效的解决了TAC竞赛给出的语言资料少的缺点，对比较少人用的语言链接到英语wiki也具有现实意义。</li>
</ol>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/11/30/转载-自然语言处理如何入门？——周明博士/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="李洁厅">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hellojet">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/30/转载-自然语言处理如何入门？——周明博士/" itemprop="url">[转载]自然语言处理如何入门？————周明博士</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-30T14:43:05+08:00">
                2018-11-30
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>作者：微软亚洲研究院<br>链接：<a href="https://www.zhihu.com/question/19895141/answer/149475410" target="_blank" rel="noopener">https://www.zhihu.com/question/19895141/answer/149475410</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
<p><strong>自然语言处理</strong>（简称NLP），是研究计算机处理人类语言的一门技术，包括：<br><strong>1.句法语义分析</strong>：对于给定的句子，进行分词、词性标记、命名实体识别和链接、句法分析、语义角色识别和多义词消歧。</p>
<p><strong>2.信息抽取</strong>：从给定文本中抽取重要的信息，比如，时间、地点、人物、事件、原因、结果、数字、日期、货币、专有名词等等。通俗说来，就是要了解谁在什么时候、什么原因、对谁、做了什么事、有什么结果。涉及到实体识别、时间抽取、因果关系抽取等关键技术。</p>
<p><strong>3.文本挖掘（或者文本数据挖掘）</strong>：包括文本聚类、分类、信息抽取、摘要、情感分析以及对挖掘的信息和知识的可视化、交互式的表达界面。目前主流的技术都是基于统计机器学习的。</p>
<p><strong>4.机器翻译</strong>：把输入的源语言文本通过自动翻译获得另外一种语言的文本。根据输入媒介不同，可以细分为文本翻译、语音翻译、手语翻译、图形翻译等。机器翻译从最早的基于规则的方法到二十年前的基于统计的方法，再到今天的基于神经网络（编码-解码）的方法，逐渐形成了一套比较严谨的方法体系。</p>
<p><strong>5.信息检索</strong>：对大规模的文档进行索引。可简单对文档中的词汇，赋之以不同的权重来建立索引，也可利用1，2，3的技术来建立更加深层的索引。在查询的时候，对输入的查询表达式比如一个检索词或者一个句子进行分析，然后在索引里面查找匹配的候选文档，再根据一个排序机制把候选文档排序，最后输出排序得分最高的文档。</p>
<p><strong>6.问答系统</strong>： 对一个自然语言表达的问题，由问答系统给出一个精准的答案。需要对自然语言查询语句进行某种程度的语义分析，包括实体链接、关系识别，形成逻辑表达式，然后到知识库中查找可能的候选答案并通过一个排序机制找出最佳的答案。</p>
<p><strong>7.对话系统</strong>：系统通过一系列的对话，跟用户进行聊天、回答、完成某一项任务。涉及到用户意图理解、通用聊天引擎、问答引擎、对话管理等技术。此外，为了体现上下文相关，要具备多轮对话能力。同时，为了体现个性化，要开发用户画像以及基于用户画像的个性化回复。</p>
<p>随着深度学习在图像识别、语音识别领域的大放异彩，人们对深度学习在NLP的价值也寄予厚望。再加上AlphaGo的成功，人工智能的研究和应用变得炙手可热。自然语言处理作为人工智能领域的认知智能，成为目前大家关注的焦点。很多研究生都在进入自然语言领域，寄望未来在人工智能方向大展身手。但是，大家常常遇到一些问题。俗话说，万事开头难。如果第一件事情成功了，学生就能建立信心，找到窍门，今后越做越好。否则，也可能就灰心丧气，甚至离开这个领域。这里针对给出我个人的建议，希望我的这些粗浅观点能够引起大家更深层次的讨论。</p>
<p><strong>建议1：如何在NLP领域快速学会第一个技能？</strong></p>
<p>我的建议是：找到一个开源项目，比如机器翻译或者深度学习的项目。理解开源项目的任务，编译通过该项目发布的示范程序，得到与项目示范程序一致的结果。然后再深入理解开源项目示范程序的算法。自己编程实现一下这个示范程序的算法。再按照项目提供的标准测试集测试自己实现的程序。如果输出的结果与项目中出现的结果不一致，就要仔细查验自己的程序，反复修改，直到结果与示范程序基本一致。如果还是不行，就大胆给项目的作者写信请教。在此基础上，再看看自己能否进一步完善算法或者实现，取得比示范程序更好的结果。</p>
<p><strong>建议2：如何选择第一个好题目？</strong></p>
<p>工程型研究生，选题很多都是老师给定的。需要采取比较实用的方法，扎扎实实地动手实现。可能不需要多少理论创新，但是需要较强的实现能力和综合创新能力。而学术型研究生需要取得一流的研究成果，因此选题需要有一定的创新。我这里给出如下的几点建议。</p>
<ul>
<li>先找到自己喜欢的研究领域。你找到一本最近的ACL会议论文集, 从中找到一个你比较喜欢的领域。在选题的时候，多注意选择蓝海的领域。这是因为蓝海的领域，相对比较新，容易出成果。</li>
<li>充分调研这个领域目前的发展状况。包括如下几个方面的调研：方法方面，是否有一套比较清晰的数学体系和机器学习体系；数据方面，有没有一个大家公认的标准训练集和测试集；研究团队，是否有著名团队和人士参加。如果以上几个方面的调研结论不是太清晰，作为初学者可能不要轻易进入。</li>
<li>在确认进入一个领域之后，按照建议一所述，需要找到本领域的开源项目或者工具，仔细研究一遍现有的主要流派和方法，先入门。</li>
<li>反复阅读本领域最新发表的文章，多阅读本领域牛人发表的文章。在深入了解已有工作的基础上，探讨还有没有一些地方可以推翻、改进、综合、迁移。注意做实验的时候，不要贪多，每次实验只需要验证一个想法。每次实验之后，必须要进行分析存在的错误，找出原因。</li>
<li>对成功的实验，进一步探讨如何改进算法。注意实验数据必须是业界公认的数据。</li>
<li>与已有的算法进行比较，体会能够得出比较一般性的结论。如果有，则去写一篇文章，否则，应该换一个新的选题。</li>
</ul>
<p><strong>建议3：如何写出第一篇论文？</strong></p>
<ul>
<li>接上一个问题，如果想法不错，且被实验所证明，就可开始写第一篇论文了。</li>
<li>确定论文的题目。在定题目的时候，一般不要“…系统”、“…研究与实践”，要避免太长的题目，因为不好体现要点。题目要具体，有深度，突出算法。</li>
<li>写论文摘要。要突出本文针对什么重要问题，提出了什么方法，跟已有工作相比，具有什么优势。实验结果表明，达到了什么水准，解决了什么问题。</li>
<li>写引言。首先讲出本项工作的背景，这个问题的定义，它具有什么重要性。然后介绍对这个问题，现有的方法是什么，有什么优点。但是（注意但是）现有的方法仍然有很多缺陷或者挑战。比如（注意比如），有什么问题。本文针对这个问题，受什么方法（谁的工作）之启发，提出了什么新的方法并做了如下几个方面的研究。然后对每个方面分门别类加以叙述，最后说明实验的结论。再说本文有几条贡献，一般写三条足矣。然后说说文章的章节组织，以及本文的重点。有的时候东西太多，篇幅有限，只能介绍最重要的部分，不需要面面俱到。</li>
<li>相关工作。对相关工作做一个梳理，按照流派划分，对主要的最多三个流派做一个简单介绍。介绍其原理，然后说明其局限性。</li>
<li>然后可设立两个章节介绍自己的工作。第一个章节是算法描述。包括问题定义，数学符号，算法描述。文章的主要公式基本都在这里。有时候要给出简明的推导过程。如果借鉴了别人的理论和算法，要给出清晰的引文信息。在此基础上，由于一般是基于机器学习或者深度学习的方法，要介绍你的模型训练方法和解码方法。第二章就是实验环节。一般要给出实验的目的，要检验什么，实验的方法，数据从哪里来，多大规模。最好数据是用公开评测数据，便于别人重复你的工作。然后对每个实验给出所需的技术参数，并报告实验结果。同时为了与已有工作比较，需要引用已有工作的结果，必要的时候需要重现重要的工作并报告结果。用实验数据说话，说明你比人家的方法要好。要对实验结果好好分析你的工作与别人的工作的不同及各自利弊，并说明其原因。对于目前尚不太好的地方，要分析问题之所在，并将其列为未来的工作。</li>
<li>结论。对本文的贡献再一次总结。既要从理论、方法上加以总结和提炼，也要说明在实验上的贡献和结论。所做的结论，要让读者感到信服，同时指出未来的研究方向。</li>
<li>参考文献。给出所有重要相关工作的论文。记住，漏掉了一篇重要的参考文献（或者牛人的工作），基本上就没有被录取的希望了。</li>
<li>写完第一稿，然后就是再改三遍。</li>
<li>把文章交给同一个项目组的人士，请他们从算法新颖度、创新性和实验规模和结论方面，以挑剔的眼光，审核你的文章。自己针对薄弱环节，进一步改进，重点加强算法深度和工作创新性。</li>
<li>然后请不同项目组的人士审阅。如果他们看不明白，说明文章的可读性不够。你需要修改篇章结构、进行文字润色，增加文章可读性。</li>
<li>如投ACL等国际会议，最好再请英文专业或者母语人士提炼文字。</li>
</ul>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal" itemscope="" itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/11/29/hello-world/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="name" content="李洁厅">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="hellojet">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2018/11/29/hello-world/" itemprop="url">Hello World</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-11-29T12:23:55+08:00">
                2018-11-29
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/images/avatar.jpg" alt="李洁厅">
            
              <p class="site-author-name" itemprop="name">李洁厅</p>
              <p class="site-description motion-element" itemprop="description">余生很长 但求无憾 以梦为马 不负韶华</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">6</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">分类</span>
                
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                
                  <span class="site-state-item-count">5</span>
                  <span class="site-state-item-name">标签</span>
                
              </div>
            

          </nav>

          
            <div class="feed-link motion-element">
              <a href="/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/hellojet" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i>GitHub</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:lijieting@zju.edu.cn" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i>E-Mail</a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://www.zhihu.com/people/hellojet" target="_blank" title="Zhihu">
                      
                        <i class="fa fa-fw fa-zhihu"></i>Zhihu</a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2018</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">李洁厅</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Gemini</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  

  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url);
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  

  

  

  
  

  

  

  

</body>
</html>
